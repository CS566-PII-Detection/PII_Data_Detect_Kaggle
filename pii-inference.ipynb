{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7803679,"sourceType":"datasetVersion","datasetId":4340749},{"sourceId":8264771,"sourceType":"datasetVersion","datasetId":4856320},{"sourceId":8268272,"sourceType":"datasetVersion","datasetId":4907560},{"sourceId":175143918,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Inference\nThis notebook does inference and post processing for PII detection data using finetuned deBERTa models. The classifer does predictions on a validation set and uses predictions to update prediction thresholds for the competition/testing data. The classifer works for single models or ensembles using weighted averaging. \n\nModel training is done here: https://www.kaggle.com/code/jonathankasprisin/pii-train\n\n### Reference\n1. https://www.kaggle.com/code/thedrcat/pii-data-detection-infer-with-w-b\n\nReferences for specfic supporting fucntions are given in the corresponding code blocks. ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport json\nimport argparse\nfrom itertools import chain\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:23.573592Z","iopub.execute_input":"2024-05-02T03:17:23.573993Z","iopub.status.idle":"2024-05-02T03:17:23.579736Z","shell.execute_reply.started":"2024-05-02T03:17:23.573962Z","shell.execute_reply":"2024-05-02T03:17:23.578694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"import yaml\n#generally the same\nDATA_PATH = '../input/pii-detection-removal-from-educational-data'\nVAL_PATH = '/kaggle/input/pii-bagging-datasets/val2.json'\nOUTPUT_DIR = \"/kaggle/working/\"\n\n\n#ensemble\nMODEL_PATHS = {\n    '/kaggle/input/pii-ens1-train/pii_ens1_base_full': 1\n}\n\nINFERENCE_MAX_LENGTH = 3500\nSTRIDE = 0\nTHRESHOLD = .99\nprint(f'Infer max length: {INFERENCE_MAX_LENGTH}, stride: {STRIDE}, threshold: {THRESHOLD}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:23.581880Z","iopub.execute_input":"2024-05-02T03:17:23.582265Z","iopub.status.idle":"2024-05-02T03:17:23.600494Z","shell.execute_reply.started":"2024-05-02T03:17:23.582232Z","shell.execute_reply":"2024-05-02T03:17:23.599573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Metric calc","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\nfrom scipy.special import softmax\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = val_parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results\n\ndef val_parse_predictions(predictions, id2label, ds, threshold=0.9):\n\n    ## only threshold 'o' label  #####\n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2) #why 2 not -1\n    print(f\" val_parse- preidicitions.shape {predictions.shape}\")\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n# ###### threshold each label #######\n#     # Ensure thresholds is a numpy array for element-wise comparison\n#     thresholds = np.array([threshold]) if np.isscalar(threshold) else np.array(threshold)\n#     if thresholds.shape[0] != predictions.shape[-1]: \n#         thresholds = np.full(predictions.shape[-1], thresholds[0])\n\n#     # Scale last dimension to probabilities for interpretability\n#     pred_softmax = softmax(predictions, axis=2) #why 2 not -1\n#     preds = predictions.argmax(-1)\n#     preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n#     O_preds = pred_softmax[:,:,12]\n#     # Find the maximum non-'O' prediction for each sample\n#     max_non_O_preds = pred_softmax[:,:,:12].max(-1)\n#     theshold_for_non_o_label = thresholds[pred_softmax[:,:,:12].argmax(-1)]\n#     preds_final = np.where(max_non_O_preds < theshold_for_non_o_label , 12 , preds_without_O)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\ndef identify_incorrect_labels(reference_df, pred_df):\n    \"\"\"\n    Identify incorrectly labeled tokens and classify them as False Negatives or False Positives.\n\n    Parameters:\n    - reference_df (DataFrame): DataFrame with the reference labels.\n    - pred_df (DataFrame): DataFrame with the predicted labels.\n\n    Returns:\n    - incorrectly_labeled (DataFrame): DataFrame with the incorrectly labeled tokens and their error types.\n    \"\"\"\n    # Drop unnecessary columns from pred_df\n    pred_df = pred_df.drop(columns=['eval_row', 'row_id'])\n\n    # Merge the DataFrames\n    merged_df = pd.merge(reference_df, pred_df, on=['document', 'token'], how='outer', suffixes=('_actual', '_pred'))\n\n    # Identify incorrectly labeled tokens\n    incorrectly_labeled = merged_df[merged_df['label_actual'] != merged_df['label_pred']].copy()\n\n    # Fill NaN values in 'label_actual' and 'label_pred' with 'O'\n    incorrectly_labeled['label_actual'] = incorrectly_labeled['label_actual'].fillna('O')\n    incorrectly_labeled['label_pred'] = incorrectly_labeled['label_pred'].fillna('O')\n\n    # Define conditions for False Negatives and False Positives\n    condition_fn = (\n        (incorrectly_labeled['label_actual'] != 'O')  &\n        ((incorrectly_labeled['label_pred'] == 'O') | (incorrectly_labeled['label_actual'] != incorrectly_labeled['label_pred']))\n    )\n    condition_fp = ((incorrectly_labeled['label_actual'] == 'O') & (incorrectly_labeled['label_pred'] != 'O'))\n\n    # Use np.select to choose between 'FN', 'FP', and None based on the conditions\n    choices = ['FN', 'FP']\n    incorrectly_labeled['error'] = np.select([condition_fn, condition_fp], choices, default=None)\n\n    return incorrectly_labeled","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:23.601762Z","iopub.execute_input":"2024-05-02T03:17:23.602061Z","iopub.status.idle":"2024-05-02T03:17:23.640497Z","shell.execute_reply.started":"2024-05-02T03:17:23.602038Z","shell.execute_reply":"2024-05-02T03:17:23.639528Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From Training Helpers\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef val_tokenize(example, tokenizer, label2id, max_length, stride):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\n        \"\".join(text),\n        return_offsets_mapping=True,\n        max_length=max_length,\n        truncation=True,\n        stride = stride,\n    ) \n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef val_create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    \n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        val_tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=2\n    )\n\n    return ds\n\ndef get_reference_df_val(raw_df): \n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token_str'] = ref_df['token']\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label', 'token_str']].copy()\n    \n    return reference_df","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:23.642776Z","iopub.execute_input":"2024-05-02T03:17:23.643154Z","iopub.status.idle":"2024-05-02T03:17:23.660990Z","shell.execute_reply.started":"2024-05-02T03:17:23.643121Z","shell.execute_reply":"2024-05-02T03:17:23.660133Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Val Score","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom transformers import Trainer\n\n#VAL load and create dataset\nprint(\"validiation dataset\")\ndata = json.load(open(VAL_PATH))\ndf = pd.DataFrame(data)\n# df = df.head(50) #TEMP\nreference_df = get_reference_df_val(df)\ndel data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:23.661955Z","iopub.execute_input":"2024-05-02T03:17:23.662220Z","iopub.status.idle":"2024-05-02T03:17:26.434669Z","shell.execute_reply.started":"2024-05-02T03:17:23.662197Z","shell.execute_reply":"2024-05-02T03:17:26.433629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load id2label configuration from first model. all models should have same id2label\nfirst_model_path = list(MODEL_PATHS.keys())[0]\nconfig = json.load(open(first_model_path + \"/config.json\"))\nid2label = config[\"id2label\"]\nlabel2id = config[\"label2id\"]\n\ndel config\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:26.435705Z","iopub.execute_input":"2024-05-02T03:17:26.435956Z","iopub.status.idle":"2024-05-02T03:17:26.794273Z","shell.execute_reply.started":"2024-05-02T03:17:26.435935Z","shell.execute_reply":"2024-05-02T03:17:26.793270Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a tokenizer and model from the pretrained model path\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n\n#create dataset to do inferenece on. Val to check errors\nds = val_create_dataset(df, tokenizer, INFERENCE_MAX_LENGTH, label2id, STRIDE)\nprint(f\"val ds exampes: {ds.num_rows}, features: {ds.num_columns}\") #DEBUG\nprint(f\"The features in the dataset are: {ds.column_names}\") #DEBUG\n\n# intialize list to save predictions to for each model\nall_preds = []\n\n# Calculate the total weight for ensemble\ntotal_weight = sum(MODEL_PATHS.values())\n\n#iterate over all the models\nfor runs, (model_path, weight) in enumerate(MODEL_PATHS.items()):\n    print(f\"inference model {runs}...\")\n    \n    #load model and collator from each model\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForTokenClassification.from_pretrained(model_path)\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n\n    #change eval size to 1 for memory\n    args = TrainingArguments(\n      \".\",\n      per_device_eval_batch_size=1,\n      report_to=\"none\",\n    )\n\n    trainer = Trainer(model=model, args=args, data_collator=collator,tokenizer=tokenizer,)\n\n    predictions = trainer.predict(ds).predictions\n    print(f\"prediction np.shape expecting: num_examples, seq_length, num_labels: {predictions.shape}\") #DEBUG\n\n    #weigh the model's predictions and add to list\n    weighted_predictions = predictions * weight\n    all_preds.append(weighted_predictions)\n    del tokenizer, model, trainer,weighted_predictions   #memory saving\n    torch.cuda.empty_cache()\n    gc.collect()\n    \nprint(f\"all_preds len expecting {len(MODEL_PATHS)}: {len(all_preds)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:17:26.795590Z","iopub.execute_input":"2024-05-02T03:17:26.797383Z","iopub.status.idle":"2024-05-02T03:23:40.597770Z","shell.execute_reply.started":"2024-05-02T03:17:26.797353Z","shell.execute_reply":"2024-05-02T03:23:40.596718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #TEMP\n# import pickle\n\n# # Replace with your file path\n# file_path = '/kaggle/input/temp-infer-debug/variables.pkl'\n\n# # Open the file in binary read mode\n# with open(file_path, 'rb') as file:\n#     # Load all objects\n#     data = pickle.load(file)\n\n# # Extract the objects from the dictionary\n# all_preds = data['all_preds']\n# total_weight = data['total_weight']\n# id2label = data['id2label']\n# ds = data['ds']\n# reference_df = data['reference_df']\n# threshold_tests = data['threshold_tests']","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:23:40.601859Z","iopub.execute_input":"2024-05-02T03:23:40.602349Z","iopub.status.idle":"2024-05-02T03:23:40.607003Z","shell.execute_reply.started":"2024-05-02T03:23:40.602313Z","shell.execute_reply":"2024-05-02T03:23:40.606066Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the weighted average of predictions\nweighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:23:40.608184Z","iopub.execute_input":"2024-05-02T03:23:40.608523Z","iopub.status.idle":"2024-05-02T03:23:41.015431Z","shell.execute_reply.started":"2024-05-02T03:23:40.608492Z","shell.execute_reply":"2024-05-02T03:23:41.014554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### for single threshold #####\nprint(\"doing threshold tests:\")\nthreshold_tests = [.6,.7,.8,.9,.99] #TEMP\nscores =[]\n\nfor threshold in threshold_tests:\n    metrics = compute_metrics((weighted_average_predictions, None), id2label, ds, reference_df, threshold=threshold)\n    f5_score = metrics['ents_f5']\n    scores.append(f5_score)\n    print(f'threshold:f5 {threshold}: {f5_score}')\n\nbest_threshold = 0.0\nbest_f5 = 0.0\nfor thresh, score in zip(threshold_tests, scores):\n    if score > best_f5:\n        best_threshold = thresh\n        best_f5 = score\nprint(f'Best f5 {best_f5}, best ensemble uniform threshold: {best_threshold}')\n\n#get incorrect labels and save to csv\npreds_df = val_parse_predictions(weighted_average_predictions, id2label, ds, threshold=best_threshold)\n\nTHRESHOLD = best_threshold","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:23:41.016786Z","iopub.execute_input":"2024-05-02T03:23:41.017508Z","iopub.status.idle":"2024-05-02T03:23:41.022649Z","shell.execute_reply.started":"2024-05-02T03:23:41.017474Z","shell.execute_reply":"2024-05-02T03:23:41.021581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Alternate threshold test ######\n# print(\"doing threshold tests:\")\n\n# #initialize starting thresholds for each class at .99\n# thresholds = np.array([0.99 for _ in range(len(id2label))])\n# metrics = compute_metrics((weighted_average_predictions, None), id2label, ds, reference_df, threshold=thresholds)\n# print(\"f5 with all .99\", metrics['ents_f5'])\n\n# best_threshold = thresholds.copy()\n# best_score = np.zeros(len(id2label))\n\n# #loop through each label threshold\n# #decrease all class thresholds and choose the best based on the per_type_{}_f5 score for each label\n# #while threshold for label is > .1 \n# while thresholds[0] > .2:\n#     metrics = compute_metrics((weighted_average_predictions, None), id2label, ds, reference_df, threshold=thresholds)\n#     #get f5 score for each label at for the threshold level. give f5 of -1 if the label isnt present resulting in no metric generated\n#     f5_score = np.array([metrics[f'ents_per_type_{id2label[str(label_idx)][2:]}_f5'] if f'ents_per_type_{id2label[str(label_idx)][2:]}_f5' in metrics else -1 for label_idx in range(len(thresholds)-1)])\n#     #for each label see if the decrease in threshold improved the score\n#     for label_idx in range(len(thresholds)-1):\n#         if f5_score[label_idx] > best_score[label_idx]: \n#             best_score[label_idx] = f5_score[label_idx]\n#             best_threshold[label_idx] = thresholds[label_idx]\n\n#     #decrease the value of that index by .1\n#     thresholds= thresholds - 0.1\n\n# #assign updated threshold for submission\n# THRESHOLD = best_threshold\n\n# metrics = compute_metrics((weighted_average_predictions, None), id2label, ds, reference_df, threshold=best_threshold)\n# print(\"f5 best_threshold\", metrics['ents_f5'])","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:23:41.023994Z","iopub.execute_input":"2024-05-02T03:23:41.024455Z","iopub.status.idle":"2024-05-02T03:29:04.182521Z","shell.execute_reply.started":"2024-05-02T03:23:41.024422Z","shell.execute_reply":"2024-05-02T03:29:04.181523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for testing in seperate cpu enviroment\nimport pickle\n\nwith open('inference_outputs.pkl', 'wb') as file:\n    pickle.dump((all_preds, weighted_average_predictions,id2label, ds, reference_df), file)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:04.183898Z","iopub.execute_input":"2024-05-02T03:29:04.184385Z","iopub.status.idle":"2024-05-02T03:29:06.207289Z","shell.execute_reply.started":"2024-05-02T03:29:04.184350Z","shell.execute_reply":"2024-05-02T03:29:06.206448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get incorrect labels and save to csv\npreds_df = val_parse_predictions(weighted_average_predictions, id2label, ds, threshold=best_threshold)\n\n#get incorrect labels and save to csv\n\nincorrectly_labeled_df = identify_incorrect_labels(reference_df, preds_df)\n          \nincorrectly_labeled_df.to_csv(\"val_incorrectly_labeled.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:06.208408Z","iopub.execute_input":"2024-05-02T03:29:06.208675Z","iopub.status.idle":"2024-05-02T03:29:38.901063Z","shell.execute_reply.started":"2024-05-02T03:29:06.208653Z","shell.execute_reply":"2024-05-02T03:29:38.899471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del all_preds, weighted_average_predictions,id2label, ds, reference_df, incorrectly_labeled_df   #memory saving\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:38.902580Z","iopub.execute_input":"2024-05-02T03:29:38.905459Z","iopub.status.idle":"2024-05-02T03:29:39.304748Z","shell.execute_reply.started":"2024-05-02T03:29:38.905411Z","shell.execute_reply":"2024-05-02T03:29:39.303563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"#functions\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\n\ndef add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices\n\ndef infer_tokenize(example, tokenizer, max_length, stride):\n    \"\"\"\n    Tokenize an example for NER using the given tokenizer.\n\n    Args:\n        example (dict): A dictionary containing \"tokens\" and \"trailing_whitespace\" lists.\n            - \"tokens\": A list of token strings.\n            - \"trailing_whitespace\": A list of boolean values indicating whether each token has trailing whitespace.\n        tokenizer: The tokenizer to use for tokenization.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: A dictionary containing tokenized output, including offsets mapping and token map.\n            - \"input_ids\": List of token IDs.\n            - \"attention_mask\": List of attention mask values.\n            - \"offset_mapping\": List of character offsets for each token.\n            - \"token_map\": List mapping each input token to its original position in the example.\n            \n    Reference: https://www.kaggle.com/code/valentinwerner/893-deberta3base-Inference\n    \"\"\"\n    #empty list to store text and tokens in respective map\n    text = []\n    token_map = []\n    \n    #keep track of tokens\n    idx = 0\n    \n    #for the example go through tokens and whitespace\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        #add token to text\n        text.append(t)\n        #extend token length number of idx\n        token_map.extend([idx]*len(t))\n        #for whitespace add a space to text and label -1 in token map\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    #Tokenize the text and return offset mapping with the token map    \n    tokenized = tokenizer(\n        \"\".join(text),\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length= max_length,\n        stride = stride,\n    )\n    length = len(tokenized.input_ids)\n        \n    return {\n        **tokenized,\n        \"length\": length,\n        \"token_map\": token_map,\n    }\n\ndef create_dataset(data, tokenizer, max_length, stride):\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n    ds = ds.map( \n        infer_tokenize,\n        fn_kwargs={\"tokenizer\": tokenizer,\n                   \"max_length\": max_length,\n                   \"stride\": stride,\n                  }, \n        num_proc=3\n    )\n    return ds\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n\n## only threshold 'o' label  #####\n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2) #why 2 not -1\n    print(f\" val_parse- preidicitions.shape {predictions.shape}\")\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n    \n# ###### threshold each label #######\n#     # Ensure thresholds is a numpy array for element-wise comparison\n#     thresholds = np.array([threshold]) if np.isscalar(threshold) else np.array(threshold)\n#     if thresholds.shape[0] != predictions.shape[-1]: \n#         thresholds = np.full(predictions.shape[-1], thresholds[0])\n\n#     # Scale last dimension to probabilities for interpretability\n#     pred_softmax = softmax(predictions, axis=2) #why 2 not -1\n#     preds = predictions.argmax(-1)\n#     preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n#     O_preds = pred_softmax[:,:,12]\n#     # Find the maximum non-'O' prediction for each sample\n#     max_non_O_preds = pred_softmax[:,:,:12].max(-1)\n#     theshold_for_non_o_label = thresholds[pred_softmax[:,:,:12].argmax(-1)]\n#     preds_final = np.where(max_non_O_preds < theshold_for_non_o_label , 12 , preds_without_O)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            token_id = token_map[start_idx] #token ID at the start of the index\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:39.332634Z","iopub.execute_input":"2024-05-02T03:29:39.333011Z","iopub.status.idle":"2024-05-02T03:29:39.358037Z","shell.execute_reply.started":"2024-05-02T03:29:39.332981Z","shell.execute_reply":"2024-05-02T03:29:39.356910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load, predict and submit","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom transformers import Trainer\n\ndata = json.load(open(DATA_PATH + \"/test.json\")) #submission)\ndf_full = pd.DataFrame(data)\ndf_full['token_indices'] = df_full['tokens'].apply(add_token_indices)\ndel data\n\n#split data into 2000 document batches- to address memory out of bounds\nsplit_size = 3000\nnum_split = len(df_full) // split_size\ndf_doc_batches = []\nfor i in range(num_split):\n    df_batch= df_full.iloc[:split_size]\n    df_doc_batches.append(df_batch)\n    df_full = df_full.iloc[split_size:]\n    print(df_full.shape[0])\n    \nif len(df_full) > 0:\n    df_doc_batches.append(df_full)\n        \ndel df_full\n\n\n# Load id2label configuration from first model. all models should have same id2label\nfirst_model_path = list(MODEL_PATHS.keys())[0]\nconfig = json.load(open(first_model_path + \"/config.json\"))\nid2label = config[\"id2label\"]\nlabel2id = config[\"label2id\"]\ndel config\n\ngc.collect()\n\n\n# Initialize a tokenizer and model from the pretrained model path\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n\n#initialize empty dataframe to append each batch to\nsub_df= pd.DataFrame()\n\n# run model in batches of documunets\nfor df in df_doc_batches:\n    #create dataset to do inferenece on. Val to check errors\n    ds = create_dataset(df, tokenizer, INFERENCE_MAX_LENGTH, STRIDE)\n    print(f\"val ds exampes: {ds.num_rows}, features: {ds.num_columns}\") #DEBUG\n\n    # intialize list to save predictions to for each model\n    all_preds = []\n\n    # Calculate the total weight for ensemble\n    total_weight = sum(MODEL_PATHS.values())\n\n    runs = 0\n    #iterate over all the models\n    for model_path, weight in MODEL_PATHS.items():\n        print(f\"inference model {runs}...\")\n        runs += 1\n\n        #load model and collator from each model\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForTokenClassification.from_pretrained(model_path)\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n\n        #change eval size to 1 for memory\n        args = TrainingArguments(\".\", per_device_eval_batch_size=1,report_to=\"none\",)\n\n        trainer = Trainer(model=model, args=args, data_collator=collator,tokenizer=tokenizer,)\n\n        predictions = trainer.predict(ds).predictions\n        print(f\"prediction np.shape expecting: num_examples, seq_length, num_labels: {predictions.shape}\") #DEBUG\n\n        #weigh the model's predictions and add to list\n        weighted_predictions = predictions * weight\n        all_preds.append(weighted_predictions)\n        del model, trainer  #memory saving\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    weighted_average_predictions = np.sum(all_preds, axis=0) / total_weight\n    preds_df = parse_predictions(weighted_average_predictions, id2label, ds, threshold=THRESHOLD)\n    print(\"pred_df type \", type(preds_df))\n    print(\"pred_df type \",type(sub_df))\n    sub_df = pd.concat([sub_df,preds_df],ignore_index=True)\n    del weighted_average_predictions, preds_df","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:39.359575Z","iopub.execute_input":"2024-05-02T03:29:39.359893Z","iopub.status.idle":"2024-05-02T03:29:44.111517Z","shell.execute_reply.started":"2024-05-02T03:29:39.359864Z","shell.execute_reply":"2024-05-02T03:29:44.110456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[\"row_id\"] = list(range(len(sub_df)))\n#look at to see\ndisplay(sub_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:44.112924Z","iopub.execute_input":"2024-05-02T03:29:44.113248Z","iopub.status.idle":"2024-05-02T03:29:44.131621Z","shell.execute_reply.started":"2024-05-02T03:29:44.113221Z","shell.execute_reply":"2024-05-02T03:29:44.130879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submit\nsub_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:29:44.135107Z","iopub.execute_input":"2024-05-02T03:29:44.135404Z","iopub.status.idle":"2024-05-02T03:29:44.143003Z","shell.execute_reply.started":"2024-05-02T03:29:44.135380Z","shell.execute_reply":"2024-05-02T03:29:44.142112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}