{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8197257,"sourceType":"datasetVersion","datasetId":4855612}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\n-removed stride compared to reference\n-raw data is full competition training comes from base_data artifact\nReference:\n- https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b \n- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=eFhyArSz826Q","metadata":{}},{"cell_type":"markdown","source":"# Run Configs","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:59:12.299869Z","iopub.execute_input":"2024-04-22T19:59:12.300186Z","iopub.status.idle":"2024-04-22T19:59:32.397433Z","shell.execute_reply.started":"2024-04-22T19:59:12.300154Z","shell.execute_reply":"2024-04-22T19:59:32.396301Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:59:32.400103Z","iopub.execute_input":"2024-04-22T19:59:32.400539Z","iopub.status.idle":"2024-04-22T19:59:51.547689Z","shell.execute_reply.started":"2024-04-22T19:59:32.400495Z","shell.execute_reply":"2024-04-22T19:59:51.546777Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:59:51.548961Z","iopub.execute_input":"2024-04-22T19:59:51.549292Z","iopub.status.idle":"2024-04-22T20:00:23.321839Z","shell.execute_reply.started":"2024-04-22T19:59:51.549263Z","shell.execute_reply":"2024-04-22T20:00:23.320854Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-22 20:00:08.654164: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 20:00:08.654279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 20:00:08.925564: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\nfrom transformers import TrainerCallback\n\ndef identify_incorrect_labels(reference_df, pred_df):\n    \"\"\"\n    Identify incorrectly labeled tokens and classify them as False Negatives or False Positives.\n\n    Parameters:\n    - reference_df (DataFrame): DataFrame with the reference labels.\n    - pred_df (DataFrame): DataFrame with the predicted labels.\n\n    Returns:\n    - incorrectly_labeled (DataFrame): DataFrame with the incorrectly labeled tokens and their error types.\n    \"\"\"\n    # Drop unnecessary columns from pred_df\n    pred_df = pred_df.drop(columns=['eval_row', 'row_id'])\n\n    # Merge the DataFrames\n    merged_df = pd.merge(reference_df, pred_df, on=['document', 'token'], how='outer', suffixes=('_actual', '_pred'))\n\n    # Identify incorrectly labeled tokens\n    incorrectly_labeled = merged_df[merged_df['label_actual'] != merged_df['label_pred']].copy()\n\n    # Fill NaN values in 'label_actual' and 'label_pred' with 'O'\n    incorrectly_labeled['label_actual'] = incorrectly_labeled['label_actual'].fillna('O')\n    incorrectly_labeled['label_pred'] = incorrectly_labeled['label_pred'].fillna('O')\n\n    # Define conditions for False Negatives and False Positives\n    condition_fn = (\n        (incorrectly_labeled['label_actual'] != 'O')  &\n        ((incorrectly_labeled['label_pred'] == 'O') | (incorrectly_labeled['label_actual'] != incorrectly_labeled['label_pred']))\n    )\n    condition_fp = ((incorrectly_labeled['label_actual'] == 'O') & (incorrectly_labeled['label_pred'] != 'O'))\n\n    # Use np.select to choose between 'FN', 'FP', and None based on the conditions\n    choices = ['FN', 'FP']\n    incorrectly_labeled['error'] = np.select([condition_fn, condition_fp], choices, default=None)\n\n    return incorrectly_labeled\n\n# #call back to log loss for sweep analysis\n# class WandbLoggingCallback(TrainerCallback):\n#     def on_log(self, args, state, control, trainer=None, **kwargs):\n#         # Log metrics to wandb\n#         if trainer is not None:\n#             logs = {}\n#             # Log loss\n#             logs[\"loss\"] = trainer.state.log_history[-1][\"loss\"]\n#             wandb.log(logs)\n            \ndef do_downsample(train_df, ratio):\n    '''\n        Down sample negative examples\n    '''\n    # Separate positive and negative samples\n    p = train_df[train_df['labels'].apply(lambda x: any(label != \"O\" for label in x))]\n    n = train_df[train_df['labels'].apply(lambda x: all(label == \"O\" for label in x))]\n\n    # Downsample negative samples\n    n = n.sample(int(len(n) * ratio))\n\n    # Combine positive and downsampled negative samples\n    df = pd.concat([p, n], ignore_index=True)\n    \n    return df\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n#CHECK- modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(parquet_path): \n    raw_df = pd.read_parquet(parquet_path)\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n    \n    return reference_df\n\n\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        #class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:23.324351Z","iopub.execute_input":"2024-04-22T20:00:23.324960Z","iopub.status.idle":"2024-04-22T20:00:23.355894Z","shell.execute_reply.started":"2024-04-22T20:00:23.324931Z","shell.execute_reply":"2024-04-22T20:00:23.354819Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length, stride):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\n        \"\".join(text),\n        return_offsets_mapping=True,\n        max_length=max_length,\n        truncation=True,\n        stride = stride,\n    ) \n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    \n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:23.357286Z","iopub.execute_input":"2024-04-22T20:00:23.357653Z","iopub.status.idle":"2024-04-22T20:00:23.383347Z","shell.execute_reply.started":"2024-04-22T20:00:23.357618Z","shell.execute_reply":"2024-04-22T20:00:23.382475Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# train Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results\n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:23.384615Z","iopub.execute_input":"2024-04-22T20:00:23.385022Z","iopub.status.idle":"2024-04-22T20:00:23.411930Z","shell.execute_reply.started":"2024-04-22T20:00:23.384997Z","shell.execute_reply":"2024-04-22T20:00:23.411118Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\nimport pickle\nimport re\n\n\ndef train(config = None):\n    torch.cuda.empty_cache() #free up memory that isnt in use\n    \n    # Initialize new wandb run to run without sweep agent\n    with wandb.init(project='piiV2', job_type='train', config=config):\n        config = wandb.config\n        \n       # Load the training data\n        train_df = pd.read_parquet(config.train_artifact_path)\n\n        # Load the validation data\n        val_df = pd.read_parquet(config.val_artifact_path)\n        eval_df = val_df.copy()\n        \n        # Load external data\n        for parquet_path in [config.external_data_1, config.external_data_2, config.external_data_3, config.external_data_4, config.external_data_5]:\n            if parquet_path != 'none':\n                print(f'Loading external data...')\n                ext_df = pd.read_parquet(parquet_path)\n                train_df = pd.concat([train_df, ext_df], ignore_index=True)\n        \n        wandb.log({'num_docs_train_raw': len(train_df)})\n        #down sample\n        train_df = do_downsample(train_df, config.downsample_ratio)\n        \n        wandb.log({'num_docs_train': len(train_df)})\n        \n        # Prepare references and labels from val set\n        reference_df = get_reference_df(config.val_artifact_path)\n        all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n        label2id = {l: i for i,l in enumerate(all_labels)}\n        id2label = {v:k for k,v in label2id.items()}\n\n        # Create the training and validation datasets\n        tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n        train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id, config.stride)\n        valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id, config.stride)\n\n        # Initialize the model and data collator\n        model = AutoModelForTokenClassification.from_pretrained(\n            config.training_model_path,\n            num_labels=len(all_labels),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n        # Define the training arguments\n        args = TrainingArguments(\n            output_dir=config.output_dir, \n            fp16=config.fp16,\n            learning_rate=config.learning_rate,\n            num_train_epochs=config.num_train_epochs,\n            per_device_train_batch_size=config.per_device_train_batch_size,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            report_to=config.report_to,\n            evaluation_strategy=config.evaluation_strategy,\n            eval_steps = config.eval_steps,\n            save_strategy = config.evaluation_strategy, #these need to be the same\n            do_eval=config.do_eval,\n            save_total_limit=config.save_total_limit,\n            logging_steps=config.logging_steps,\n            lr_scheduler_type=config.lr_scheduler_type,\n            warmup_ratio=config.warmup_ratio,\n            weight_decay=config.weight_decay,\n            load_best_model_at_end = config.load_best_model_at_end,\n            metric_for_best_model = config.metric_for_best_model ,\n            greater_is_better = config.greater_is_better,\n        )\n\n        #class weights based on dataset to go to CustomTrainer Class\n        class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n        # Initialize Trainer with custom class weights\n        trainer = CustomTrainer(\n            model=model, \n            args=args, \n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            data_collator=collator, \n            tokenizer=tokenizer,\n            compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n            class_weights=class_weights,\n            #callbacks=[WandbLoggingCallback], #added for wandb anaylsis\n        )\n\n        # Train the model\n        trainer.train()    \n\n        # Make predictions on the validation dataset\n        preds = trainer.predict(valid_ds)\n\n        #theshold tests\n        print(\"doing threshold tests:\")\n        threshold_tests = [.7, 0.99] #TEMP\n        scores =[]\n        \n        for threshold in threshold_tests:\n            metrics = compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=threshold)\n            f5_score = metrics['ents_f5']\n            scores.append(f5_score)\n            wandb.log({'threshold': threshold, 'final_f5': f5_score})\n            print(f'threshold:f5 {threshold}: {f5_score}')\n\n        best_threshold = 0.0  \n        best_f5 = 0.0  \n        for thresh, score in zip(threshold_tests, scores):\n            if score > best_f5:\n                best_threshold = thresh\n                best_f5 = score\n            \n        wandb.config.best_threshold = best_threshold\n        preds_df = parse_predictions(preds.predictions, id2label, valid_ds, threshold=best_threshold)\n        \n        #make DF of errors and save to wandb\n#TEMP\n#         incorrectly_labeled = identify_incorrect_labels(reference_df, preds_df)\n#         errors_table = wandb.Table(dataframe=incorrectly_labeled)\n#         wandb.log({'errors_table': errors_table})\n        \n        # Save the model and upload it to Kaggle\n        os.makedirs(config.experiment, exist_ok=True)\n        trainer.save_model(config.experiment)\n        tokenizer.save_pretrained(config.experiment)\n        print('Experiment finished, test it out on the inference notebook!')\n    \n    return best_threshold","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:23.413240Z","iopub.execute_input":"2024-04-22T20:00:23.413589Z","iopub.status.idle":"2024-04-22T20:00:23.437133Z","shell.execute_reply.started":"2024-04-22T20:00:23.413564Z","shell.execute_reply":"2024-04-22T20:00:23.435974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# W and B \n- login\n- default config\n- update config\n- train","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:23.438342Z","iopub.execute_input":"2024-04-22T20:00:23.438653Z","iopub.status.idle":"2024-04-22T20:00:25.611679Z","shell.execute_reply.started":"2024-04-22T20:00:23.438630Z","shell.execute_reply":"2024-04-22T20:00:25.610767Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#Default Config\nsweep_config = {\n    'method': 'bayes' #grid, random, bayes\n    }\n\n#metrics for evaluation\nmetric = {\n    'name': 'loss',\n    'goal': 'minimize'   \n    }\n\nsweep_config['metric'] = metric\n\n#intialize parameters \nparameters_dict = {\n    'experiment': {'value': 'pii_00'},\n    'threshold': {'value': 0.99},\n    'o_weight': {'value': 0.05},  # set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': 1.0},  # set to 1 for no downsample\n    'raw_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/train2_fromval.parquet'},\n    'val_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'external_data_1': {'value': 'none'},\n    'external_data_2': {'value': 'none'},\n    'external_data_3': {'value': 'none'},\n    'external_data_4': {'value': 'none'},\n    'external_data_5': {'value': 'none'},\n    'output_dir': {'value': 'output'},\n    'inference_max_length': {'value': 1024},\n    'training_max_length': {'value': 1024},\n    'stride': {'value': 0}, # set to 0 for no effect\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'fp16': {'value': True},\n    'learning_rate': {'value': 1e-5},\n    'num_train_epochs': {'value': .1},\n    'per_device_train_batch_size': {'value': 4},\n    'per_device_eval_batch_size': {'value': 4},\n    'gradient_accumulation_steps': {'value': 2},\n    'report_to': {'value': 'wandb'},\n    'evaluation_strategy': {'value': 'epoch'},\n    'eval_steps': {'value': 20},\n    'do_eval': {'value': False},\n    'save_total_limit': {'value': 2},\n    'logging_steps': {'value': 10},\n    'lr_scheduler_type': {'value': 'cosine'},\n    'warmup_ratio': {'value': 0.1},\n    'weight_decay': {'value': 0.01},\n    'load_best_model_at_end': {'value': True},\n    'metric_for_best_model': {'value': 'ents_f5'},\n    'greater_is_better': {'value': True},\n}\n    \nsweep_config['parameters'] = parameters_dict\ntrain_config = parameters_dict\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:25.612832Z","iopub.execute_input":"2024-04-22T20:00:25.613385Z","iopub.status.idle":"2024-04-22T20:00:25.624453Z","shell.execute_reply.started":"2024-04-22T20:00:25.613360Z","shell.execute_reply":"2024-04-22T20:00:25.623532Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#update train parameters using dictionary so that it works with sweep\n\ntrain_config.update({\n    'experiment': {\n        'value': 'pii0test'},\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/data_more2_fixed.parquet'},\n#     'external_data_1': {'value': '/kaggle/input/pii-datasets/PII_datasets/pii_dataset_fixed.parquet'},\n#     'external_data_2': {'value': '/kaggle/input/pii-datasets/PII_datasets/moredata_dataset_fixed.parquet'},\n    'num_train_epochs': {\n        'value': .1},\n    'per_device_eval_batch_size': {\n        'value': 2},\n    'per_device_train_batch_size': {\n        'value': 2},\n    'gradient_accumulation_steps': {\n        'value': 3},\n    'learning_rate': {\n        'value': 1.5e-5},\n    'evaluation_strategy': {\n        'value': 'epoch'},\n    'stride': {'value': 126},\n    'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': 1},  # set to 1 for no downsample\n    'inference_max_length': {'value': 3500},\n    'training_max_length': {'value': 3500},\n    })","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:25.625582Z","iopub.execute_input":"2024-04-22T20:00:25.626127Z","iopub.status.idle":"2024-04-22T20:00:25.644864Z","shell.execute_reply.started":"2024-04-22T20:00:25.626083Z","shell.execute_reply":"2024-04-22T20:00:25.644011Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#nested dictionary of parameters interested in and method we are trying\nimport pprint\npprint.pprint(train_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:25.647580Z","iopub.execute_input":"2024-04-22T20:00:25.647864Z","iopub.status.idle":"2024-04-22T20:00:25.658325Z","shell.execute_reply.started":"2024-04-22T20:00:25.647841Z","shell.execute_reply":"2024-04-22T20:00:25.657293Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'do_eval': {'value': False},\n 'downsample_ratio': {'value': 1},\n 'eval_steps': {'value': 20},\n 'evaluation_strategy': {'value': 'epoch'},\n 'experiment': {'value': 'pii0test'},\n 'external_data_1': {'value': 'none'},\n 'external_data_2': {'value': 'none'},\n 'external_data_3': {'value': 'none'},\n 'external_data_4': {'value': 'none'},\n 'external_data_5': {'value': 'none'},\n 'fp16': {'value': True},\n 'gradient_accumulation_steps': {'value': 3},\n 'greater_is_better': {'value': True},\n 'inference_max_length': {'value': 3500},\n 'learning_rate': {'value': 1.5e-05},\n 'load_best_model_at_end': {'value': True},\n 'logging_steps': {'value': 10},\n 'lr_scheduler_type': {'value': 'cosine'},\n 'metric_for_best_model': {'value': 'ents_f5'},\n 'num_train_epochs': {'value': 0.1},\n 'o_weight': {'value': 0.76},\n 'output_dir': {'value': 'output'},\n 'per_device_eval_batch_size': {'value': 2},\n 'per_device_train_batch_size': {'value': 2},\n 'raw_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n 'report_to': {'value': 'wandb'},\n 'save_total_limit': {'value': 2},\n 'stride': {'value': 126},\n 'threshold': {'value': 0.99},\n 'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/data_more2_fixed.parquet'},\n 'training_max_length': {'value': 3500},\n 'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n 'val_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n 'warmup_ratio': {'value': 0.1},\n 'weight_decay': {'value': 0.01}}\n","output_type":"stream"}]},{"cell_type":"code","source":"# runs training script\n\n# Extract inner values from the dictionary\nconfig = {k: v['value'] for k, v in train_config.items()}\n\n# Convert to SimpleNamespace\nconfig = SimpleNamespace(**config)\nbest_threshold = train(config)\nprint(f'Best Threshold : {best_threshold}')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:25.659521Z","iopub.execute_input":"2024-04-22T20:00:25.659825Z","iopub.status.idle":"2024-04-22T20:10:49.666511Z","shell.execute_reply.started":"2024-04-22T20:00:25.659801Z","shell.execute_reply":"2024-04-22T20:10:49.665554Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkasprisi\u001b[0m (\u001b[33mcsci566sp24\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240422_200025-w9efcv01</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/piiV2/runs/w9efcv01' target=\"_blank\">classic-puddle-32</a></strong> to <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/piiV2/runs/w9efcv01' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2/runs/w9efcv01</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a29eed3944394c398641c80f857a3cb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ddd1b7aeef482ca6eb9fdc18de577e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d688bdee33f54c6db10f68c9ee7f67c9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/361 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e767f17ee549d2957b868a12bdac36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3406 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa662e01dd6048d2b56d4f13c46b0ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c486c65fe05648158f9e3b7b9460c83f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 03:43, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>3.313794</td>\n      <td>0.000063</td>\n      <td>0.055939</td>\n      <td>0.001592</td>\n      <td>0.000012</td>\n      <td>0.238095</td>\n      <td>0.000322</td>\n      <td>0.000449</td>\n      <td>0.493151</td>\n      <td>0.011423</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000474</td>\n      <td>0.021497</td>\n      <td>0.007950</td>\n      <td>0.000023</td>\n      <td>0.523810</td>\n      <td>0.000595</td>\n      <td>0.000010</td>\n      <td>0.045455</td>\n      <td>0.000258</td>\n      <td>0.000270</td>\n      <td>0.166667</td>\n      <td>0.006737</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"doing threshold tests:\nthreshold:f5 0.7: 0.001591623796362845\nthreshold:f5 0.99: 0.001591623796362845\nExperiment finished, test it out on the inference notebook!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁</td></tr><tr><td>eval/ents_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁</td></tr><tr><td>eval/ents_r</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>final_f5</td><td>▁▁</td></tr><tr><td>num_docs_train</td><td>▁</td></tr><tr><td>num_docs_train_raw</td><td>▁</td></tr><tr><td>threshold</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.00159</td></tr><tr><td>eval/ents_p</td><td>6e-05</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.0006</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>2e-05</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>0.52381</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.00795</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.00047</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.0215</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.00032</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>1e-05</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.2381</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.00026</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>1e-05</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.04545</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.01142</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.00045</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.49315</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.00674</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.00027</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.16667</td></tr><tr><td>eval/ents_r</td><td>0.05594</td></tr><tr><td>eval/loss</td><td>3.31379</td></tr><tr><td>eval/runtime</td><td>220.5244</td></tr><tr><td>eval/samples_per_second</td><td>15.445</td></tr><tr><td>eval/steps_per_second</td><td>0.966</td></tr><tr><td>final_f5</td><td>0.00159</td></tr><tr><td>num_docs_train</td><td>361</td></tr><tr><td>num_docs_train_raw</td><td>361</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>1660386096768.0</td></tr><tr><td>train/epoch</td><td>0.1</td></tr><tr><td>train/global_step</td><td>3</td></tr><tr><td>train_loss</td><td>3.24515</td></tr><tr><td>train_runtime</td><td>227.747</td></tr><tr><td>train_samples_per_second</td><td>0.159</td></tr><tr><td>train_steps_per_second</td><td>0.013</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">classic-puddle-32</strong> at: <a href='https://wandb.ai/csci566sp24/piiV2/runs/w9efcv01' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2/runs/w9efcv01</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240422_200025-w9efcv01/logs</code>"},"metadata":{}},{"name":"stdout","text":"Best Threshold : 0.7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TODO\n- move helper functions to seperate script","metadata":{}}]}