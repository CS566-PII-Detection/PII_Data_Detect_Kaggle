{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":6065,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":4686}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Training Notebook\n\n# Overview\n    - Trained on kaggle GPU P100\n# To try\n\nhttps://www.kaggle.com/code/jonathankasprisin/915-deberta3base-training-test/edit ->\n\n- Downsampling negative samples (samples without labels, but they possible still work as examples where names should not be tagged as name)\n- Adding @moths external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/469493\n- Adding PJMathematicianss external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470921\n- However, I used my cleaned version instead (the punctuation is flawed in the original data set at the time of this trainign): https://www.kaggle.com/code/valentinwerner/fix-punctuation-tokenization-external-dataset","metadata":{}},{"cell_type":"markdown","source":"# Config and import\n\n- Try: max length adjust\n","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:16.087320Z","iopub.execute_input":"2024-02-26T04:09:16.087664Z","iopub.status.idle":"2024-02-26T04:09:33.653734Z","shell.execute_reply.started":"2024-02-26T04:09:16.087636Z","shell.execute_reply":"2024-02-26T04:09:33.652657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:33.655969Z","iopub.execute_input":"2024-02-26T04:09:33.656269Z","iopub.status.idle":"2024-02-26T04:09:54.308369Z","shell.execute_reply.started":"2024-02-26T04:09:33.656241Z","shell.execute_reply":"2024-02-26T04:09:54.307554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = '../input/pii-detection-removal-from-educational-data'\nTRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\" #/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"../output\"\n\n#print files with pathname\nfor dirname, _, filenames in os.walk(DATA_PATH):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:54.309468Z","iopub.execute_input":"2024-02-26T04:09:54.310054Z","iopub.status.idle":"2024-02-26T04:09:54.316340Z","shell.execute_reply.started":"2024-02-26T04:09:54.310028Z","shell.execute_reply":"2024-02-26T04:09:54.315542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Selection\nTo try\n- down sample of negative examples\n- augment data","metadata":{}},{"cell_type":"code","source":"#data from orginal training json\ndata = json.load(open(DATA_PATH+ \"/train.json\"))\n\nprint(\"Training Data: \", len(data))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T04:09:54.317615Z","iopub.execute_input":"2024-02-26T04:09:54.317897Z","iopub.status.idle":"2024-02-26T04:09:56.903314Z","shell.execute_reply.started":"2024-02-26T04:09:54.317874Z","shell.execute_reply":"2024-02-26T04:09:56.902321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set up labeling for NER with #Targets: B-Beginning entity, I-inside entity, O- outside entity\n\n#Extract all unique labels w/ list comprehension. Use chain to flatten list of lists\nall_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n\n#Create dictionary of label to id\nlabel2id = {l: i for i,l in enumerate(all_labels)}\n\n#Create dictionary of id to label\nid2label = {v:k for k,v in label2id.items()}\n\n#target labels identified in the training data\ntarget = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n]\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:56.905942Z","iopub.execute_input":"2024-02-26T04:09:56.906241Z","iopub.status.idle":"2024-02-26T04:09:56.984028Z","shell.execute_reply.started":"2024-02-26T04:09:56.906216Z","shell.execute_reply":"2024-02-26T04:09:56.983075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize Data","metadata":{}},{"cell_type":"code","source":"#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True)\n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:56.985524Z","iopub.execute_input":"2024-02-26T04:09:56.986166Z","iopub.status.idle":"2024-02-26T04:09:56.997995Z","shell.execute_reply.started":"2024-02-26T04:09:56.986141Z","shell.execute_reply":"2024-02-26T04:09:56.997105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load tokenizer based on pretrained model\ntokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\n#convert to hugging face Dataset object\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n})\n\n# Map the tokenize function to your dataset\nds = ds.map(\n    tokenize,\n    fn_kwargs={      # pass keyword args\n        \"tokenizer\": tokenizer,\n        \"label2id\": label2id,\n        \"max_length\": TRAINING_MAX_LENGTH\n    }, \n    num_proc=3   #apply in paralell using 3 processes\n)\n\n#todo check for UNK token due to fast tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:09:56.999207Z","iopub.execute_input":"2024-02-26T04:09:56.999514Z","iopub.status.idle":"2024-02-26T04:11:29.987676Z","shell.execute_reply.started":"2024-02-26T04:09:56.999491Z","shell.execute_reply":"2024-02-26T04:11:29.986718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TEMP check token and labels from first example before and after tokenzization \n\n#get first example from ds\nx = ds[0]\n\n# for before tokenization print all tokens that are not outside an entity\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\n#print **** to seperate \nprint(\"*\"*10)\n\n#print all tokens and label after tokenization\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:11:29.989055Z","iopub.execute_input":"2024-02-26T04:11:29.989350Z","iopub.status.idle":"2024-02-26T04:11:30.006746Z","shell.execute_reply.started":"2024-02-26T04:11:29.989322Z","shell.execute_reply":"2024-02-26T04:11:30.005790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics and Training","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\n\ndef compute_metrics(p, all_labels):\n    \"\"\"Compute the F1, recall, precision metrics for a NER task.\n\n    Args:\n        p (Tuple[np.ndarray, np.ndarray]): The predictions and labels.\n        all_labels (List[str]): The list of all possible labels.\n\n    Returns:\n        Dict[str, float]: The computed metrics (recall, precision, f1_score).\n    Ref: https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n    #Note: seqeval framework for sequence labeling like NER\n    \n    # Unpack the predictions and labels\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f5_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f5': f5_score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:11:30.008024Z","iopub.execute_input":"2024-02-26T04:11:30.008319Z","iopub.status.idle":"2024-02-26T04:11:30.026259Z","shell.execute_reply.started":"2024-02-26T04:11:30.008295Z","shell.execute_reply":"2024-02-26T04:11:30.025410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load\nmodel = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,        #pretrained model\n    num_labels=len(all_labels), #num of unique labels for finetuning\n    id2label=id2label,          #dicts for converting in fine tuning\n    label2id=label2id,\n    ignore_mismatched_sizes=True #pretrained model might have been trained on different num of labels\n)\n\n#collate list of sample from dataset into batches. 16 might be benefical for GPU architecture\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:11:30.027304Z","iopub.execute_input":"2024-02-26T04:11:30.027589Z","iopub.status.idle":"2024-02-26T04:11:42.013705Z","shell.execute_reply.started":"2024-02-26T04:11:30.027558Z","shell.execute_reply":"2024-02-26T04:11:42.012939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO - do we need evaluation set \n\n# final_ds = ds.train_test_split(test_size=0.2, seed=42) # cannot use stratify_by_column='group'\n# final_ds","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:11:42.014937Z","iopub.execute_input":"2024-02-26T04:11:42.015278Z","iopub.status.idle":"2024-02-26T04:11:42.019816Z","shell.execute_reply.started":"2024-02-26T04:11:42.015247Z","shell.execute_reply":"2024-02-26T04:11:42.018836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Configure training process\n#no validation set specified\ntraining_args = TrainingArguments(\n    output_dir= OUTPUT_DIR,  # Directory to save checkpoints and logs\n    #fp16 =True,               #mix-precision training on 16 bit to reduce memory and speed up training\n    #learning_rate=2e-5,       # intial learning rate\n    gradient_accumulation_steps=2,  #how many batches to acculumate gradient before optimization if batch size limited by GPU memory\n    report_to=\"none\",        #where training report progress, \"none\" prevents wandb login\n    num_train_epochs=3,      # Number of training epochs\n    per_device_train_batch_size=4,  # Batch size based per GPU\n    #save_steps=500,          # Save model checkpoints every X steps\n    do_eval = False,          #whether or not to perform eval during training\n    evaluation_strategy=\"no\",    # When to evaluate during training {no, steps or epoch}\n    #eval_steps=100,          # Evaluate every X steps if stretegy is \"steps\"\n    #logging_dir=OUTPUT_DIR+\"/logs\",    # Directory to save training logs\n    logging_steps=100,       # Log training progress every X steps\n    #load_best_model_at_end=True,   # Load the best model at the end of training\n    metric_for_best_model=\"f5\",  # Metric to determine the best model (\"accuracy\", f1...)\n    #greater_is_better=True,      # if higher eval metric is better. True for f1 and acc\n    save_total_limit=1,      # how many checkpoints to keep at end (1 means most recent)\n    #lr_scheduler_type='cosine', #\n    #warmup_ratio=0.1,           #steps to gradually increase learning rate. can help stabalize training at begining\n    #weight_decay=0.01,          # l2 regularization to prevent overfitting\n    \n)\n\n#inialize trainer for training and evaluation interface\ntrainer = Trainer(\n    model=model, \n    args=training_args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels), #partial to fix all_label argument\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:26:47.274138Z","iopub.execute_input":"2024-02-26T04:26:47.274839Z","iopub.status.idle":"2024-02-26T04:26:47.289938Z","shell.execute_reply.started":"2024-02-26T04:26:47.274804Z","shell.execute_reply":"2024-02-26T04:26:47.288854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#measure execution time if cpu\n#%%time\n\n#train model \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:26:51.195654Z","iopub.execute_input":"2024-02-26T04:26:51.196486Z","iopub.status.idle":"2024-02-26T04:27:38.411831Z","shell.execute_reply.started":"2024-02-26T04:26:51.196459Z","shell.execute_reply":"2024-02-26T04:27:38.410484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_pii2d_1024_v1\")\ntokenizer.save_pretrained(\"deberta3base_pii2d_1024_v1\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T04:11:43.093236Z","iopub.status.idle":"2024-02-26T04:11:43.093635Z","shell.execute_reply.started":"2024-02-26T04:11:43.093417Z","shell.execute_reply":"2024-02-26T04:11:43.093431Z"},"trusted":true},"execution_count":null,"outputs":[]}]}