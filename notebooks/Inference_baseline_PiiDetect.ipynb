{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa871fd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-07T02:24:57.402648Z",
     "iopub.status.busy": "2024-02-07T02:24:57.402034Z",
     "iopub.status.idle": "2024-02-07T02:24:57.409111Z",
     "shell.execute_reply": "2024-02-07T02:24:57.408052Z"
    },
    "papermill": {
     "duration": 0.015933,
     "end_time": "2024-02-07T02:24:57.411938",
     "exception": false,
     "start_time": "2024-02-07T02:24:57.396005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Kaggle notes:\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "#kaggle file organization\n",
    "#/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2\n",
    "#/kaggle/input/pii-detection-removal-from-educational-data\n",
    "#/kaggle/working/\n",
    "#/kaggle/temp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb359ef",
   "metadata": {
    "papermill": {
     "duration": 0.002469,
     "end_time": "2024-02-07T02:24:57.417879",
     "exception": false,
     "start_time": "2024-02-07T02:24:57.415410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If not running on Kaggle, will need to install Kaggle API with !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6771c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# set global flags\n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f5bba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T02:24:57.425760Z",
     "iopub.status.busy": "2024-02-07T02:24:57.425293Z",
     "iopub.status.idle": "2024-02-07T02:24:57.441684Z",
     "shell.execute_reply": "2024-02-07T02:24:57.440587Z"
    },
    "papermill": {
     "duration": 0.024374,
     "end_time": "2024-02-07T02:24:57.445152",
     "exception": false,
     "start_time": "2024-02-07T02:24:57.420778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def download_data():\n",
    "    \"\"\"\n",
    "    Downloads the required dataset based on the environment.\n",
    "    \n",
    "    If running on Kaggle, the dataset is downloaded from the input folder.\n",
    "    If running locally, the dataset is downloaded from Kaggle competition.\n",
    "    Reference: https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster\n",
    "    \"\"\"\n",
    "    if IS_KAGGLE:\n",
    "        path = Path('../input/pii-detection-removal-from-educational-data')\n",
    "#        ! pip install -q []\n",
    "    else:\n",
    "        import zipfile, kaggle\n",
    "        datadir_path = Path('../data/external')\n",
    "        competition_data_str = 'pii-detection-removal-from-educational-data'\n",
    "        path = datadir_path/Path(competition_data_str)\n",
    "        if not os.path.exists(path):\n",
    "            #download the dataset from kaggle given competition name\n",
    "            kaggle.api.competition_download_files(competition_data_str, path=datadir_path)\n",
    "            #move the zip file to the data directory\n",
    "            #os.rename(f'{competition_data_str}.zip', path.with_suffix('.zip'))\n",
    "            zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665bcbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T02:24:57.455127Z",
     "iopub.status.busy": "2024-02-07T02:24:57.454144Z",
     "iopub.status.idle": "2024-02-07T02:24:57.462121Z",
     "shell.execute_reply": "2024-02-07T02:24:57.460867Z"
    },
    "papermill": {
     "duration": 0.01691,
     "end_time": "2024-02-07T02:24:57.465667",
     "exception": false,
     "start_time": "2024-02-07T02:24:57.448757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\external\\pii-detection-removal-from-educational-data\\sample_submission.csv\n",
      "..\\data\\external\\pii-detection-removal-from-educational-data\\test.json\n",
      "..\\data\\external\\pii-detection-removal-from-educational-data\\train.json\n"
     ]
    }
   ],
   "source": [
    "#Config, import and download data\n",
    "\n",
    "#Get Data and print files in path from download data\n",
    "DATA_PATH = download_data()\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#Inference model         \n",
    "if IS_KAGGLE:       \n",
    "    INFERENCE_MODEL_PATH = \"/kaggle/input/deberta_v3/keras/deberta_v3_base_en/2\"   \n",
    "    OUTPUT_DIR = 'output'  # your output path\n",
    "else:\n",
    "    INFERENCE_MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "    OUTPUT_DIR = 'output'  # your output path\n",
    "    \n",
    "INFERENCE_MAX_LENGTH = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f44eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warnings for notebook remove to Trouble shoot\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d83361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77d0b4",
   "metadata": {},
   "source": [
    "# Load pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba416abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tokenize(example, tokenizer):\n",
    "    # TODO why this set up?\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00440172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154d6551875e4f52863140b4190e4f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jkasp\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Instantiate (object what is an instance of the class) tokenizer associated with pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(INFERENCE_MODEL_PATH)\n",
    "\n",
    "#Instantiate model for classification task\n",
    "model = AutoModelForTokenClassification.from_pretrained(INFERENCE_MODEL_PATH)\n",
    "\n",
    "#TODO if local review symlinks and caching for warning message is letting you know that the caching system will still work,\n",
    "# but it might require more disk space because it can't use symlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b867a4",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1feccbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a86ff9ef2941b686695bcd1ce5c3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'INFERENCE_MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 625, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3458, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"c:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3361, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\jkasp\\AppData\\Local\\Temp\\ipykernel_4560\\2618265092.py\", line 19, in infer_tokenize\nNameError: name 'INFERENCE_MAX_LENGTH' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m      5\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailing_whitespace\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailing_whitespace\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data],\n\u001b[0;32m     10\u001b[0m })\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Tokenize the dataset. Apply tokenize function to each element of ds\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#pass specific tokenizer to tokenize function\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#num_proc is the number of processes to use. Select based on CPU cores\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#output is a new dataset with tokenized text\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer_tokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    556\u001b[0m }\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3197\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3191\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3193\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3194\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3195\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3196\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3198\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3199\u001b[0m     ):\n\u001b[0;32m   3200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3201\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\utils\\py_utils.py:665\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\datasets\\utils\\py_utils.py:665\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\jkasp\\miniconda3\\envs\\cs566_project_env\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INFERENCE_MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "#load json file to dictionary\n",
    "data = json.load(open(DATA_PATH/'test.json'))\n",
    "\n",
    "#Create a dataset from dictionary \n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [x[\"document\"] for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "\n",
    "#Tokenize the dataset. Apply tokenize function to each element of ds\n",
    "#pass specific tokenizer to tokenize function\n",
    "#num_proc is the number of processes to use. Select based on CPU cores\n",
    "#output is a new dataset with tokenized text\n",
    "ds = ds.map(infer_tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d4990",
   "metadata": {},
   "source": [
    "# Set up Trainer API\n",
    "- defines training loop for NER pipeline\n",
    "- handles training, validation and evalutiaon \n",
    "\n",
    "References:\n",
    "- https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/\n",
    "- https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect samples into batches for training or evaluation.\n",
    "#padding to make each batch the same length. 8 or 16 can be more efficient for GPU use\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "#TODO review args for API\n",
    "#TrainingArguments is a class that contains all the attributes needed to initiate a training.\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#initialize the trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa14b2",
   "metadata": {},
   "source": [
    "# Predicitions and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d66799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  /valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n",
    "predictions = trainer.predict(ds).predictions\n",
    "pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n",
    "\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "preds = predictions.argmax(-1)\n",
    "preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n",
    "O_preds = pred_softmax[:,:,12]\n",
    "\n",
    "threshold = 0.9\n",
    "preds_final = np.where(O_preds < threshold, preds_without_O , preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n",
    "\n",
    "triplets = []\n",
    "document, token, label, token_str = [], [], [], []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n",
    "\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]\n",
    "\n",
    "        if start_idx + end_idx == 0: continue\n",
    "\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "        # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if label_pred != \"O\" and token_id != -1:\n",
    "            triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "            if triplet not in triplets:\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                triplets.append(triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00879d",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"document\": document,\n",
    "    \"token\": token,\n",
    "    \"label\": label,\n",
    "    \"token_str\": token_str\n",
    "})\n",
    "\n",
    "df[\"row_id\"] = list(range(len(df)))\n",
    "display(df.head(20))\n",
    "\n",
    "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.038603,
   "end_time": "2024-02-07T02:24:57.891535",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-07T02:24:53.852932",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
