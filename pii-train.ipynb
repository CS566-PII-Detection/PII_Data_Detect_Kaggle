{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8264771,"sourceType":"datasetVersion","datasetId":4856320}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\nThis notebook is used for experimentation and training models for PII. Training is logged using W&B to monitor, analyze and evaluate performance for formulating additional experiments. Select iterations of previous experiments can be found under prior runs. A sweep implimentation for hyperparamter optimization is also present but commented out for training. \n\nData artifacts are generated using this notebook: https://www.kaggle.com/code/jonathankasprisin/pii-prep-ens\nFinal inference post processing and submission is done using this notebook: https://www.kaggle.com/code/jonathankasprisin/pii-inference/\n\n\nReference:\n1. https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b \n2. https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=eFhyArSz826Q\n\nAdditional references for specific functions are provided in their code blocks. ","metadata":{}},{"cell_type":"markdown","source":"# Run Configs","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# #if google colab\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n# !pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:44:41.103743Z","iopub.execute_input":"2024-04-29T23:44:41.104079Z","iopub.status.idle":"2024-04-29T23:44:41.114898Z","shell.execute_reply.started":"2024-04-29T23:44:41.104050Z","shell.execute_reply":"2024-04-29T23:44:41.113971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:44:41.116665Z","iopub.execute_input":"2024-04-29T23:44:41.117554Z","iopub.status.idle":"2024-04-29T23:44:59.573976Z","shell.execute_reply.started":"2024-04-29T23:44:41.117518Z","shell.execute_reply":"2024-04-29T23:44:59.572954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip freeze >requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:44:59.575247Z","iopub.execute_input":"2024-04-29T23:44:59.575521Z","iopub.status.idle":"2024-04-29T23:45:02.615391Z","shell.execute_reply.started":"2024-04-29T23:44:59.575497Z","shell.execute_reply":"2024-04-29T23:45:02.614091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:02.618588Z","iopub.execute_input":"2024-04-29T23:45:02.619326Z","iopub.status.idle":"2024-04-29T23:45:23.845898Z","shell.execute_reply.started":"2024-04-29T23:45:02.619285Z","shell.execute_reply":"2024-04-29T23:45:23.845083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"\n# References:\n# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\nfrom transformers import TrainerCallback\n\ndef identify_incorrect_labels(reference_df, pred_df):\n    \"\"\"\n    Identify incorrectly labeled tokens and classify them as False Negatives or False Positives.\n\n    Parameters:\n    - reference_df (DataFrame): DataFrame with the reference labels.\n    - pred_df (DataFrame): DataFrame with the predicted labels.\n\n    Returns:\n    - incorrectly_labeled (DataFrame): DataFrame with the incorrectly labeled tokens and their error types.\n    \"\"\"\n    # Drop unnecessary columns from pred_df\n    pred_df = pred_df.drop(columns=['eval_row', 'row_id'])\n\n    # Merge the DataFrames\n    merged_df = pd.merge(reference_df, pred_df, on=['document', 'token'], how='outer', suffixes=('_actual', '_pred'))\n\n    # Identify incorrectly labeled tokens\n    incorrectly_labeled = merged_df[merged_df['label_actual'] != merged_df['label_pred']].copy()\n\n    # Fill NaN values in 'label_actual' and 'label_pred' with 'O'\n    incorrectly_labeled['label_actual'] = incorrectly_labeled['label_actual'].fillna('O')\n    incorrectly_labeled['label_pred'] = incorrectly_labeled['label_pred'].fillna('O')\n\n    # Define conditions for False Negatives and False Positives\n    condition_fn = (\n        (incorrectly_labeled['label_actual'] != 'O')  &\n        ((incorrectly_labeled['label_pred'] == 'O') | (incorrectly_labeled['label_actual'] != incorrectly_labeled['label_pred']))\n    )\n    condition_fp = ((incorrectly_labeled['label_actual'] == 'O') & (incorrectly_labeled['label_pred'] != 'O'))\n\n    # Use np.select to choose between 'FN', 'FP', and None based on the conditions\n    choices = ['FN', 'FP']\n    incorrectly_labeled['error'] = np.select([condition_fn, condition_fp], choices, default=None)\n\n    return incorrectly_labeled\n\n            \ndef do_downsample(train_df, ratio):\n    '''\n        Down sample negative examples\n    '''\n    # Separate positive and negative samples\n    p = train_df[train_df['labels'].apply(lambda x: any(label != \"O\" for label in x))]\n    n = train_df[train_df['labels'].apply(lambda x: all(label == \"O\" for label in x))]\n\n    # Downsample negative samples\n    n = n.sample(int(len(n) * ratio))\n\n    # Combine positive and downsampled negative samples\n    df = pd.concat([p, n], ignore_index=True)\n    \n    return df\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n#modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(parquet_path): \n    raw_df = pd.read_parquet(parquet_path)\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token_str'] = ref_df['token']\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label', 'token_str']].copy()\n    \n    return reference_df\n\n\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        #class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:23.847136Z","iopub.execute_input":"2024-04-29T23:45:23.847842Z","iopub.status.idle":"2024-04-29T23:45:23.876074Z","shell.execute_reply.started":"2024-04-29T23:45:23.847816Z","shell.execute_reply":"2024-04-29T23:45:23.875034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length, stride):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\n        \"\".join(text),\n        return_offsets_mapping=True,\n        max_length=max_length,\n        truncation=True,\n        stride = stride,\n    ) \n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    \n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=2\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:23.877506Z","iopub.execute_input":"2024-04-29T23:45:23.877978Z","iopub.status.idle":"2024-04-29T23:45:23.913553Z","shell.execute_reply.started":"2024-04-29T23:45:23.877943Z","shell.execute_reply":"2024-04-29T23:45:23.912743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\nfrom collections import defaultdict\nfrom typing import Dict\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:23.914673Z","iopub.execute_input":"2024-04-29T23:45:23.914937Z","iopub.status.idle":"2024-04-29T23:45:23.937237Z","shell.execute_reply.started":"2024-04-29T23:45:23.914915Z","shell.execute_reply":"2024-04-29T23:45:23.936319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\nimport pickle\nimport re\nimport gc\n\n\ndef train(config = None):\n    gc.collect()\n    torch.cuda.empty_cache() #free up memory that isnt in use\n    \n    # Initialize new wandb run to run without sweep agent\n    with wandb.init(project='pii_compare2', job_type='train', config=config):\n        config = wandb.config\n        \n       # Load the training data\n        train_df = pd.read_parquet(config.train_artifact_path)\n\n        # Load the validation data\n        val_df = pd.read_parquet(config.val_artifact_path)\n        \n        # Load external data\n        for parquet_path in [config.external_data_1, config.external_data_2, config.external_data_3, config.external_data_4, config.external_data_5]:\n            if parquet_path != 'none':\n                print(f'Loading external data...')\n                ext_df = pd.read_parquet(parquet_path)\n                train_df = pd.concat([train_df, ext_df], ignore_index=True)\n                del ext_df\n        \n        wandb.log({'num_docs_train_raw': len(train_df)})\n        #down sample\n        train_df = do_downsample(train_df, config.downsample_ratio)\n        wandb.log({'num_docs_train': len(train_df)})\n        \n        # Prepare references and labels from val set\n        reference_df = get_reference_df(config.val_artifact_path)\n        all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n        label2id = {l: i for i,l in enumerate(all_labels)}\n        id2label = {v:k for k,v in label2id.items()}\n\n        # Create the training and validation datasets\n        tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n        train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id, config.stride)\n        valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id, config.stride)\n        del train_df\n        del val_df\n        gc.collect()\n\n        # Initialize the model and data collator\n        model = AutoModelForTokenClassification.from_pretrained(\n            config.training_model_path,\n            num_labels=len(all_labels),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n\n        # Define the training arguments\n        args = TrainingArguments(\n            output_dir=config.output_dir, \n            fp16=config.fp16,\n            learning_rate=config.learning_rate,\n            num_train_epochs=config.num_train_epochs,\n            per_device_train_batch_size=config.per_device_train_batch_size,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            report_to=config.report_to,\n            evaluation_strategy=config.evaluation_strategy,\n            eval_steps = config.eval_steps,\n            save_strategy = config.evaluation_strategy, #these need to be the same\n            do_eval=config.do_eval,\n            save_total_limit=config.save_total_limit,\n            logging_steps=config.logging_steps,\n            lr_scheduler_type=config.lr_scheduler_type,\n            warmup_ratio=config.warmup_ratio,\n            weight_decay=config.weight_decay,\n            load_best_model_at_end = config.load_best_model_at_end,\n            metric_for_best_model = config.metric_for_best_model ,\n            greater_is_better = config.greater_is_better,\n        )\n\n        #class weights based on dataset to go to CustomTrainer Class\n        class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n        # Initialize Trainer with custom class weights\n        trainer = CustomTrainer(\n            model=model, \n            args=args, \n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            data_collator=collator, \n            tokenizer=tokenizer,\n            compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n            class_weights=class_weights,\n        )\n\n        # Train the model\n        trainer.train()    \n\n        del train_ds\n        gc.collect()\n        torch.cuda.empty_cache() #free up memory that isnt in use\n        \n        # Make predictions on the validation dataset\n        preds = trainer.predict(valid_ds)\n\n        #theshold tests\n        print(\"doing threshold tests:\")\n        threshold_tests = [.6,.7,.8,.9,.99] #[.7,.9, 0.99] #TEMP\n        scores =[]\n        \n        for threshold in threshold_tests:\n            metrics = compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=threshold)\n            f5_score = metrics['ents_f5']\n            scores.append(f5_score)\n            wandb.log({'threshold': threshold, 'final_f5': f5_score})\n            print(f'threshold:f5 {threshold}: {f5_score}')\n\n        best_threshold = 0.0  \n        best_f5 = 0.0  \n        for thresh, score in zip(threshold_tests, scores):\n            if score > best_f5:\n                best_threshold = thresh\n                best_f5 = score\n            \n        wandb.config.best_threshold = best_threshold\n        wandb.log({'val_f5': best_f5})\n        preds_df = parse_predictions(preds.predictions, id2label, valid_ds, threshold=best_threshold)\n        \n        #make DF of errors and save to wandb\n        incorrectly_labeled = identify_incorrect_labels(reference_df, preds_df)\n        errors_table = wandb.Table(dataframe=incorrectly_labeled)\n        wandb.log({'errors_table': errors_table})\n        \n        # Save the model and upload it to Kaggle\n        os.makedirs(config.experiment, exist_ok=True)\n        trainer.save_model(config.experiment)\n        tokenizer.save_pretrained(config.experiment)\n        \n#         #pickle for testing. Open a file in binary write mode\n#         with open('arguments.pkl', 'wb') as file:\n#             # Dump all objects at once using a tuple\n#             pickle.dump((preds.predictions, id2label, valid_ds, reference_df, threshold_tests), file)\n    #best_threshold =0    \n    print('Experiment finished, test it out on the inference notebook!')\n    \n    return best_threshold","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:23.938657Z","iopub.execute_input":"2024-04-29T23:45:23.939070Z","iopub.status.idle":"2024-04-29T23:45:23.963588Z","shell.execute_reply.started":"2024-04-29T23:45:23.939047Z","shell.execute_reply":"2024-04-29T23:45:23.962766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# W and B \n","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# from google.colab import userdata\n# wandb_api_key = userdata.get('WANDB_API_KEY')\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:23.966678Z","iopub.execute_input":"2024-04-29T23:45:23.967395Z","iopub.status.idle":"2024-04-29T23:45:25.882856Z","shell.execute_reply.started":"2024-04-29T23:45:23.967366Z","shell.execute_reply":"2024-04-29T23:45:25.881963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Default Config\nsweep_config = {\n    'method': 'bayes' #grid, random, bayes\n    }\n\n#metrics for evaluation\nmetric = {\n    'name': 'val_f5',\n    'goal': 'maximize'\n    }\n\nsweep_config['metric'] = metric\n\n#intialize parameters\nparameters_dict = {\n    'experiment': {'value': 'pii_00'},\n    'threshold': {'value': 0.99},\n    'o_weight': {'value': 0.05},  # set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': 1.0},  # set to 1 for no downsample\n    'raw_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/train2_fromval.parquet'},\n    'val_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'external_data_1': {'value': 'none'},\n    'external_data_2': {'value': 'none'},\n    'external_data_3': {'value': 'none'},\n    'external_data_4': {'value': 'none'},\n    'external_data_5': {'value': 'none'},\n    'output_dir': {'value': 'output'},\n    'inference_max_length': {'value': 1024},\n    'training_max_length': {'value': 1024},\n    'stride': {'value': 0}, # set to 0 for no effect\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'fp16': {'value': True},\n    'learning_rate': {'value': 1e-5},\n    'num_train_epochs': {'value': .2},\n    'per_device_train_batch_size': {'value': 1},\n    'per_device_eval_batch_size': {'value': 1},\n    'gradient_accumulation_steps': {'value': 1},\n    'report_to': {'value': 'wandb'},\n    'evaluation_strategy': {'value': 'no'},\n    'eval_steps': {'value': 20},\n    'do_eval': {'value': False},\n    'save_total_limit': {'value': 1},\n    'logging_steps': {'value': 10},\n    'lr_scheduler_type': {'value': 'cosine'},\n    'warmup_ratio': {'value': 0.1},\n    'weight_decay': {'value': 0.01},\n    'load_best_model_at_end': {'value': False},\n    'metric_for_best_model': {'value': 'ents_f5'},\n    'greater_is_better': {'value': True},\n}\n\nsweep_config['parameters'] = parameters_dict\ntrain_config = parameters_dict","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:25.883920Z","iopub.execute_input":"2024-04-29T23:45:25.884483Z","iopub.status.idle":"2024-04-29T23:45:25.897869Z","shell.execute_reply.started":"2024-04-29T23:45:25.884459Z","shell.execute_reply":"2024-04-29T23:45:25.896837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best single model ","metadata":{}},{"cell_type":"code","source":"#base\n  \ntrain_config.update({\n'experiment': {\n    'value': f'pii_ens1_base_full'},\n'training_model_path': {'value': 'microsoft/deberta-v3-base'},\n'train_artifact_path': {'value': f'/kaggle/input/pii-bagging-datasets/artifacts/total_train.parquet'},\n'external_data_1': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n'learning_rate': {'value': 5e-5},\n'num_train_epochs': {'value': 3},\n'gradient_accumulation_steps': {'value': 1},\n'per_device_train_batch_size': {'value': 1},\n'stride': {'value': 256},\n'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n'downsample_ratio' : {'value': .75},  # set to 1 for no downsample\n'training_max_length': {'value': 1500},\n})\n\n# Extract inner values from the dictionary\nconfig = {k: v['value'] for k, v in train_config.items()}\n\n# Convert to SimpleNamespace\nconfig = SimpleNamespace(**config)\nbest_threshold = train(config)\nprint(f'Best Threshold : {best_threshold}')\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prior experiments\n","metadata":{}},{"cell_type":"code","source":"# #Ensemble loop for bags train_bag_0.parquet to train_bag_4.parquet\n\n# for i in range(1):\n    \n#     train_config.update({\n#     'experiment': {\n#         'value': 'pii_ens1_small_val'},#f'pii_ens1v2_bag{i}'},\n#     'training_model_path': {'value': 'microsoft/deberta-v3-small'},\n#     'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/docs_over_3000.parquet'}, #f'/kaggle/input/pii-bagging-datasets/artifacts/train_bag{i}.parquet'},\n#     'external_data_1': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n#     'learning_rate': {'value': 5e-5},\n#     'num_train_epochs': {'value': 3},\n#     'gradient_accumulation_steps': {'value': 2},\n#     'stride': {'value': 256},\n#     'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n#     'downsample_ratio' : {'value': .75},  # set to 1 for no downsample\n#     'training_max_length': {'value': 3000},\n#     })\n\n#     # Extract inner values from the dictionary\n#     config = {k: v['value'] for k, v in train_config.items()}\n\n#     # Convert to SimpleNamespace\n#     config = SimpleNamespace(**config)\n#     best_threshold = train(config)\n#     print(f'Best Threshold : {best_threshold}')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:25.898965Z","iopub.execute_input":"2024-04-29T23:45:25.899277Z","iopub.status.idle":"2024-04-29T23:45:26.042345Z","shell.execute_reply.started":"2024-04-29T23:45:25.899247Z","shell.execute_reply":"2024-04-29T23:45:26.041183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# base","metadata":{}},{"cell_type":"code","source":"# #base\n\n# id2label ={'0': 'B-EMAIL', '1': 'B-ID_NUM', '2': 'B-NAME_STUDENT', '3': 'B-PHONE_NUM', '4': 'B-STREET_ADDRESS', '5': 'B-URL_PERSONAL', '6': 'B-USERNAME', '7': 'I-ID_NUM', '8': 'I-NAME_STUDENT', '9': 'I-PHONE_NUM', '10': 'I-STREET_ADDRESS', '11': 'I-URL_PERSONAL',} # '12': 'O'}\n\n# for label in id2label.values():\n    \n#     train_config.update({\n#     'experiment': {\n#         'value': f'pii_ens1_{label}'},\n#     'training_model_path': {'value': 'microsoft/deberta-v3-base'},\n#     'train_artifact_path': {'value': f'/kaggle/input/pii-bagging-datasets/artifacts/by_class_{label}.parquet'},\n#     'external_data_1': {'value': 'none'},\n#     'learning_rate': {'value': 5e-5},\n#     'num_train_epochs': {'value': 2},\n#     'gradient_accumulation_steps': {'value': 1},\n#     'per_device_train_batch_size': {'value': 2},\n#     'stride': {'value': 256},\n#     'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n#     'downsample_ratio' : {'value': .75},  # set to 1 for no downsample\n#     'training_max_length': {'value': 1500},\n#     })\n\n#     # Extract inner values from the dictionary\n#     config = {k: v['value'] for k, v in train_config.items()}\n\n#     # Convert to SimpleNamespace\n#     config = SimpleNamespace(**config)\n#     best_threshold = train(config)\n#     print(f'Best Threshold : {best_threshold}')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:49:45.747057Z","iopub.execute_input":"2024-04-29T23:49:45.747445Z","iopub.status.idle":"2024-04-29T23:51:26.360386Z","shell.execute_reply.started":"2024-04-29T23:49:45.747416Z","shell.execute_reply":"2024-04-29T23:51:26.358835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# large","metadata":{}},{"cell_type":"code","source":"# #Ensemble loop for bags train_bag_0.parquet to train_bag_4.parquet\n\n# for i in range(1):\n    \n#     train_config.update({\n#     'experiment': {\n#         'value': 'pii_ens1_large_val'},#f'pii_ens1v2_bag{i}'},\n#     'training_model_path': {'value': 'microsoft/deberta-v3-large'},\n#     'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/docs_over_3000.parquet'}, #f'/kaggle/input/pii-bagging-datasets/artifacts/train_bag{i}.parquet'},\n#     'external_data_1': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n#     'learning_rate': {'value': 1e-5},\n#     'num_train_epochs': {'value': 3},\n#     'gradient_accumulation_steps': {'value': 1},\n#     'stride': {'value': 0},\n#     'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n#     'downsample_ratio' : {'value': .75},  # set to 1 for no downsample\n#     'training_max_length': {'value': 1000},\n#     })\n\n#     # Extract inner values from the dictionary\n#     config = {k: v['value'] for k, v in train_config.items()}\n\n#     # Convert to SimpleNamespace\n#     config = SimpleNamespace(**config)\n#     best_threshold = train(config)\n#     print(f'Best Threshold : {best_threshold}')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:27.073399Z","iopub.status.idle":"2024-04-29T23:45:27.073898Z","shell.execute_reply.started":"2024-04-29T23:45:27.073643Z","shell.execute_reply":"2024-04-29T23:45:27.073661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lakshyak","metadata":{}},{"cell_type":"code","source":"# #lakshyak\n\n# for i in range(1):\n    \n#     train_config.update({\n#     'experiment': {\n#         'value': 'pii_ens1_large_val'},#f'pii_ens1v2_bag{i}'},\n#     'training_model_path': {'value': 'microsoft/deberta-v3-large'},\n#     'train_artifact_path': {'value': '/kaggle/input/pii-prep-ens1/artifacts/docs_over_3000.parquet'}, #f'/kaggle/input/pii-bagging-datasets/artifacts/train_bag{i}.parquet'},\n#     'external_data_1': {'value': '/kaggle/input/pii-prep-ens1/artifacts/val2.parquet'},,\n#     'learning_rate': {'value': 5e-5},\n#     'num_train_epochs': {'value': 3},\n#     'stride': {'value': 256},\n#     'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n#     'downsample_ratio' : {'value': .75},  # set to 1 for no downsample\n#     'training_max_length': {'value': 2000},\n#     })\n\n#     # Extract inner values from the dictionary\n#     config = {k: v['value'] for k, v in train_config.items()}\n\n#     # Convert to SimpleNamespace\n#     config = SimpleNamespace(**config)\n#     best_threshold = train(config)\n#     print(f'Best Threshold : {best_threshold}')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:45:27.075164Z","iopub.status.idle":"2024-04-29T23:45:27.075594Z","shell.execute_reply.started":"2024-04-29T23:45:27.075371Z","shell.execute_reply":"2024-04-29T23:45:27.075389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sweep\n\nNote for running sweep it is recommended to not calculate and log the error table in the training script. ","metadata":{}},{"cell_type":"code","source":"# #update train parameters using dictionary so that it works with sweep\n\n# # Update 'method' in sweep_config\n# sweep_config['method'] = 'grid'\n\n# train_config.update({\n#     'experiment': {\n#         'value': 'pii_train2'},\n#     'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n#     'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/train2_fromval.parquet'},\n#     'learning_rate': {'value': 1e-4},\n#     'stride': {'value': 128},\n#     'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n#     'downsample_ratio' : {'value': 1},  # set to 1 for no downsample\n#     'inference_max_length': {'value': 1024},\n#     })\n\n# parameters_dict.update({\n#     'training_max_length': {'values': [512, 1024, 2000, 3000]},\n# })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # runs training script\n# sweep_id = wandb.sweep(sweep_config, project= 'PII_sweep2')\n# wandb.agent(sweep_id, train, count=4)","metadata":{},"execution_count":null,"outputs":[]}]}