{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8198200,"sourceType":"datasetVersion","datasetId":4856320}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\n-removed stride compared to reference\n-raw data is full competition training comes from base_data artifact\nReference:\n- https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b \n- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=eFhyArSz826Q","metadata":{}},{"cell_type":"markdown","source":"# Run Configs","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install seqeval accelerate evaluate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:28:00.549167Z","iopub.execute_input":"2024-04-22T22:28:00.549852Z","iopub.status.idle":"2024-04-22T22:28:19.475991Z","shell.execute_reply.started":"2024-04-22T22:28:00.549813Z","shell.execute_reply":"2024-04-22T22:28:19.474765Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:28:19.478175Z","iopub.execute_input":"2024-04-22T22:28:19.478970Z","iopub.status.idle":"2024-04-22T22:28:37.274999Z","shell.execute_reply.started":"2024-04-22T22:28:19.478929Z","shell.execute_reply":"2024-04-22T22:28:37.274151Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:28:37.276141Z","iopub.execute_input":"2024-04-22T22:28:37.276441Z","iopub.status.idle":"2024-04-22T22:29:07.702010Z","shell.execute_reply.started":"2024-04-22T22:28:37.276415Z","shell.execute_reply":"2024-04-22T22:29:07.701260Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-22 22:28:53.987310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 22:28:53.987418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 22:28:54.256267: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\nfrom transformers import TrainerCallback\n\ndef identify_incorrect_labels(reference_df, pred_df):\n    \"\"\"\n    Identify incorrectly labeled tokens and classify them as False Negatives or False Positives.\n\n    Parameters:\n    - reference_df (DataFrame): DataFrame with the reference labels.\n    - pred_df (DataFrame): DataFrame with the predicted labels.\n\n    Returns:\n    - incorrectly_labeled (DataFrame): DataFrame with the incorrectly labeled tokens and their error types.\n    \"\"\"\n    # Drop unnecessary columns from pred_df\n    pred_df = pred_df.drop(columns=['eval_row', 'row_id'])\n\n    # Merge the DataFrames\n    merged_df = pd.merge(reference_df, pred_df, on=['document', 'token'], how='outer', suffixes=('_actual', '_pred'))\n\n    # Identify incorrectly labeled tokens\n    incorrectly_labeled = merged_df[merged_df['label_actual'] != merged_df['label_pred']].copy()\n\n    # Fill NaN values in 'label_actual' and 'label_pred' with 'O'\n    incorrectly_labeled['label_actual'] = incorrectly_labeled['label_actual'].fillna('O')\n    incorrectly_labeled['label_pred'] = incorrectly_labeled['label_pred'].fillna('O')\n\n    # Define conditions for False Negatives and False Positives\n    condition_fn = (\n        (incorrectly_labeled['label_actual'] != 'O')  &\n        ((incorrectly_labeled['label_pred'] == 'O') | (incorrectly_labeled['label_actual'] != incorrectly_labeled['label_pred']))\n    )\n    condition_fp = ((incorrectly_labeled['label_actual'] == 'O') & (incorrectly_labeled['label_pred'] != 'O'))\n\n    # Use np.select to choose between 'FN', 'FP', and None based on the conditions\n    choices = ['FN', 'FP']\n    incorrectly_labeled['error'] = np.select([condition_fn, condition_fp], choices, default=None)\n\n    return incorrectly_labeled\n\n# #call back to log loss for sweep analysis\n# class WandbLoggingCallback(TrainerCallback):\n#     def on_log(self, args, state, control, trainer=None, **kwargs):\n#         # Log metrics to wandb\n#         if trainer is not None:\n#             logs = {}\n#             # Log loss\n#             logs[\"loss\"] = trainer.state.log_history[-1][\"loss\"]\n#             wandb.log(logs)\n            \ndef do_downsample(train_df, ratio):\n    '''\n        Down sample negative examples\n    '''\n    # Separate positive and negative samples\n    p = train_df[train_df['labels'].apply(lambda x: any(label != \"O\" for label in x))]\n    n = train_df[train_df['labels'].apply(lambda x: all(label == \"O\" for label in x))]\n\n    # Downsample negative samples\n    n = n.sample(int(len(n) * ratio))\n\n    # Combine positive and downsampled negative samples\n    df = pd.concat([p, n], ignore_index=True)\n    \n    return df\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n#CHECK- modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(parquet_path): \n    raw_df = pd.read_parquet(parquet_path)\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n    \n    return reference_df\n\n\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, train_dataloader=None, eval_dataloader=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.train_dataloader = train_dataloader\n        self.eval_dataloader = eval_dataloader\n        #class_weights is a Tensor of weights for each class, adjust for distributed training on multiple GPUs\n        self.class_weights = class_weights.to(self.model.device) if class_weights is not None else None\n\n    def get_train_dataloader(self):\n        return self.train_dataloader\n\n    def get_eval_dataloader(self, eval_dataset=None):\n        return self.eval_dataloader\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n                        #debug statements here\n            print(f\"Logits shape: {logits.shape}\")\n            print(f\"Labels shape: {labels.shape}\")\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:43:44.519534Z","iopub.execute_input":"2024-04-22T22:43:44.519917Z","iopub.status.idle":"2024-04-22T22:43:44.553021Z","shell.execute_reply.started":"2024-04-22T22:43:44.519885Z","shell.execute_reply":"2024-04-22T22:43:44.551916Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length, stride):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\n        \"\".join(text),\n        return_offsets_mapping=True,\n        max_length=max_length,\n        truncation=True,\n        stride = stride,\n    ) \n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    \n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:43:44.554935Z","iopub.execute_input":"2024-04-22T22:43:44.555252Z","iopub.status.idle":"2024-04-22T22:43:44.574342Z","shell.execute_reply.started":"2024-04-22T22:43:44.555217Z","shell.execute_reply":"2024-04-22T22:43:44.573639Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# train Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results\n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id, stride):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length,\n            \"stride\": stride,\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-22T22:43:44.575455Z","iopub.execute_input":"2024-04-22T22:43:44.575747Z","iopub.status.idle":"2024-04-22T22:43:44.602271Z","shell.execute_reply.started":"2024-04-22T22:43:44.575725Z","shell.execute_reply":"2024-04-22T22:43:44.600605Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#reference https://www.kaggle.com/discussions/getting-started/140636\n!pip install GPUtil -q\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-22T22:43:44.603784Z","iopub.execute_input":"2024-04-22T22:43:44.604246Z","iopub.status.idle":"2024-04-22T22:43:56.949368Z","shell.execute_reply.started":"2024-04-22T22:43:44.604214Z","shell.execute_reply":"2024-04-22T22:43:56.948087Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator, DataLoaderConfiguration\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\nimport pickle\nimport re\n\n\ndef train(config = None):\n    free_gpu_cache()  #free up memory that isnt in use\n    \n    # Initialize new wandb run to run without sweep agent\n    with wandb.init(project='piiV2', job_type='train', config=config):\n        config = wandb.config\n        \n       # Load the training data\n        train_df = pd.read_parquet(config.train_artifact_path)\n\n        # Load the validation data\n        val_df = pd.read_parquet(config.val_artifact_path)\n        \n        # Load external data\n        for parquet_path in [config.external_data_1, config.external_data_2, config.external_data_3, config.external_data_4, config.external_data_5]:\n            if parquet_path != 'none':\n                print(f'Loading external data...')\n                ext_df = pd.read_parquet(parquet_path)\n                train_df = pd.concat([train_df, ext_df], ignore_index=True)\n        \n        wandb.log({'num_docs_train_raw': len(train_df)})\n        #down sample\n        train_df = do_downsample(train_df, config.downsample_ratio)\n        \n        wandb.log({'num_docs_train': len(train_df)})\n        \n        # Prepare references and labels from val set\n        reference_df = get_reference_df(config.val_artifact_path)\n        all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n        label2id = {l: i for i,l in enumerate(all_labels)}\n        id2label = {v:k for k,v in label2id.items()}\n\n        # Create the training and validation datasets\n        tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n        train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id, config.stride)\n        valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id, config.stride)\n    \n        \n        #ADD\n        # Initialize the Accelerator\n        accelerator = Accelerator()\n        \n        # Initialize the model and data collator\n        model = AutoModelForTokenClassification.from_pretrained(\n            config.training_model_path,\n            num_labels=len(all_labels),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        #ADD\n        optimizer = torch.optim.Adam(model.parameters())\n        # Use the Accelerator to prepare the model, optimizer, and data loader\n        model, optimizer = accelerator.prepare(model, optimizer)\n        \n        \n        collator = DataCollatorForTokenClassification(tokenizer,padding=True, pad_to_multiple_of=16)\n        \n        # Create DataLoaders for training and validation datasets\n        train_loader = DataLoader(train_ds, batch_size=config.per_device_train_batch_size,shuffle=True, collate_fn=collator)\n        valid_loader = DataLoader(valid_ds, batch_size=config.per_device_eval_batch_size,shuffle=True, collate_fn=collator)\n        train_loader = accelerator.prepare(train_loader)\n        valid_loader = accelerator.prepare(valid_loader)\n       \n    \n        # Define the training arguments\n        args = TrainingArguments(\n            output_dir=config.output_dir, \n            fp16=config.fp16,\n            learning_rate=config.learning_rate,\n            num_train_epochs=config.num_train_epochs,\n            per_device_train_batch_size=config.per_device_train_batch_size,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            report_to=config.report_to,\n            evaluation_strategy=config.evaluation_strategy,\n            eval_steps = config.eval_steps,\n            save_strategy = config.evaluation_strategy, #these need to be the same\n            do_eval=config.do_eval,\n            save_total_limit=config.save_total_limit,\n            logging_steps=config.logging_steps,\n            lr_scheduler_type=config.lr_scheduler_type,\n            warmup_ratio=config.warmup_ratio,\n            weight_decay=config.weight_decay,\n            load_best_model_at_end = config.load_best_model_at_end,\n            metric_for_best_model = config.metric_for_best_model ,\n            greater_is_better = config.greater_is_better,\n            max_steps=int(config.num_train_epochs * (len(train_ds) / config.per_device_train_batch_size)),  # #docs/batch_size * epochs\n        )\n\n        #class weights based on dataset to go to CustomTrainer Class\n        class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n        # Initialize Trainer with custom class weights\n        trainer = CustomTrainer(\n            model=model, \n            args=args, \n            train_dataloader=train_loader,  #ADD pass DataLoader instead of Dataset to adjusted CustomTrainer\n            eval_dataloader=valid_loader,#ADD pass DataLoader instead of Dataset\n            data_collator=collator, \n            tokenizer=tokenizer,\n            compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n            class_weights=class_weights,\n        )\n\n        # Train the model\n        trainer.train()    \n\n        # Make predictions on the validation dataset\n        preds = trainer.predict(valid_ds)\n\n        #theshold tests\n        print(\"doing threshold tests:\")\n        threshold_tests = [.7, 0.99] #TEMP\n        scores =[]\n        \n        for threshold in threshold_tests:\n            metrics = compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=threshold)\n            f5_score = metrics['ents_f5']\n            scores.append(f5_score)\n            wandb.log({'threshold': threshold, 'final_f5': f5_score})\n            print(f'threshold:f5 {threshold}: {f5_score}')\n\n        best_threshold = 0.0  \n        best_f5 = 0.0  \n        for thresh, score in zip(threshold_tests, scores):\n            if score > best_f5:\n                best_threshold = thresh\n                best_f5 = score\n            \n        wandb.config.best_threshold = best_threshold\n        preds_df = parse_predictions(preds.predictions, id2label, valid_ds, threshold=best_threshold)\n        \n        #make DF of errors and save to wandb\n#TEMP\n#         incorrectly_labeled = identify_incorrect_labels(reference_df, preds_df)\n#         errors_table = wandb.Table(dataframe=incorrectly_labeled)\n#         wandb.log({'errors_table': errors_table})\n        \n        # Save the model and upload it to Kaggle\n        os.makedirs(config.experiment, exist_ok=True)\n        trainer.save_model(config.experiment)\n        tokenizer.save_pretrained(config.experiment)\n        print('Experiment finished, test it out on the inference notebook!')\n    \n    return best_threshold","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.182897Z","iopub.execute_input":"2024-04-22T23:03:28.183317Z","iopub.status.idle":"2024-04-22T23:03:28.210622Z","shell.execute_reply.started":"2024-04-22T23:03:28.183273Z","shell.execute_reply":"2024-04-22T23:03:28.209533Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# W and B \n- login\n- default config\n- update config\n- train","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.212552Z","iopub.execute_input":"2024-04-22T23:03:28.212900Z","iopub.status.idle":"2024-04-22T23:03:28.444085Z","shell.execute_reply.started":"2024-04-22T23:03:28.212870Z","shell.execute_reply":"2024-04-22T23:03:28.443221Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#Default Config\nsweep_config = {\n    'method': 'bayes' #grid, random, bayes\n    }\n\n#metrics for evaluation\nmetric = {\n    'name': 'loss',\n    'goal': 'minimize'   \n    }\n\nsweep_config['metric'] = metric\n\n#intialize parameters \nparameters_dict = {\n    'experiment': {'value': 'pii_00'},\n    'threshold': {'value': 0.99},\n    'o_weight': {'value': 0.05},  # set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': 1.0},  # set to 1 for no downsample\n    'raw_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/train2_fromval.parquet'},\n    'val_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/val2.parquet'},\n    'external_data_1': {'value': 'none'},\n    'external_data_2': {'value': 'none'},\n    'external_data_3': {'value': 'none'},\n    'external_data_4': {'value': 'none'},\n    'external_data_5': {'value': 'none'},\n    'output_dir': {'value': 'output'},\n    'inference_max_length': {'value': 1024},\n    'training_max_length': {'value': 1024},\n    'stride': {'value': 0}, # set to 0 for no effect\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'fp16': {'value': True},\n    'learning_rate': {'value': 1.5e-5},\n    'num_train_epochs': {'value': 3},\n    'per_device_train_batch_size': {'value': 2},\n    'per_device_eval_batch_size': {'value': 2},\n    'gradient_accumulation_steps': {'value': 3},\n    'report_to': {'value': 'wandb'},\n    'evaluation_strategy': {'value': 'epoch'},\n    'eval_steps': {'value': 20},\n    'do_eval': {'value': False},\n    'save_total_limit': {'value': 2},\n    'logging_steps': {'value': 10},\n    'lr_scheduler_type': {'value': 'cosine'},\n    'warmup_ratio': {'value': 0.1},\n    'weight_decay': {'value': 0.01},\n    'load_best_model_at_end': {'value': True},\n    'metric_for_best_model': {'value': 'ents_f5'},\n    'greater_is_better': {'value': True},\n}\n    \nsweep_config['parameters'] = parameters_dict\ntrain_config = parameters_dict","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.445254Z","iopub.execute_input":"2024-04-22T23:03:28.445610Z","iopub.status.idle":"2024-04-22T23:03:28.456467Z","shell.execute_reply.started":"2024-04-22T23:03:28.445579Z","shell.execute_reply":"2024-04-22T23:03:28.455599Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#update train parameters using dictionary so that it works with sweep\n\ntrain_config.update({\n    'experiment': {\n        'value': 'pii_data_train2'},\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/train2_fromval.parquet'},\n    'learning_rate': {'value': 1.5e-5},\n    'stride': {'value': 126},\n    'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': .1},  # set to 1 for no downsample\n    'inference_max_length': {'value': 3500},\n    'training_max_length': {'value': 3500},\n    })","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.458669Z","iopub.execute_input":"2024-04-22T23:03:28.459241Z","iopub.status.idle":"2024-04-22T23:03:28.474455Z","shell.execute_reply.started":"2024-04-22T23:03:28.459210Z","shell.execute_reply":"2024-04-22T23:03:28.473766Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# #nested dictionary of parameters interested in and method we are trying\n# import pprint\n# pprint.pprint(train_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.475465Z","iopub.execute_input":"2024-04-22T23:03:28.475757Z","iopub.status.idle":"2024-04-22T23:03:28.490972Z","shell.execute_reply.started":"2024-04-22T23:03:28.475734Z","shell.execute_reply":"2024-04-22T23:03:28.490102Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# runs training script\n\n# Extract inner values from the dictionary\nconfig = {k: v['value'] for k, v in train_config.items()}\n\n# Convert to SimpleNamespace\nconfig = SimpleNamespace(**config)\nbest_threshold = train(config)\nprint(f'Best Threshold : {best_threshold}')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:28.491998Z","iopub.execute_input":"2024-04-22T23:03:28.493102Z","iopub.status.idle":"2024-04-22T23:04:28.259520Z","shell.execute_reply.started":"2024-04-22T23:03:28.493069Z","shell.execute_reply":"2024-04-22T23:04:28.258034Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Initial GPU Usage\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  4% |\n|  1 |  0% |  0% |\nGPU Usage after emptying the cache\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  4% |\n|  1 |  0% |  0% |\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240422_230328-ru06a90k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/piiV2/runs/ru06a90k' target=\"_blank\">rare-pine-51</a></strong> to <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/piiV2/runs/ru06a90k' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2/runs/ru06a90k</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349cafeddb864c269fafc14a210a53cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3406 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8035867c1b264636a9eb67a462119d0a"}},"metadata":{}},{"name":"stdout","text":"Type of first item in training dataset: {'full_text': <class 'str'>, 'document': <class 'int'>, 'tokens': <class 'list'>, 'trailing_whitespace': <class 'list'>, 'provided_labels': <class 'list'>, 'token_indices': <class 'list'>, 'input_ids': <class 'list'>, 'token_type_ids': <class 'list'>, 'attention_mask': <class 'list'>, 'offset_mapping': <class 'list'>, 'labels': <class 'list'>, 'length': <class 'int'>, 'token_map': <class 'list'>}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_34/3275137807.py\", line 125, in train\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1780, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2085, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py\", line 452, in __iter__\n    current_batch = next(dataloader_iter)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n    return self.torch_call(features)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 333, in torch_call\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3369, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`full_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>num_docs_train</td><td>▁</td></tr><tr><td>num_docs_train_raw</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>num_docs_train</td><td>750</td></tr><tr><td>num_docs_train_raw</td><td>3401</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rare-pine-51</strong> at: <a href='https://wandb.ai/csci566sp24/piiV2/runs/ru06a90k' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2/runs/ru06a90k</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/piiV2' target=\"_blank\">https://wandb.ai/csci566sp24/piiV2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240422_230328-ru06a90k/logs</code>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:759\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 759\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[63], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert to SimpleNamespace\u001b[39;00m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m SimpleNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m----> 8\u001b[0m best_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Threshold : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[58], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    113\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    114\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m    115\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation dataset\u001b[39;00m\n\u001b[1;32m    128\u001b[0m preds \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(valid_ds)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2086\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:333\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    329\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 333\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3369\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3366\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3367\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:224\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    220\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    771\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    772\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    778\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`full_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."],"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`full_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error"}]},{"cell_type":"markdown","source":"# Additional runs\n- move helper functions to seperate script","metadata":{}},{"cell_type":"code","source":"train_config.update({\n    'experiment': {\n        'value': 'pii_data_total_train'},\n    'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n    'train_artifact_path': {'value': '/kaggle/input/pii-bagging-datasets/artifacts/total_train.parquet'},\n    'external_data_1': {'value': 'none'},\n    'learning_rate': {'value': 1.5e-5},\n    'stride': {'value': 126},\n    'o_weight': {'value': .76}, #set to 1 for equal weight for classes\n    'downsample_ratio' : {'value': 1},  # set to 1 for no downsample\n    'inference_max_length': {'value': 3500},\n    'training_max_length': {'value': 3500},\n    })\n\n# Extract inner values from the dictionary\nconfig = {k: v['value'] for k, v in train_config.items()}\n\n# Convert to SimpleNamespace\nconfig = SimpleNamespace(**config)\nbest_threshold = train(config)\nprint(f'Best Threshold : {best_threshold}')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:04:28.260582Z","iopub.status.idle":"2024-04-22T23:04:28.261333Z","shell.execute_reply.started":"2024-04-22T23:04:28.261127Z","shell.execute_reply":"2024-04-22T23:04:28.261146Z"},"trusted":true},"execution_count":null,"outputs":[]}]}