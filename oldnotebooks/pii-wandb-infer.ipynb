{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":172239162,"sourceType":"kernelVersion"},{"sourceId":171708141,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Inference\nREference https://www.kaggle.com/code/thedrcat/pii-data-detection-infer-with-w-b","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport json\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:08.215753Z","iopub.execute_input":"2024-04-10T19:36:08.216060Z","iopub.status.idle":"2024-04-10T19:36:08.226834Z","shell.execute_reply.started":"2024-04-10T19:36:08.216035Z","shell.execute_reply":"2024-04-10T19:36:08.225934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"INFERENCE_MODEL_PATH = '/kaggle/input/pii-wandb-sweep/pii_sweep'\nDATA_PATH = '../input/pii-detection-removal-from-educational-data'\nVAL_PATH = '/kaggle/input/pii-wandb-prep/val.json'\n\ntrain_config_json = '/kaggle/input/train-fork-of-pii-wandb-sweep/pii009/config.json'\nOUTPUT_DIR = \"/kaggle/working/\"\n\n# Load the configuration\nwith open(train_config_json, 'r') as f:\n    config = json.load(f)\n    \nINFERENCE_MAX_LENGTH = config['inference_max_length']\nTHRESHOLD=config['best_threshold']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:38:15.477711Z","iopub.execute_input":"2024-04-10T19:38:15.478617Z","iopub.status.idle":"2024-04-10T19:38:15.483250Z","shell.execute_reply.started":"2024-04-10T19:38:15.478582Z","shell.execute_reply":"2024-04-10T19:38:15.482237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"def add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:08.227821Z","iopub.execute_input":"2024-04-10T19:36:08.228098Z","iopub.status.idle":"2024-04-10T19:36:08.238619Z","shell.execute_reply.started":"2024-04-10T19:36:08.228075Z","shell.execute_reply":"2024-04-10T19:36:08.237771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# helpers","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From Training Helpers\n","metadata":{}},{"cell_type":"code","source":"def infer_tokenize(example, tokenizer):\n    \"\"\"\n    Tokenize an example for NER using the given tokenizer.\n\n    Args:\n        example (dict): A dictionary containing \"tokens\" and \"trailing_whitespace\" lists.\n            - \"tokens\": A list of token strings.\n            - \"trailing_whitespace\": A list of boolean values indicating whether each token has trailing whitespace.\n        tokenizer: The tokenizer to use for tokenization.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: A dictionary containing tokenized output, including offsets mapping and token map.\n            - \"input_ids\": List of token IDs.\n            - \"attention_mask\": List of attention mask values.\n            - \"offset_mapping\": List of character offsets for each token.\n            - \"token_map\": List mapping each input token to its original position in the example.\n            \n    Reference: https://www.kaggle.com/code/valentinwerner/893-deberta3base-Inference\n    \"\"\"\n    #empty list to store text and tokens in respective map\n    text = []\n    token_map = []\n    \n    #keep track of tokens\n    idx = 0\n    \n    #for the example go through tokens and whitespace\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        #add token to text\n        text.append(t)\n        #extend token length number of idx\n        token_map.extend([idx]*len(t))\n        #for whitespace add a space to text and label -1 in token map\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    #Tokenize the text and return offset mapping with the token map    \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    length = len(tokenized.input_ids)\n        \n    return {\n        **tokenized,\n        \"length\": length,\n        \"token_map\": token_map,\n    }\n\ndef create_dataset(data, tokenizer, max_length):\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n    ds = ds.map(\n        infer_tokenize,\n        fn_kwargs={\"tokenizer\": tokenizer,\n                   # \"max_length\": max_length #CHECK\n                  }, \n        num_proc=3\n    )\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:08.239889Z","iopub.execute_input":"2024-04-10T19:36:08.240151Z","iopub.status.idle":"2024-04-10T19:36:08.253826Z","shell.execute_reply.started":"2024-04-10T19:36:08.240129Z","shell.execute_reply":"2024-04-10T19:36:08.252996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and predict","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(INFERENCE_MODEL_PATH)\nmodel = AutoModelForTokenClassification.from_pretrained(INFERENCE_MODEL_PATH)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n# Load id2label configuration from model\nconfig = json.load(open(INFERENCE_MODEL_PATH + \"/config.json\"))\nid2label = config[\"id2label\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:08.256584Z","iopub.execute_input":"2024-04-10T19:36:08.256966Z","iopub.status.idle":"2024-04-10T19:36:08.715041Z","shell.execute_reply.started":"2024-04-10T19:36:08.256936Z","shell.execute_reply":"2024-04-10T19:36:08.714191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\n\ntest_data = json.load(open(DATA_PATH + \"/test.json\"))\nsub_df = pd.DataFrame(test_data)\n\nsub_df['token_indices'] = sub_df['tokens'].apply(add_token_indices)\nsub_ds = create_dataset(sub_df, tokenizer, INFERENCE_MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:08.716001Z","iopub.execute_input":"2024-04-10T19:36:08.716285Z","iopub.status.idle":"2024-04-10T19:36:09.717554Z","shell.execute_reply.started":"2024-04-10T19:36:08.716262Z","shell.execute_reply":"2024-04-10T19:36:09.716428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\n#CHECK no training_args\ntrainer = Trainer(\n    model=model, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\n\npreds = trainer.predict(sub_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:09.719246Z","iopub.execute_input":"2024-04-10T19:36:09.720021Z","iopub.status.idle":"2024-04-10T19:36:10.219875Z","shell.execute_reply.started":"2024-04-10T19:36:09.719975Z","shell.execute_reply":"2024-04-10T19:36:10.218957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Process","metadata":{}},{"cell_type":"code","source":"#helper\n#note need to update softmax\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Apply softmax to the predictions\n    pred_softmax = softmax(predictions, axis=2)\n    preds = pred_softmax.argmax(-1)\n    # Get the index of the maximum value along the last axis for the predictions without the 'O' label\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    # Get the softmax values for the 'O' label\n    O_preds = pred_softmax[:,:,12]\n    # Use the threshold to decide whether to use the original predictions or the predictions without the 'O' label\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.221320Z","iopub.execute_input":"2024-04-10T19:36:10.221817Z","iopub.status.idle":"2024-04-10T19:36:10.236259Z","shell.execute_reply.started":"2024-04-10T19:36:10.221782Z","shell.execute_reply":"2024-04-10T19:36:10.235303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label.keys()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.237604Z","iopub.execute_input":"2024-04-10T19:36:10.237941Z","iopub.status.idle":"2024-04-10T19:36:10.252487Z","shell.execute_reply.started":"2024-04-10T19:36:10.237910Z","shell.execute_reply":"2024-04-10T19:36:10.251555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CHECK why preds.predictions\npreds_df = parse_predictions(preds.predictions, id2label, sub_ds, threshold=THRESHOLD)\n\n#look at to see\ndisplay(preds_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.253580Z","iopub.execute_input":"2024-04-10T19:36:10.254247Z","iopub.status.idle":"2024-04-10T19:36:10.354646Z","shell.execute_reply.started":"2024-04-10T19:36:10.254219Z","shell.execute_reply":"2024-04-10T19:36:10.353794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.355850Z","iopub.execute_input":"2024-04-10T19:36:10.356167Z","iopub.status.idle":"2024-04-10T19:36:10.363177Z","shell.execute_reply.started":"2024-04-10T19:36:10.356135Z","shell.execute_reply":"2024-04-10T19:36:10.362273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test other F5 validation checks\n- move helper functions to seperate script","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef determine_metrics_preds_gt(preds_final, gt_ds, id2label, beta=5):\n    \"\"\"\n    Create a DataFrame of submission information.\n\n    Parameters:\n    - preds_final (list): List of predictions.\n    - id2label (dict): Dictionary mapping label IDs to labels.\n    - gt_ds (Dataset): Dataset containing the token maps, offset mappings, tokens, gt_labels, and documents.\n\n    Returns:\n    - DataFrame: DataFrame containing the submission information.\n    \"\"\"\n    # Create lists of submission information\n    triplets = []\n    document, token, p_label, gt_label, token_str, compare = [], [], [], [], [], []\n\n    for p, gt_labels, token_map, offsets, tokens, doc in zip(preds_final, gt_ds[\"labels\"], gt_ds[\"token_map\"], gt_ds[\"offset_mapping\"], gt_ds[\"tokens\"], gt_ds[\"document\"]):\n        # Iterate through each label and its offset\n        for label_pred, label_gt, (start_idx, end_idx) in zip(p, gt_labels, offsets):\n            label_pred = id2label[str(label_pred)]  # Predicted label\n\n            if start_idx + end_idx == 0: continue   # For special token or padding token\n\n            if token_map[start_idx] == -1:  # Label is for whitespace so go to next\n                start_idx += 1\n\n            # Ignore leading whitespace token \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n            \n            # Break if index exceeds the length of token mapping\n            if start_idx >= len(token_map): break\n            \n            token_id = token_map[start_idx]  # Token ID at start of index\n\n            # Ignore \"O\" labels and whitespace labels\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n                \n                # If the ground truth label is missing, it's a false positive\n                if label_gt == \"O\" and label_pred !=\"O\": match = \"FP\"\n                    \n                # If the predicted label is missing or doesn't match the ground truth, it's a false negative\n                elif (label_pred is None) or (label_gt != label_pred) : match = \"FN\"\n    \n                # If the ground truth label is missing, it's a false positive\n                elif label_gt == \"O\" and label_pred !=\"O\": match = \"FP\"\n\n                # If the predicted label matches the ground truth, it's a true positive\n                elif label_gt == label_pred : match = \"TP\"\n                    \n                else: match = \"?\"\n\n                # Add triplet if not in list of triplets\n                if triplet not in triplets:\n                    document.append(doc)\n                    token.append(token_id)\n                    p_label.append(label_pred)\n                    gt_label.append(label_gt)\n                    token_str.append(tokens[token_id])\n                    compare.append(match)\n                    triplets.append(triplet)\n\n    # Create a DataFrame of submission information\n    df = pd.DataFrame({\n        \"document\": document,\n        \"token\": token,\n        \"pred_label\": p_label,\n        \"gt_label\": gt_label,\n        \"token_str\": token_str,\n        \"compare\": compare\n    })\n    \n    # Count the number of false positives, false negatives, and true positives\n    FP = (df['compare'] == \"FP\").sum()\n    FN = (df['compare'] == \"FN\").sum()\n    TP = (df['compare'] == \"TP\").sum()\n    \n    # Calculate the precision, recall, and F-beta score\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    fbeta_mircro_score = (1 + (beta**2)) * precision * recall / (((beta**2) * precision) + recall) if precision + recall > 0 else 0\n    \n    # Print the precision, recall, and F-beta score\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F-beta score: {fbeta_mircro_score}\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.364796Z","iopub.execute_input":"2024-04-10T19:36:10.365157Z","iopub.status.idle":"2024-04-10T19:36:10.383458Z","shell.execute_reply.started":"2024-04-10T19:36:10.365126Z","shell.execute_reply":"2024-04-10T19:36:10.382688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test Metrics\ndef pii_metrics_score(pred_df, gt_df, beta=5):\n    \"\"\"\n    Calculate print the Precision, Recall and Micro F-beta score for predicted PII labels. Determines which were false negatives\n\n    Parameters:\n    - pred_df (DataFrame): DataFrame containing predicted PII labels [\"row_id\", \"document\", \"token\", \"label\"].\n    - gt_df (DataFrame): DataFrame containing ground truth PII labels [\"row_id\", \"document\", \"token\", \"label\"].\n    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n\n    Returns:\n    - results (dict): Dictionary containing the precision, recall, and F-beta score.\n    \"\"\"   \n    # Merge the predicted and ground truth DataFrames on 'document' and 'token' columns\n    df = pred_df.merge(gt_df, how='outer', on=['document', \"token\"], suffixes=('_pred', '_gt'))\n\n    # Initialize a new column 'compare' with empty strings\n    df['compare'] = \"\"\n\n    # If the predicted label is missing or doesn't match the ground truth, it's a false negative\n    df.loc[df.label_pred.isna() | (df.label_gt != df.label_pred), 'compare'] = \"FN\"\n    \n    # If the ground truth label is missing, it's a false positive\n    df.loc[df.label_gt.isna(), 'compare'] = \"FP\"\n\n    # If the predicted label matches the ground truth, it's a true positive\n    df.loc[(df.label_pred.notna()) & (df.label_gt == df.label_pred), 'compare'] = \"TP\"\n    \n    # Count the number of false positives, false negatives, and true positives\n    FP = (df['compare'] == \"FP\").sum()\n    FN = (df['compare'] == \"FN\").sum()\n    TP = (df['compare'] == \"TP\").sum()\n\n\n    # Calculate the precision, recall, and F-beta score\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    fbeta_mircro_score = (1 + (beta**2)) * precision * recall / (((beta**2) * precision) + recall) if precision + recall > 0 else 0\n\n    # Get a DataFrame of false negatives\n    fn_df = df.loc[df.label_pred.isna() | (df.label_gt != df.label_pred)]\n    \n    # Print the precision, recall, and F-beta score\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F-beta score: {fbeta_mircro_score}\")\n    \n    return fn_df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.384687Z","iopub.execute_input":"2024-04-10T19:36:10.385045Z","iopub.status.idle":"2024-04-10T19:36:10.398503Z","shell.execute_reply.started":"2024-04-10T19:36:10.385021Z","shell.execute_reply":"2024-04-10T19:36:10.397598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"submission sample metrics:\")\npred_df = pd.read_csv('/kaggle/working/submission.csv')\ngt_df= pd.read_csv('/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv')\nfn_df = pii_metrics_score(pred_df, gt_df, beta=5)\n\nfn_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.401561Z","iopub.execute_input":"2024-04-10T19:36:10.401877Z","iopub.status.idle":"2024-04-10T19:36:10.433207Z","shell.execute_reply.started":"2024-04-10T19:36:10.401853Z","shell.execute_reply":"2024-04-10T19:36:10.432222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data = json.load(open('/kaggle/input/pii-wandb-prep/val.json'))\nval_df = pd.DataFrame(val_data)\n\nprint(val_df.columns)\n\nval_df['token_indices'] = val_df['tokens'].apply(add_token_indices)\n\nds = Dataset.from_pandas(val_df)\nval_gt_ds = ds.map(\n        infer_tokenize,\n        fn_kwargs={\"tokenizer\": tokenizer,}, \n        num_proc=3\n    )\n\nval_ds = val_gt_ds.remove_columns('labels')\n\npreds = trainer.predict(val_ds)\n\npredictions = preds.predictions\nthreshold =THRESHOLD\n# Apply softmax to the predictions\npred_softmax = softmax(predictions, axis=2)\npreds = pred_softmax.argmax(-1)\n# Get the index of the maximum value along the last axis for the predictions without the 'O' label\npreds_without_O = pred_softmax[:,:,:12].argmax(-1)\n# Get the softmax values for the 'O' label\nO_preds = pred_softmax[:,:,12]\n# Use the threshold to decide whether to use the original predictions or the predictions without the 'O' label\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\nprint(\"validation sample metrics:\")\n\nval_compare_df= determine_metrics_preds_gt(preds_final, val_gt_ds, id2label)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:40:47.118357Z","iopub.execute_input":"2024-04-10T19:40:47.118789Z","iopub.status.idle":"2024-04-10T19:41:20.412594Z","shell.execute_reply.started":"2024-04-10T19:40:47.118756Z","shell.execute_reply":"2024-04-10T19:41:20.411579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of false positives, false negatives, and true positives\nprint(\"FP: \", (val_compare_df['compare'] == \"FP\").sum()) \nprint(\"FN: \", (val_compare_df['compare'] == \"FN\").sum()) \nprint(\"TP: \", (val_compare_df['compare'] == \"TP\").sum())\nprint(\"?: \", (val_compare_df['compare'] == \"?\").sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:39:15.922204Z","iopub.execute_input":"2024-04-10T19:39:15.922992Z","iopub.status.idle":"2024-04-10T19:39:15.931264Z","shell.execute_reply.started":"2024-04-10T19:39:15.922958Z","shell.execute_reply":"2024-04-10T19:39:15.930279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_compare_df[val_compare_df['compare'] == \"FP\"].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:36:10.513725Z","iopub.status.idle":"2024-04-10T19:36:10.514079Z","shell.execute_reply.started":"2024-04-10T19:36:10.513921Z","shell.execute_reply":"2024-04-10T19:36:10.513935Z"},"trusted":true},"execution_count":null,"outputs":[]}]}