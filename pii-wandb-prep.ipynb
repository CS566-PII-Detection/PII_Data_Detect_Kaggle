{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII Data Prep with W and B\n\n## Reference\nhttps://www.kaggle.com/code/thedrcat/pii-data-preparation-cv-stride-viz-and-w-b#Share-your-findings","metadata":{}},{"cell_type":"markdown","source":"# Config and Import","metadata":{}},{"cell_type":"code","source":"!pip install wandb -q","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:40:50.309699Z","iopub.execute_input":"2024-04-10T18:40:50.311005Z","iopub.status.idle":"2024-04-10T18:41:04.189747Z","shell.execute_reply.started":"2024-04-10T18:40:50.310943Z","shell.execute_reply":"2024-04-10T18:41:04.187785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T18:41:04.192637Z","iopub.execute_input":"2024-04-10T18:41:04.193033Z","iopub.status.idle":"2024-04-10T18:41:05.268630Z","shell.execute_reply.started":"2024-04-10T18:41:04.192995Z","shell.execute_reply":"2024-04-10T18:41:05.267404Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = '../input/pii-detection-removal-from-educational-data'\nOUTPUT_DIR = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:05.276009Z","iopub.execute_input":"2024-04-10T18:41:05.276507Z","iopub.status.idle":"2024-04-10T18:41:05.284463Z","shell.execute_reply.started":"2024-04-10T18:41:05.276467Z","shell.execute_reply":"2024-04-10T18:41:05.283246Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"#split data into training and validation set\ndef PIId2_split_by_sampling(df, test_ratio):\n    \"\"\"Split PII data into training and test set. \n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        test_ratio (float, default=0.2): \n            The proportion of the dataset to include in the test split.\n\n    Returns:\n        train_df, test_df: Training and test splits of the input DataFrame.\n    \"\"\"\n    # Get the number of rows in the DataFrame\n    num_rows = len(df)\n    \n    # Get the split index\n    split_idx = int(num_rows * test_ratio)\n    \n    # Shuffle the DataFrame rows\n    df_shuffled = df.sample(frac=1, random_state=42)\n    \n    # Split the DataFrame into train and test\n    train_df = df_shuffled.iloc[split_idx:]\n    test_df = df_shuffled.iloc[:split_idx]\n    \n#     #TEMP to test\n#     # Add a new column 'valid' with boolean values - need to modify the move missing if want to keep\n#     train_df['valid'] = False\n#     test_df['valid'] = True\n    \n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:05.286117Z","iopub.execute_input":"2024-04-10T18:41:05.286538Z","iopub.status.idle":"2024-04-10T18:41:05.300818Z","shell.execute_reply.started":"2024-04-10T18:41:05.286501Z","shell.execute_reply":"2024-04-10T18:41:05.299171Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def move_missing_labels(to_df, from_df):\n    \"\"\"\n    Move documents containing missing labels from the extra DataFrame to the desired DataFrame.\n\n    Parameters:\n    - to_df (DataFrame): DataFrame where the documents should be moved to.\n    - from_df (DataFrame): DataFrame where the documents should be moved from.\n\n    Returns:\n    - desired_df, extra_df: Updated DataFrames.\n    \"\"\"\n    # Find the labels that are missing from the to_df DataFrame\n    missing_labels = set(from_df['labels'].explode().unique()) - set(to_df['labels'].explode().unique())\n\n    # For each missing label, move a document from the extra DataFrame to the desired DataFrame\n    for label in missing_labels:\n        if label in from_df['labels'].explode().unique():\n            document = from_df[from_df['labels'].apply(lambda x: label in x)].sample(n=1)\n            to_df = pd.concat([to_df, document])\n            from_df = from_df.drop(document.index)\n\n    return to_df, from_df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:05.302527Z","iopub.execute_input":"2024-04-10T18:41:05.302892Z","iopub.status.idle":"2024-04-10T18:41:05.317691Z","shell.execute_reply.started":"2024-04-10T18:41:05.302860Z","shell.execute_reply":"2024-04-10T18:41:05.316307Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Validation set and Miniset","metadata":{}},{"cell_type":"code","source":"comp_train= json.load(open(DATA_PATH +'/train.json'))\n\n#Convert to df for EDA\nbase_df= pd.DataFrame(comp_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:05.319148Z","iopub.execute_input":"2024-04-10T18:41:05.319602Z","iopub.status.idle":"2024-04-10T18:41:07.922381Z","shell.execute_reply.started":"2024-04-10T18:41:05.319565Z","shell.execute_reply":"2024-04-10T18:41:07.921115Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#TODO see if can remove since only needed for stride, \n#note if remove adjust create_dataset in https://www.kaggle.com/code/jonathankasprisin/pii-wandb-training/edit\ndef add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices\n\nbase_df['token_indices'] = base_df['tokens'].apply(add_token_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:07.925368Z","iopub.execute_input":"2024-04-10T18:41:07.926649Z","iopub.status.idle":"2024-04-10T18:41:08.185047Z","shell.execute_reply.started":"2024-04-10T18:41:07.926599Z","shell.execute_reply":"2024-04-10T18:41:08.183934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = PIId2_split_by_sampling(base_df, .1)\ntrain_df_overlap = train_df\n\nval_df, train_df = move_missing_labels(val_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:08.186844Z","iopub.execute_input":"2024-04-10T18:41:08.187282Z","iopub.status.idle":"2024-04-10T18:41:12.309499Z","shell.execute_reply.started":"2024-04-10T18:41:08.187244Z","shell.execute_reply":"2024-04-10T18:41:12.308176Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame to a JSON file \nval_df.to_json(OUTPUT_DIR + \"/val.json\", orient=\"records\")\ntrain_df.to_json(OUTPUT_DIR + \"/train_df_fromval.json\", orient=\"records\")\ntrain_df_overlap.to_json(OUTPUT_DIR + \"/train_df_overlap_fromval.json\", orient=\"records\")\n\nprint(\"size of base: \", len(base_df))\nprint(\"size of val_df: \", len(val_df))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:12.313267Z","iopub.execute_input":"2024-04-10T18:41:12.314678Z","iopub.status.idle":"2024-04-10T18:41:15.675723Z","shell.execute_reply.started":"2024-04-10T18:41:12.314627Z","shell.execute_reply":"2024-04-10T18:41:15.673720Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"size of base:  6807\nsize of val_df:  688\n","output_type":"stream"}]},{"cell_type":"code","source":"mini_overlap, overlap_extra = PIId2_split_by_sampling(train_df_overlap, .7)\nmini_no_overlap, no_overlap_extra= PIId2_split_by_sampling(train_df, .7)\n\n\nmini_overlap, overlap_extra = move_missing_labels(mini_overlap, overlap_extra)\nmini_no_overlap, no_overlap_extra= move_missing_labels(mini_no_overlap, no_overlap_extra)\n\n# Save the DataFrame to a JSON file \nmini_overlap.to_json(OUTPUT_DIR + \"/mini_overlap.json\", orient=\"records\")\nmini_no_overlap.to_json(OUTPUT_DIR + \"/mini_no_overlap.json\", orient=\"records\")\n\nprint(\"size of base: \", len(base_df))\nprint(\"size of mini_overlap: \", len(mini_overlap))\nprint(\"size of mini_no_overlap: \", len(mini_no_overlap))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:41:15.677179Z","iopub.execute_input":"2024-04-10T18:41:15.677538Z","iopub.status.idle":"2024-04-10T18:41:19.558498Z","shell.execute_reply.started":"2024-04-10T18:41:15.677509Z","shell.execute_reply":"2024-04-10T18:41:19.557483Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"size of base:  6807\nsize of mini_overlap:  1843\nsize of mini_no_overlap:  1838\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\nfrom itertools import chain\n#Val set analysis\nval_df.info()\ndf = val_df\ntotal_count = df['labels'].apply(len).sum()\n# Flatten the list of labels\nall_labels_flat = list(chain(* val_df.labels.values))\n\n# Count the occurrences of each unique label\nlabel_counts = Counter(all_labels_flat)\n\nentity_count = total_count - label_counts['O']\n\nprint(f'total labels {total_count} \\n entity labels {entity_count}')\n# Print the counts\nfor label, count in label_counts.items():\n    print(f'{label}: {count}')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:04:08.773178Z","iopub.execute_input":"2024-04-10T19:04:08.774739Z","iopub.status.idle":"2024-04-10T19:04:08.833007Z","shell.execute_reply.started":"2024-04-10T19:04:08.774682Z","shell.execute_reply":"2024-04-10T19:04:08.832102Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 688 entries, 4624 to 550\nData columns (total 6 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   document             688 non-null    int64 \n 1   full_text            688 non-null    object\n 2   tokens               688 non-null    object\n 3   trailing_whitespace  688 non-null    object\n 4   labels               688 non-null    object\n 5   token_indices        688 non-null    object\ndtypes: int64(1), object(5)\nmemory usage: 53.8+ KB\ntotal labels 518807 \n entity labels 324\nO: 518483\nB-NAME_STUDENT: 138\nI-NAME_STUDENT: 130\nB-URL_PERSONAL: 15\nB-ID_NUM: 2\nB-PHONE_NUM: 3\nI-PHONE_NUM: 6\nB-EMAIL: 5\nB-STREET_ADDRESS: 2\nI-STREET_ADDRESS: 20\nI-URL_PERSONAL: 1\nI-ID_NUM: 1\nB-USERNAME: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Visualization of an Essay\ncredit : https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\nimport spacy\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = spacy.blank(\"en\")\n\noptions = {\n    \"colors\": {\n        \"B-NAME_STUDENT\": \"aqua\",\n        \"I-NAME_STUDENT\": \"skyblue\",\n        \"B-EMAIL\": \"limegreen\",\n        \"I-EMAIL\": \"lime\",\n        \"B-USERNAME\": \"hotpink\",\n        \"I-USERNAME\": \"lightpink\",\n        \"B-ID_NUM\": \"purple\",\n        \"I-ID_NUM\": \"rebeccapurple\",\n        \"B-PHONE_NUM\": \"red\",\n        \"I-PHONE_NUM\": \"salmon\",\n        \"B-URL_PERSONAL\": \"silver\",\n        \"I-URL_PERSONAL\": \"lightgray\",\n        \"B-STREET_ADDRESS\": \"brown\",\n        \"I-STREET_ADDRESS\": \"chocolate\",\n    }\n}\n\ndef visualize(row):\n    doc = nlp(row.full_text)\n    doc.ents = [\n        Span(doc, idx, idx + 1, label=label)\n        for idx, label in enumerate(row.labels)\n        if label != \"O\"\n    ]\n    html = displacy.render(doc, style=\"ent\", jupyter=False, options=options)\n    return html","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:18:49.950507Z","iopub.execute_input":"2024-04-09T03:18:49.950838Z","iopub.status.idle":"2024-04-09T03:18:56.263106Z","shell.execute_reply.started":"2024-04-09T03:18:49.950810Z","shell.execute_reply":"2024-04-09T03:18:56.261760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n# html = visualize(base_df.loc[0])\n# display(HTML(html))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save to W and B\n\nRunning code below needs to have WANDB_API_KEY secret in kaggle secrets. Access api token through add-ons in notebook\n\n","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n\n\nimport wandb\nwandb.login(key=wandb_api_key)\nwandb.init(project='pii', job_type='preprocessing')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:35:52.518705Z","iopub.execute_input":"2024-04-09T03:35:52.519184Z","iopub.status.idle":"2024-04-09T03:36:13.616171Z","shell.execute_reply.started":"2024-04-09T03:35:52.519147Z","shell.execute_reply":"2024-04-09T03:36:13.615065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#log data as artifacts\n\n#Parquet is a columnar storage file format that is optimized for use with big data processing frameworks.\n#Steps\n# Save the base dataframe 'df' to a parquet file named 'base_data.parquet'\n# Create a new Weights & Biases artifact named 'base_data' of type 'dataset'\n# Create a new Weights & Biases artifact named 'base_data' of type 'dataset'\n# Add the 'base_data.parquet' file to the 'base_data' artifact\n# Log the 'raw_data' artifact to Weights & Biases, this will upload the artifact to the Weights & Biases servers\n\n#base data\nbase_df.to_parquet('base_data.parquet', index=False)\nbase_data = wandb.Artifact(name=\"base_data\", type=\"dataset\")\nbase_data.add_file('base_data.parquet')\nwandb.log_artifact(base_data)\n\n#miniset data dataframe 'mini_no_overlap' \nmini_no_overlap.to_parquet('mini_no_overlap.parquet', index=False)\nmini_no_overlap_data = wandb.Artifact(name=\"mini_no_overlap_data\", type=\"dataset\")\nmini_no_overlap_data.add_file('mini_no_overlap.parquet')\nwandb.log_artifact(mini_no_overlap_data)\n\n#validation set dataframe 'val_df'\nval_df.to_parquet('val_data.parquet', index=False)\nval_data = wandb.Artifact(name=\"val_data\", type=\"dataset\")\nval_data.add_file('val_data.parquet')\nwandb.log_artifact(val_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:57:59.156736Z","iopub.execute_input":"2024-04-09T03:57:59.157227Z","iopub.status.idle":"2024-04-09T03:58:03.203275Z","shell.execute_reply.started":"2024-04-09T03:57:59.157194Z","shell.execute_reply":"2024-04-09T03:58:03.201928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO troubleshoot\n\n# # We will generate html viz for every mini_no_overlap essay, wrap it up in `wandb.Html` and create a W&B table to inspect it\n# df=mini_no_overlap\n\n# wandb_htmls = [wandb.Html(visualize(row)) for _, row in df.iterrows()]\n# df['visualization'] = wandb_htmls\n# table = wandb.Table(dataframe=df)\n# wandb.log({'original_dataset': table})","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:59:15.188967Z","iopub.execute_input":"2024-04-09T03:59:15.189357Z","iopub.status.idle":"2024-04-09T03:59:15.199279Z","shell.execute_reply.started":"2024-04-09T03:59:15.189328Z","shell.execute_reply":"2024-04-09T03:59:15.196766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finish W&B run\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:59:29.780964Z","iopub.execute_input":"2024-04-09T03:59:29.782137Z","iopub.status.idle":"2024-04-09T03:59:35.445735Z","shell.execute_reply.started":"2024-04-09T03:59:29.782087Z","shell.execute_reply":"2024-04-09T03:59:35.444783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TO DOs\n- Truncation with Stride, tokenizers striding method?\n- add hyperparameters to config wandb.config.update","metadata":{}}]}