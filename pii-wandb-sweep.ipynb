{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":171708141,"sourceType":"kernelVersion"},{"sourceId":171885797,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\n-removed stride compared to reference\n-raw data is full competition training comes from base_data artifact\nReference:\n- https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b \n- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=eFhyArSz826Q","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:16:20.670648Z","iopub.execute_input":"2024-04-14T19:16:20.671667Z","iopub.status.idle":"2024-04-14T19:16:31.422476Z","shell.execute_reply.started":"2024-04-14T19:16:20.671630Z","shell.execute_reply":"2024-04-14T19:16:31.421310Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:16:31.424622Z","iopub.execute_input":"2024-04-14T19:16:31.424969Z","iopub.status.idle":"2024-04-14T19:16:43.841186Z","shell.execute_reply.started":"2024-04-14T19:16:31.424937Z","shell.execute_reply":"2024-04-14T19:16:43.840086Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:16:43.842788Z","iopub.execute_input":"2024-04-14T19:16:43.843230Z","iopub.status.idle":"2024-04-14T19:16:43.849759Z","shell.execute_reply.started":"2024-04-14T19:16:43.843187Z","shell.execute_reply":"2024-04-14T19:16:43.848894Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\nfrom transformers import TrainerCallback\n\nclass WandbLoggingCallback(TrainerCallback):\n    def on_log(self, args, state, control, trainer=None, **kwargs):\n        # Log metrics to wandb\n        if trainer is not None:\n            wandb.log(trainer.log_history[-1])\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n#CHECK- modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(artifact, filename='val_data.parquet'): \n    raw_artifact = wandb.use_artifact(artifact)\n    raw_artifact_dir = raw_artifact.download()\n    raw_df = pd.read_parquet(raw_artifact_dir + f'/{filename}')\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n    \n    return reference_df\n\n\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Assuming class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:16:43.852709Z","iopub.execute_input":"2024-04-14T19:16:43.853072Z","iopub.status.idle":"2024-04-14T19:16:43.877768Z","shell.execute_reply.started":"2024-04-14T19:16:43.853041Z","shell.execute_reply":"2024-04-14T19:16:43.876862Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True) #TODO check truncation\n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:16:43.879111Z","iopub.execute_input":"2024-04-14T19:16:43.879362Z","iopub.status.idle":"2024-04-14T19:16:43.894062Z","shell.execute_reply.started":"2024-04-14T19:16:43.879340Z","shell.execute_reply":"2024-04-14T19:16:43.893321Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Metric Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:16:43.895256Z","iopub.execute_input":"2024-04-14T19:16:43.895546Z","iopub.status.idle":"2024-04-14T19:16:43.916997Z","shell.execute_reply.started":"2024-04-14T19:16:43.895523Z","shell.execute_reply":"2024-04-14T19:16:43.916201Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\n\n\ndef train(config = None):\n    # Initialize new wandb run\n    with wandb.init(config=config):\n    #if called by wandb.agent this will be set by sweep controller\n        config = wandb.config\n\n       # Load the training data\n        train_artifact = wandb.use_artifact(config.train_artifact)\n        train_artifact_dir = train_artifact.download()\n        print(f'train art dir: {train_artifact_dir}')\n        print(f'train_artifact_name: {config.train_artifact_name}')\n        train_df = pd.read_parquet(train_artifact_dir + '/'+ config.train_artifact_name +'.parquet')\n\n        # Load the validation data\n        val_artifact = wandb.use_artifact(config.val_artifact)\n        val_artifact_dir = val_artifact.download()\n        val_df = pd.read_parquet(val_artifact_dir + '/' + config.val_artifact_name + '.parquet')\n        eval_df = val_df.copy()\n\n        # Prepare references and labels from val set\n        reference_df = get_reference_df(config.val_artifact)\n        all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n        label2id = {l: i for i,l in enumerate(all_labels)}\n        id2label = {v:k for k,v in label2id.items()}\n\n        # Create the training and validation datasets\n        tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n        train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id)\n        valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id)\n\n        # Initialize the model and data collator\n        model = AutoModelForTokenClassification.from_pretrained(\n            config.training_model_path,\n            num_labels=len(all_labels),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n        # Define the training arguments\n        args = TrainingArguments(\n            output_dir=config.output_dir, \n            fp16=config.fp16,\n            learning_rate=config.learning_rate,\n            num_train_epochs=config.num_train_epochs,\n            per_device_train_batch_size=config.per_device_train_batch_size,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            report_to=config.report_to,\n            evaluation_strategy=config.evaluation_strategy,\n            do_eval=config.do_eval,\n            save_total_limit=config.save_total_limit,\n            logging_steps=config.logging_steps,\n            lr_scheduler_type=config.lr_scheduler_type,\n            warmup_ratio=config.warmup_ratio,\n            weight_decay=config.weight_decay,\n\n            #other args? metric_for_best_model, greater_is_better, \n        )\n\n        #class weights based on dataset to go to CustomTrainer Class #TODO try without\n        # Calculate class weights based on your dataset (TODO: move to config)\n        class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n        # Initialize Trainer with custom class weights\n        trainer = CustomTrainer(\n            model=model, \n            args=args, \n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            data_collator=collator, \n            tokenizer=tokenizer,\n            compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n            class_weights=class_weights,\n            callbacks=[WandbLoggingCallback], #added for wandb anaylsis\n        )\n\n        # Train the model\n        trainer.train()    \n\n        # Make predictions on the validation dataset\n        preds = trainer.predict(valid_ds)\n\n        threshold_tests = [0.9, 0.95, 0.99]\n        scores =[]\n        \n        for threshold in threshold_tests:\n            metrics = compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=threshold)\n            f5_score = metrics['ents_f5']\n            scores.append(f5_score)\n            wandb.log({'threshold': threshold, 'final_f5': f5_score})\n\n        best_threshold = 0.0  \n        best_f5 = 0.0  \n        for thresh, score in zip(threshold_tests, scores):\n            if score > best_f5:\n                best_threshold = thresh\n                best_f5 = score\n            \n        wandb.config.best_threshold = best_threshold\n        \n        print('Experiment finished!')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:17:49.952725Z","iopub.execute_input":"2024-04-14T19:17:49.953244Z","iopub.status.idle":"2024-04-14T19:17:49.974042Z","shell.execute_reply.started":"2024-04-14T19:17:49.953193Z","shell.execute_reply":"2024-04-14T19:17:49.973072Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# W and B ","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:17:49.975928Z","iopub.execute_input":"2024-04-14T19:17:49.976184Z","iopub.status.idle":"2024-04-14T19:17:50.320086Z","shell.execute_reply.started":"2024-04-14T19:17:49.976162Z","shell.execute_reply":"2024-04-14T19:17:50.319288Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# #stop previous sweeps that might be running\n# # Replace 'entity/project/your_sweep_id' with your actual sweep ID\n# sweep = api.sweep(\"csci566sp24/PII_sweep_0/your_sweep_id\")\n\n# # Stop all runs in the sweep\n# for run in sweep.runs:\n#     run.finish()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:17:50.321023Z","iopub.execute_input":"2024-04-14T19:17:50.321261Z","iopub.status.idle":"2024-04-14T19:17:50.325327Z","shell.execute_reply.started":"2024-04-14T19:17:50.321239Z","shell.execute_reply":"2024-04-14T19:17:50.324318Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Sweep config\n#search type\nsweep_config = {\n    'method': 'bayes' #grid, random, bayes\n    }\n\n#metrics for evaluation\nmetric = {\n    'name': 'loss',\n    'goal': 'minimize'   \n    }\n\nsweep_config['metric'] = metric\n\n#intialize parameters \nparameters_dict = {\n    'experiment': {\n        'value': 'pii_sweep0'\n    },\n    'threshold': {\n        'value': 0.99\n    },\n    'o_weight': {\n        'value': 0.05\n    },\n    'raw_artifact': {\n        'value': 'csci566sp24/pii/base_data:v1'\n    },\n    'train_artifact': {\n        'value': 'csci566sp24/pii/mini_no_overlap_data:v4'\n    },\n    'val_artifact': {\n        'value': 'csci566sp24/pii/val_data:v2'\n    },\n    'train_artifact_name': {\n        'value': 'mini_no_overlap'\n    },\n    'val_artifact_name': {\n        'value': 'val_data'\n    },\n    'output_dir': {\n        'value': 'output'\n    },\n    'inference_max_length': {\n        'value': 1024\n    },\n    'training_max_length': {\n        'value': 1024\n    },\n    'training_model_path': {\n        'value': 'microsoft/deberta-v3-xsmall'\n    },\n    'fp16': {\n        'value': True\n    },\n    'learning_rate': {\n        'value': 1e-5\n    },\n    'num_train_epochs': {\n        'value': .5\n    },\n    'per_device_train_batch_size': {\n        'value': 4\n    },\n    'per_device_eval_batch_size': {\n        'value': 4\n    },\n    'gradient_accumulation_steps': {\n        'value': 2\n    },\n    'report_to': {\n        'value': 'wandb'\n    },\n    'evaluation_strategy': {\n        'value': 'epoch'\n    },\n    'do_eval': {\n        'value': False\n    },\n    'save_total_limit': {\n        'value': 1\n    },\n    'logging_steps': {\n        'value': 10\n    },\n    'lr_scheduler_type': {\n        'value': 'cosine'\n    },\n    'warmup_ratio': {\n        'value': 0.1\n    },\n    'weight_decay': {\n        'value': 0.01\n    },\n}\n\nsweep_config['parameters'] = parameters_dict\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:17:50.326497Z","iopub.execute_input":"2024-04-14T19:17:50.326760Z","iopub.status.idle":"2024-04-14T19:17:50.336630Z","shell.execute_reply.started":"2024-04-14T19:17:50.326736Z","shell.execute_reply":"2024-04-14T19:17:50.335852Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# udpates for this sweep\n#update values we dont want to vary during the sweep\n# parameters_dict.update({\n#     'epochs': {\n#         'value': 1}\n#     })\nparameters_dict.update({\n    'learning_rate': {\n        'min': 1e-6,\n        'max': 1e-3\n    },\n    'num_train_epochs': {\n        'min': 1,\n        'max': 6\n    },\n    'per_device_train_batch_size': {\n        'values': [2, 4, 8]\n    },\n        'per_device_eval_batch_size': {\n        'values': [2, 4, 8]\n    },\n    'gradient_accumulation_steps': {\n        'values': [2, 4, 8]\n    },\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:17:50.338610Z","iopub.execute_input":"2024-04-14T19:17:50.338912Z","iopub.status.idle":"2024-04-14T19:17:50.350081Z","shell.execute_reply.started":"2024-04-14T19:17:50.338886Z","shell.execute_reply":"2024-04-14T19:17:50.349354Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#nested dictionary of parameters interested in and method we are trying\nimport pprint\n\npprint.pprint(sweep_config)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T19:17:50.350968Z","iopub.execute_input":"2024-04-14T19:17:50.351196Z","iopub.status.idle":"2024-04-14T19:17:50.366010Z","shell.execute_reply.started":"2024-04-14T19:17:50.351176Z","shell.execute_reply":"2024-04-14T19:17:50.365072Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"{'method': 'bayes',\n 'metric': {'goal': 'minimize', 'name': 'loss'},\n 'parameters': {'do_eval': {'value': False},\n                'evaluation_strategy': {'value': 'epoch'},\n                'experiment': {'value': 'pii_sweep0'},\n                'fp16': {'value': True},\n                'gradient_accumulation_steps': {'values': [2, 4, 8]},\n                'inference_max_length': {'value': 1024},\n                'learning_rate': {'max': 0.001, 'min': 1e-06},\n                'logging_steps': {'value': 10},\n                'lr_scheduler_type': {'value': 'cosine'},\n                'num_train_epochs': {'max': 6, 'min': 1},\n                'o_weight': {'value': 0.05},\n                'output_dir': {'value': 'output'},\n                'per_device_eval_batch_size': {'values': [2, 4, 8]},\n                'per_device_train_batch_size': {'values': [2, 4, 8]},\n                'raw_artifact': {'value': 'csci566sp24/pii/base_data:v1'},\n                'report_to': {'value': 'wandb'},\n                'save_total_limit': {'value': 1},\n                'threshold': {'value': 0.99},\n                'train_artifact': {'value': 'csci566sp24/pii/mini_no_overlap_data:v4'},\n                'train_artifact_name': {'value': 'mini_no_overlap'},\n                'training_max_length': {'value': 1024},\n                'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n                'val_artifact': {'value': 'csci566sp24/pii/val_data:v2'},\n                'val_artifact_name': {'value': 'val_data'},\n                'warmup_ratio': {'value': 0.1},\n                'weight_decay': {'value': 0.01}}}\n","output_type":"stream"}]},{"cell_type":"code","source":"# runs training script\nsweep_id = wandb.sweep(sweep_config, project= 'PII_sweep')\nwandb.agent(sweep_id, train, count=5)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:17:50.367103Z","iopub.execute_input":"2024-04-14T19:17:50.367427Z","iopub.status.idle":"2024-04-14T20:13:11.913212Z","shell.execute_reply.started":"2024-04-14T19:17:50.367394Z","shell.execute_reply":"2024-04-14T20:13:11.912302Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Create sweep with ID: hf4xtl3b\nSweep URL: https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f6a9rgsi with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008019704261548991\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.99\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_191815-f6a9rgsi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/f6a9rgsi' target=\"_blank\">super-sweep-1</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/f6a9rgsi' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/f6a9rgsi</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9464aafdab4bdd868bbfb48717d46e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043b1df1ccb54277858451b0f46d950f"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [190/190 09:23, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.067300</td>\n      <td>0.100129</td>\n      <td>0.047003</td>\n      <td>0.737313</td>\n      <td>0.471167</td>\n      <td>0.047181</td>\n      <td>0.907749</td>\n      <td>0.533489</td>\n      <td>0.024390</td>\n      <td>0.062500</td>\n      <td>0.058957</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.012400</td>\n      <td>0.035010</td>\n      <td>0.216126</td>\n      <td>0.776119</td>\n      <td>0.705784</td>\n      <td>0.253361</td>\n      <td>0.904059</td>\n      <td>0.822785</td>\n      <td>0.064378</td>\n      <td>0.937500</td>\n      <td>0.616114</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Experiment finished!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁█</td></tr><tr><td>eval/ents_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>█▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁█</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁▁</td></tr><tr><td>eval/ents_r</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>final_f5</td><td>█▆▁</td></tr><tr><td>threshold</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇███████</td></tr><tr><td>train/grad_norm</td><td>▅█▃▆▁▆▂▁▁▁▄▃▁▃▁▁▂▁▄</td></tr><tr><td>train/learning_rate</td><td>▅███▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.70578</td></tr><tr><td>eval/ents_p</td><td>0.21613</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.82278</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.25336</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.90406</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.61611</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.06438</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.9375</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.0</td></tr><tr><td>eval/ents_r</td><td>0.77612</td></tr><tr><td>eval/loss</td><td>0.03501</td></tr><tr><td>eval/runtime</td><td>28.5092</td></tr><tr><td>eval/samples_per_second</td><td>24.133</td></tr><tr><td>eval/steps_per_second</td><td>1.508</td></tr><tr><td>final_f5</td><td>0.70578</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>701664441858816.0</td></tr><tr><td>train/epoch</td><td>1.98</td></tr><tr><td>train/global_step</td><td>190</td></tr><tr><td>train/grad_norm</td><td>45697.60547</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0124</td></tr><tr><td>train_loss</td><td>0.10689</td></tr><tr><td>train_runtime</td><td>566.2475</td></tr><tr><td>train_samples_per_second</td><td>10.812</td></tr><tr><td>train_steps_per_second</td><td>0.336</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">super-sweep-1</strong> at: <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/f6a9rgsi' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/f6a9rgsi</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_191815-f6a9rgsi/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r1efs84o with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006794650494801601\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.99\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_192934-r1efs84o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/r1efs84o' target=\"_blank\">smooth-sweep-2</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/r1efs84o' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/r1efs84o</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"368b37a512214486855fe06bd113a359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bddca8122a214dc7989e0cace790abc4"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [144/144 13:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.045300</td>\n      <td>0.051519</td>\n      <td>0.247086</td>\n      <td>0.632836</td>\n      <td>0.596989</td>\n      <td>0.282640</td>\n      <td>0.726937</td>\n      <td>0.685493</td>\n      <td>0.093168</td>\n      <td>0.937500</td>\n      <td>0.695187</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.021200</td>\n      <td>0.037674</td>\n      <td>0.288248</td>\n      <td>0.776119</td>\n      <td>0.728684</td>\n      <td>0.317708</td>\n      <td>0.900369</td>\n      <td>0.841045</td>\n      <td>0.154639</td>\n      <td>0.937500</td>\n      <td>0.784708</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.027027</td>\n      <td>0.333333</td>\n      <td>0.232143</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.008800</td>\n      <td>0.033880</td>\n      <td>0.391045</td>\n      <td>0.782090</td>\n      <td>0.753123</td>\n      <td>0.422067</td>\n      <td>0.889299</td>\n      <td>0.852981</td>\n      <td>0.234375</td>\n      <td>0.937500</td>\n      <td>0.840517</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.962963</td>\n      <td>0.040000</td>\n      <td>0.333333</td>\n      <td>0.260000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Experiment finished!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁▇█</td></tr><tr><td>eval/ents_p</td><td>▁▃█</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁▁█</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁▁█</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁▁█</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁▇█</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁▆█</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁██</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁██</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁▃█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>▁██</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁▅█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁▄█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁▁▁</td></tr><tr><td>eval/ents_r</td><td>▁██</td></tr><tr><td>eval/loss</td><td>█▃▁</td></tr><tr><td>eval/runtime</td><td>▇█▁</td></tr><tr><td>eval/samples_per_second</td><td>▂▁█</td></tr><tr><td>eval/steps_per_second</td><td>▂▁█</td></tr><tr><td>final_f5</td><td>██▁</td></tr><tr><td>threshold</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▄▇▅▃▂▂▁█▁▁█▃▂▆</td></tr><tr><td>train/learning_rate</td><td>▆██▇▇▆▅▄▄▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.75312</td></tr><tr><td>eval/ents_p</td><td>0.39104</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.96296</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>0.5</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>1.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.26</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.04</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.33333</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.85298</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.42207</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.8893</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.84052</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.23438</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.9375</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.0</td></tr><tr><td>eval/ents_r</td><td>0.78209</td></tr><tr><td>eval/loss</td><td>0.03388</td></tr><tr><td>eval/runtime</td><td>28.4618</td></tr><tr><td>eval/samples_per_second</td><td>24.173</td></tr><tr><td>eval/steps_per_second</td><td>1.511</td></tr><tr><td>final_f5</td><td>0.75312</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>1187869249936416.0</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>144</td></tr><tr><td>train/grad_norm</td><td>58907.33984</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0088</td></tr><tr><td>train_loss</td><td>0.1348</td></tr><tr><td>train_runtime</td><td>811.448</td></tr><tr><td>train_samples_per_second</td><td>11.317</td></tr><tr><td>train_steps_per_second</td><td>0.177</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">smooth-sweep-2</strong> at: <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/r1efs84o' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/r1efs84o</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_192934-r1efs84o/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47xydlzs with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009959361716125456\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.99\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_194454-47xydlzs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/47xydlzs' target=\"_blank\">fearless-sweep-3</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/47xydlzs' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/47xydlzs</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81492641df8f47b5a88619a3a4482fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80f4e9187f9c43099810797d55d62082"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [48/48 08:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.102000</td>\n      <td>0.054787</td>\n      <td>0.049678</td>\n      <td>0.322388</td>\n      <td>0.266186</td>\n      <td>0.060311</td>\n      <td>0.343173</td>\n      <td>0.290730</td>\n      <td>0.023734</td>\n      <td>0.937500</td>\n      <td>0.377907</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041000</td>\n      <td>0.040614</td>\n      <td>0.104904</td>\n      <td>0.779104</td>\n      <td>0.624689</td>\n      <td>0.103275</td>\n      <td>0.907749</td>\n      <td>0.698482</td>\n      <td>0.141509</td>\n      <td>0.937500</td>\n      <td>0.770751</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Experiment finished!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁█</td></tr><tr><td>eval/ents_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>▁█</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁▁</td></tr><tr><td>eval/ents_r</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>final_f5</td><td>█▆▁</td></tr><tr><td>threshold</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▃▄▅▇██</td></tr><tr><td>train/global_step</td><td>▁▃▄▅▇█████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▃</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.62469</td></tr><tr><td>eval/ents_p</td><td>0.1049</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.69848</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.10327</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.90775</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.77075</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.14151</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.9375</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.0</td></tr><tr><td>eval/ents_r</td><td>0.7791</td></tr><tr><td>eval/loss</td><td>0.04061</td></tr><tr><td>eval/runtime</td><td>28.4906</td></tr><tr><td>eval/samples_per_second</td><td>24.148</td></tr><tr><td>eval/steps_per_second</td><td>1.509</td></tr><tr><td>final_f5</td><td>0.62469</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>792924332867136.0</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>48</td></tr><tr><td>train/grad_norm</td><td>17720.49805</td></tr><tr><td>train/learning_rate</td><td>8e-05</td></tr><tr><td>train/loss</td><td>0.041</td></tr><tr><td>train_loss</td><td>0.24053</td></tr><tr><td>train_runtime</td><td>540.634</td></tr><tr><td>train_samples_per_second</td><td>11.324</td></tr><tr><td>train_steps_per_second</td><td>0.089</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fearless-sweep-3</strong> at: <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/47xydlzs' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/47xydlzs</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_194454-47xydlzs/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 22jun4z3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005881918490137749\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.99\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_195538-22jun4z3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/22jun4z3' target=\"_blank\">azure-sweep-4</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/22jun4z3' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/22jun4z3</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ab983a144944fe7bcdf273faa604535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8c562c627548db99a9001a210a66a3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [191/191 04:37, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.181300</td>\n      <td>0.210575</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Experiment finished!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁</td></tr><tr><td>eval/ents_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁</td></tr><tr><td>eval/ents_r</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>final_f5</td><td>▁▁▁</td></tr><tr><td>threshold</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███████</td></tr><tr><td>train/grad_norm</td><td>▇▃▂▃▁▁▃▃▁▁▂█▂▂▂▁▂▂▆</td></tr><tr><td>train/learning_rate</td><td>▄███▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.0</td></tr><tr><td>eval/ents_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.0</td></tr><tr><td>eval/ents_r</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.21057</td></tr><tr><td>eval/runtime</td><td>28.771</td></tr><tr><td>eval/samples_per_second</td><td>23.913</td></tr><tr><td>eval/steps_per_second</td><td>1.495</td></tr><tr><td>final_f5</td><td>0.0</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>382215971930880.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>191</td></tr><tr><td>train/grad_norm</td><td>97988.66406</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1813</td></tr><tr><td>train_loss</td><td>0.22988</td></tr><tr><td>train_runtime</td><td>278.7584</td></tr><tr><td>train_samples_per_second</td><td>10.981</td></tr><tr><td>train_steps_per_second</td><td>0.685</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">azure-sweep-4</strong> at: <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/22jun4z3' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/22jun4z3</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_195538-22jun4z3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cz7qfvub with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006229368964842015\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.99\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_200200-cz7qfvub</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/cz7qfvub' target=\"_blank\">firm-sweep-5</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/hf4xtl3b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/cz7qfvub' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/cz7qfvub</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d1bf19688d499aa6a65279c01f96ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c48f30660c34b2c9a2a190ce5bba6b2"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [190/190 09:25, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Ents P</th>\n      <th>Ents R</th>\n      <th>Ents F5</th>\n      <th>Ents Per Type Name Student P</th>\n      <th>Ents Per Type Name Student R</th>\n      <th>Ents Per Type Name Student F5</th>\n      <th>Ents Per Type Street Address P</th>\n      <th>Ents Per Type Street Address R</th>\n      <th>Ents Per Type Street Address F5</th>\n      <th>Ents Per Type Url Personal P</th>\n      <th>Ents Per Type Url Personal R</th>\n      <th>Ents Per Type Url Personal F5</th>\n      <th>Ents Per Type Phone Num P</th>\n      <th>Ents Per Type Phone Num R</th>\n      <th>Ents Per Type Phone Num F5</th>\n      <th>Ents Per Type Email P</th>\n      <th>Ents Per Type Email R</th>\n      <th>Ents Per Type Email F5</th>\n      <th>Ents Per Type Id Num P</th>\n      <th>Ents Per Type Id Num R</th>\n      <th>Ents Per Type Id Num F5</th>\n      <th>Ents Per Type Username P</th>\n      <th>Ents Per Type Username R</th>\n      <th>Ents Per Type Username F5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.135300</td>\n      <td>0.199522</td>\n      <td>0.000497</td>\n      <td>0.391045</td>\n      <td>0.012514</td>\n      <td>0.000497</td>\n      <td>0.483395</td>\n      <td>0.012588</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.045300</td>\n      <td>0.074783</td>\n      <td>0.071908</td>\n      <td>0.543284</td>\n      <td>0.433890</td>\n      <td>0.071908</td>\n      <td>0.671587</td>\n      <td>0.508489</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Experiment finished!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>▁█</td></tr><tr><td>eval/ents_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>▁█</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>▁█</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>▁▁</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>▁▁</td></tr><tr><td>eval/ents_r</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>final_f5</td><td>█▆▁</td></tr><tr><td>threshold</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇███████</td></tr><tr><td>train/grad_norm</td><td>▇██▇▃▆▂▁▂▂▄▃▂▁▂▂▄▂▂</td></tr><tr><td>train/learning_rate</td><td>▅███▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▂▂▁▂▁▁▁▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/ents_f5</td><td>0.43389</td></tr><tr><td>eval/ents_p</td><td>0.07191</td></tr><tr><td>eval/ents_per_type_EMAIL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_EMAIL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_ID_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_f5</td><td>0.50849</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_p</td><td>0.07191</td></tr><tr><td>eval/ents_per_type_NAME_STUDENT_r</td><td>0.67159</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_PHONE_NUM_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_STREET_ADDRESS_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_URL_PERSONAL_r</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_f5</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_p</td><td>0.0</td></tr><tr><td>eval/ents_per_type_USERNAME_r</td><td>0.0</td></tr><tr><td>eval/ents_r</td><td>0.54328</td></tr><tr><td>eval/loss</td><td>0.07478</td></tr><tr><td>eval/runtime</td><td>28.4655</td></tr><tr><td>eval/samples_per_second</td><td>24.17</td></tr><tr><td>eval/steps_per_second</td><td>1.511</td></tr><tr><td>final_f5</td><td>0.43389</td></tr><tr><td>threshold</td><td>0.99</td></tr><tr><td>total_flos</td><td>701664441858816.0</td></tr><tr><td>train/epoch</td><td>1.98</td></tr><tr><td>train/global_step</td><td>190</td></tr><tr><td>train/grad_norm</td><td>20613.25977</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0453</td></tr><tr><td>train_loss</td><td>0.17862</td></tr><tr><td>train_runtime</td><td>568.347</td></tr><tr><td>train_samples_per_second</td><td>10.772</td></tr><tr><td>train_steps_per_second</td><td>0.334</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">firm-sweep-5</strong> at: <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/cz7qfvub' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/cz7qfvub</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240414_200200-cz7qfvub/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"# TODO\n- move helper functions to seperate script","metadata":{}}]}