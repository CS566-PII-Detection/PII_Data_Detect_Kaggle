{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":171708141,"sourceType":"kernelVersion"},{"sourceId":171885797,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\n-removed stride compared to reference\n-raw data is full competition training comes from base_data artifact\nReference:\n- https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b \n- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=eFhyArSz826Q","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:00.075256Z","iopub.execute_input":"2024-04-14T17:14:00.075634Z","iopub.status.idle":"2024-04-14T17:14:18.198597Z","shell.execute_reply.started":"2024-04-14T17:14:00.075592Z","shell.execute_reply":"2024-04-14T17:14:18.197387Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:18.200842Z","iopub.execute_input":"2024-04-14T17:14:18.201130Z","iopub.status.idle":"2024-04-14T17:14:35.840774Z","shell.execute_reply.started":"2024-04-14T17:14:18.201103Z","shell.execute_reply":"2024-04-14T17:14:35.839665Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T17:14:35.842223Z","iopub.execute_input":"2024-04-14T17:14:35.842532Z","iopub.status.idle":"2024-04-14T17:14:52.821600Z","shell.execute_reply.started":"2024-04-14T17:14:35.842506Z","shell.execute_reply":"2024-04-14T17:14:52.820740Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-14 17:14:44.294980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-14 17:14:44.295095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-14 17:14:44.382078: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\nfrom transformers import TrainerCallback\n\nclass WandbLoggingCallback(TrainerCallback):\n    def on_log(self, args, state, control, trainer=None, **kwargs):\n        # Log metrics to wandb\n        wandb.log(trainer.log_history[-1])\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n#CHECK- modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(artifact, filename='val_data.parquet'): \n    raw_artifact = wandb.use_artifact(artifact)\n    raw_artifact_dir = raw_artifact.download()\n    raw_df = pd.read_parquet(raw_artifact_dir + f'/{filename}')\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n    \n    return reference_df\n\n\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Assuming class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T17:14:52.822836Z","iopub.execute_input":"2024-04-14T17:14:52.823452Z","iopub.status.idle":"2024-04-14T17:14:52.846957Z","shell.execute_reply.started":"2024-04-14T17:14:52.823421Z","shell.execute_reply":"2024-04-14T17:14:52.845967Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True) #TODO check truncation\n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:52.850411Z","iopub.execute_input":"2024-04-14T17:14:52.850969Z","iopub.status.idle":"2024-04-14T17:14:52.866097Z","shell.execute_reply.started":"2024-04-14T17:14:52.850941Z","shell.execute_reply":"2024-04-14T17:14:52.865275Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Metric Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:52.867292Z","iopub.execute_input":"2024-04-14T17:14:52.867592Z","iopub.status.idle":"2024-04-14T17:14:52.891929Z","shell.execute_reply.started":"2024-04-14T17:14:52.867566Z","shell.execute_reply":"2024-04-14T17:14:52.891018Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\n\n\ndef train(config = None):\n    # Initialize new wandb run\n    with wandb.init(config=config):\n    #if called by wandb.agent this will be set by sweep controller\n        config = wandb.config\n\n       # Load the training data\n        train_artifact = wandb.use_artifact(config.train_artifact)\n        train_artifact_dir = train_artifact.download()\n        print(f'train art dir: {train_artifact_dir}')\n        print(f'train_artifact_name: {config.train_artifact_name}')\n        train_df = pd.read_parquet(train_artifact_dir + '/'+ config.train_artifact_name +'.parquet')\n\n        # Load the validation data\n        val_artifact = wandb.use_artifact(config.val_artifact)\n        val_artifact_dir = val_artifact.download()\n        val_df = pd.read_parquet(val_artifact_dir + '/' + config.val_artifact_name + '.parquet')\n        eval_df = val_df.copy()\n\n        # Prepare references and labels from val set\n        reference_df = get_reference_df(config.val_artifact)\n        all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n        label2id = {l: i for i,l in enumerate(all_labels)}\n        id2label = {v:k for k,v in label2id.items()}\n\n        # Create the training and validation datasets\n        tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n        train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id)\n        valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id)\n\n        # Initialize the model and data collator\n        model = AutoModelForTokenClassification.from_pretrained(\n            config.training_model_path,\n            num_labels=len(all_labels),\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n        # Define the training arguments\n        args = TrainingArguments(\n            output_dir=config.output_dir, \n            fp16=config.fp16,\n            learning_rate=config.learning_rate,\n            num_train_epochs=config.num_train_epochs,\n            per_device_train_batch_size=config.per_device_train_batch_size,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            report_to=config.report_to,\n            evaluation_strategy=config.evaluation_strategy,\n            do_eval=config.do_eval,\n            save_total_limit=config.save_total_limit,\n            logging_steps=config.logging_steps,\n            lr_scheduler_type=config.lr_scheduler_type,\n            warmup_ratio=config.warmup_ratio,\n            weight_decay=config.weight_decay,\n\n            #other args? metric_for_best_model, greater_is_better, \n        )\n\n        #class weights based on dataset to go to CustomTrainer Class #TODO try without\n        # Calculate class weights based on your dataset (TODO: move to config)\n        class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n        # Initialize Trainer with custom class weights\n        trainer = CustomTrainer(\n            model=model, \n            args=args, \n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            data_collator=collator, \n            tokenizer=tokenizer,\n            compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n            class_weights=class_weights,\n            callbacks=[WandbLoggingCallback], #added for wandb anaylsis\n        )\n\n        # Train the model\n        trainer.train()    \n\n        # Make predictions on the validation dataset\n        preds = trainer.predict(valid_ds)\n\n        threshold_tests = [0.9, 0.95, 0.99]\n        for threshold in threshold_tests:\n            metrics = compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=threshold)\n            f5_score = metrics['ents_f5']\n            wandb.log({'threshold': threshold, 'final_f5': f5_score})\n\n \n        best_row = max(wandb.run.history, key=lambda row: row['final_f5'])\n        best_threshold = best_row['threshold']\n        wandb.config.best_threshold = best_threshold\n\n\n        print('Experiment finished!')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:52.893456Z","iopub.execute_input":"2024-04-14T17:14:52.893808Z","iopub.status.idle":"2024-04-14T17:14:52.913022Z","shell.execute_reply.started":"2024-04-14T17:14:52.893772Z","shell.execute_reply":"2024-04-14T17:14:52.912144Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# W and B ","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:52.914197Z","iopub.execute_input":"2024-04-14T17:14:52.914810Z","iopub.status.idle":"2024-04-14T17:14:54.722804Z","shell.execute_reply.started":"2024-04-14T17:14:52.914781Z","shell.execute_reply":"2024-04-14T17:14:54.721878Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# #stop previous sweeps that might be running\n# # Replace 'entity/project/your_sweep_id' with your actual sweep ID\n# sweep = api.sweep(\"csci566sp24/PII_sweep_0/your_sweep_id\")\n\n# # Stop all runs in the sweep\n# for run in sweep.runs:\n#     run.finish()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T17:14:54.724202Z","iopub.execute_input":"2024-04-14T17:14:54.725363Z","iopub.status.idle":"2024-04-14T17:14:54.729278Z","shell.execute_reply.started":"2024-04-14T17:14:54.725325Z","shell.execute_reply":"2024-04-14T17:14:54.728277Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Sweep config\n#search type\nsweep_config = {\n    'method': 'bayes' #grid, random, bayes\n    }\n\n#metrics for evaluation\nmetric = {\n    'name': 'loss',\n    'goal': 'minimize'   \n    }\n\nsweep_config['metric'] = metric\n\n#intialize parameters \nparameters_dict = {\n    'experiment': {\n        'value': 'pii_sweep0'\n    },\n    'threshold': {\n        'value': 0.9\n    },\n    'o_weight': {\n        'value': 0.05\n    },\n    'raw_artifact': {\n        'value': 'csci566sp24/pii/base_data:v1'\n    },\n    'train_artifact': {\n        'value': 'csci566sp24/pii/mini_no_overlap_data:v4'\n    },\n    'val_artifact': {\n        'value': 'csci566sp24/pii/val_data:v2'\n    },\n    'train_artifact_name': {\n        'value': 'mini_no_overlap'\n    },\n    'val_artifact_name': {\n        'value': 'val_data'\n    },\n    'output_dir': {\n        'value': 'output'\n    },\n    'inference_max_length': {\n        'value': 1024\n    },\n    'training_max_length': {\n        'value': 1024\n    },\n    'training_model_path': {\n        'value': 'microsoft/deberta-v3-xsmall'\n    },\n    'fp16': {\n        'value': True\n    },\n    'learning_rate': {\n        'value': 1e-5\n    },\n    'num_train_epochs': {\n        'value': 0.1\n    },\n    'per_device_train_batch_size': {\n        'value': 16\n    },\n    'per_device_eval_batch_size': {\n        'value': 16\n    },\n    'gradient_accumulation_steps': {\n        'value': 2\n    },\n    'report_to': {\n        'value': 'wandb'\n    },\n    'evaluation_strategy': {\n        'value': 'epoch'\n    },\n    'do_eval': {\n        'value': False\n    },\n    'save_total_limit': {\n        'value': 1\n    },\n    'logging_steps': {\n        'value': 50\n    },\n    'lr_scheduler_type': {\n        'value': 'cosine'\n    },\n    'warmup_ratio': {\n        'value': 0.1\n    },\n    'weight_decay': {\n        'value': 0.01\n    },\n}\n\nsweep_config['parameters'] = parameters_dict\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T17:14:54.730944Z","iopub.execute_input":"2024-04-14T17:14:54.731313Z","iopub.status.idle":"2024-04-14T17:14:54.743444Z","shell.execute_reply.started":"2024-04-14T17:14:54.731279Z","shell.execute_reply":"2024-04-14T17:14:54.742512Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# udpates for this sweep\n#update values we dont want to vary during the sweep\n# parameters_dict.update({\n#     'epochs': {\n#         'value': 1}\n#     })\nparameters_dict.update({\n    'learning_rate': {\n        'min': 1e-6,\n        'max': 1e-3\n    },\n    'num_train_epochs': {\n        'min': 1,\n        'max': 6\n    },\n    'per_device_train_batch_size': {\n        'values': [2, 4, 8]\n    },\n        'per_device_eval_batch_size': {\n        'values': [2, 4, 8]\n    },\n    'gradient_accumulation_steps': {\n        'values': [2, 4, 8]\n    },\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:54.744778Z","iopub.execute_input":"2024-04-14T17:14:54.745157Z","iopub.status.idle":"2024-04-14T17:14:54.754946Z","shell.execute_reply.started":"2024-04-14T17:14:54.745125Z","shell.execute_reply":"2024-04-14T17:14:54.754032Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#nested dictionary of parameters interested in and method we are trying\nimport pprint\n\npprint.pprint(sweep_config)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-14T17:14:54.756133Z","iopub.execute_input":"2024-04-14T17:14:54.756789Z","iopub.status.idle":"2024-04-14T17:14:54.767482Z","shell.execute_reply.started":"2024-04-14T17:14:54.756755Z","shell.execute_reply":"2024-04-14T17:14:54.766511Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{'method': 'bayes',\n 'metric': {'goal': 'minimize', 'name': 'loss'},\n 'parameters': {'do_eval': {'value': False},\n                'evaluation_strategy': {'value': 'epoch'},\n                'experiment': {'value': 'pii_sweep0'},\n                'fp16': {'value': True},\n                'gradient_accumulation_steps': {'values': [2, 4, 8]},\n                'inference_max_length': {'value': 1024},\n                'learning_rate': {'max': 0.001, 'min': 1e-06},\n                'logging_steps': {'value': 50},\n                'lr_scheduler_type': {'value': 'cosine'},\n                'num_train_epochs': {'max': 6, 'min': 1},\n                'o_weight': {'value': 0.05},\n                'output_dir': {'value': 'output'},\n                'per_device_eval_batch_size': {'values': [2, 4, 8]},\n                'per_device_train_batch_size': {'values': [2, 4, 8]},\n                'raw_artifact': {'value': 'csci566sp24/pii/base_data:v1'},\n                'report_to': {'value': 'wandb'},\n                'save_total_limit': {'value': 1},\n                'threshold': {'value': 0.9},\n                'train_artifact': {'value': 'csci566sp24/pii/mini_no_overlap_data:v4'},\n                'train_artifact_name': {'value': 'mini_no_overlap'},\n                'training_max_length': {'value': 1024},\n                'training_model_path': {'value': 'microsoft/deberta-v3-xsmall'},\n                'val_artifact': {'value': 'csci566sp24/pii/val_data:v2'},\n                'val_artifact_name': {'value': 'val_data'},\n                'warmup_ratio': {'value': 0.1},\n                'weight_decay': {'value': 0.01}}}\n","output_type":"stream"}]},{"cell_type":"code","source":"# runs training script\nsweep_id = wandb.sweep(sweep_config, project= 'PII_sweep')\nwandb.agent(sweep_id, train, count=5)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T17:14:54.768624Z","iopub.execute_input":"2024-04-14T17:14:54.769048Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Create sweep with ID: ptz6d86l\nSweep URL: https://wandb.ai/csci566sp24/PII_sweep/sweeps/ptz6d86l\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 166p7qu9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdo_eval: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tevaluation_strategy: epoch\n\u001b[34m\u001b[1mwandb\u001b[0m: \texperiment: pii_sweep0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfp16: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinference_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008389823575438757\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlogging_steps: 50\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \to_weight: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_dir: output\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_eval_batch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \traw_artifact: csci566sp24/pii/base_data:v1\n\u001b[34m\u001b[1mwandb\u001b[0m: \treport_to: wandb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_total_limit: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tthreshold: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact: csci566sp24/pii/mini_no_overlap_data:v4\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_artifact_name: mini_no_overlap\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_max_length: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_model_path: microsoft/deberta-v3-xsmall\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact: csci566sp24/pii/val_data:v2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tval_artifact_name: val_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkasprisi\u001b[0m (\u001b[33mcsci566sp24\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240414_171457-166p7qu9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/PII_sweep/runs/166p7qu9' target=\"_blank\">drawn-sweep-1</a></strong> to <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/ptz6d86l' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/ptz6d86l</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/PII_sweep' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/csci566sp24/PII_sweep/sweeps/ptz6d86l' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/sweeps/ptz6d86l</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/PII_sweep/runs/166p7qu9' target=\"_blank\">https://wandb.ai/csci566sp24/PII_sweep/runs/166p7qu9</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc2e906b1c54f0e943d2c4ea02d0a21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c8c211a8afa4dd2b59f90d93f229292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d152ad567a4c088f544bcc5be09403"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4efa01465f4a8e8f3bf9ae825a54c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12305db1f1d949a2a3d4d774e8fcbdc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a84dbd4acc4d49a5a180a777206479"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_total_limit' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'fp16' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='21' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 21/288 00:46 < 10:59, 0.41 it/s, Epoch 0.21/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"# TODO\n- move helper functions to seperate script","metadata":{}}]}