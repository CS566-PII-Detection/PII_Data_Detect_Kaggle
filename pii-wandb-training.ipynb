{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":171708141,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Training\n-removed stride compared to reference\n-raw data is full competition training comes from base_data artifact\nReference: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b ","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace\n# Baseline config for all experiments\nconfig = SimpleNamespace(\n    experiment='pii000',\n    threshold=0.9,\n    o_weight=0.05,\n    raw_artifact='csci566sp24/pii/base_data:v0',\n    train_artifact = 'csci566sp24/pii/mini_no_overlap_data:v0',\n    val_artifact = 'csci566sp24/pii/val_data:v0',\n    train_artifact_name = 'mini_no_overlap',\n    val_artifact_name = 'val_data',\n    external_data_1='none',\n    external_data_2='none',\n    external_data_3='none',\n    external_data_4='none',\n    external_data_5='none',\n    output_dir=\"output\",\n    inference_max_length=1024, #input sequence for inference\n    training_max_length=1024,  #inference sequence for training\n    training_model_path=\"microsoft/deberta-v3-xsmall\", #base model\n    fp16=True,\n    learning_rate=1e-5,\n    num_train_epochs=0.1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,\n    report_to=\"wandb\",\n    evaluation_strategy=\"epoch\",\n    do_eval=True,\n    save_total_limit=1,\n    logging_steps=10,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    \n#     load_best_model_at_end = True,\n#     metric_for_best_model = \"f1\",\n#     greater_is_better = True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:22.884628Z","iopub.execute_input":"2024-04-12T21:28:22.884979Z","iopub.status.idle":"2024-04-12T21:28:22.899673Z","shell.execute_reply.started":"2024-04-12T21:28:22.884949Z","shell.execute_reply":"2024-04-12T21:28:22.898710Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Define the configuration for this experiment\nconfig.experiment = 'pii006.1'\nconfig.raw_artifact='csci566sp24/pii/base_data:v1'\nconfig.train_artifact = 'csci566sp24/pii/mini_no_overlap_data:v4' \nconfig.val_artifact = 'csci566sp24/pii/val_data:v2'\nconfig.train_artifact_name = 'mini_no_overlap' \nconfig.val_artifact_name = 'val_data'\nconfig.learning_rate = 2e-5 \nconfig.num_train_epochs =  2\nconfig.training_model_path = \"microsoft/deberta-v3-xsmall\" \nconfig.per_device_train_batch_size = 4\nconfig.per_device_eval_batch_size = 4\nconfig.gradient_accumulation_steps = 2\nconfig.evaluation_strategy = \"no\"\nconfig.do_eval = False","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:22.901417Z","iopub.execute_input":"2024-04-12T21:28:22.901684Z","iopub.status.idle":"2024-04-12T21:28:22.914552Z","shell.execute_reply.started":"2024-04-12T21:28:22.901661Z","shell.execute_reply":"2024-04-12T21:28:22.913742Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pickle ","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:22.915567Z","iopub.execute_input":"2024-04-12T21:28:22.915863Z","iopub.status.idle":"2024-04-12T21:28:22.925274Z","shell.execute_reply.started":"2024-04-12T21:28:22.915839Z","shell.execute_reply":"2024-04-12T21:28:22.924405Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install seqeval evaluate transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:22.926385Z","iopub.execute_input":"2024-04-12T21:28:22.926719Z","iopub.status.idle":"2024-04-12T21:28:35.550442Z","shell.execute_reply.started":"2024-04-12T21:28:22.926687Z","shell.execute_reply":"2024-04-12T21:28:35.549174Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb -q\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:35.554164Z","iopub.execute_input":"2024-04-12T21:28:35.554559Z","iopub.status.idle":"2024-04-12T21:28:48.717663Z","shell.execute_reply.started":"2024-04-12T21:28:35.554522Z","shell.execute_reply":"2024-04-12T21:28:48.716697Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:28:48.718848Z","iopub.execute_input":"2024-04-12T21:28:48.719111Z","iopub.status.idle":"2024-04-12T21:28:56.022648Z","shell.execute_reply.started":"2024-04-12T21:28:48.719088Z","shell.execute_reply":"2024-04-12T21:28:56.021695Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-04-12 21:28:53.141330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-12 21:28:53.141392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-12 21:28:53.142926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input?select=utils.py\n# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nfrom spacy.tokens import Span\nfrom spacy import displacy\nimport argparse\nfrom ast import literal_eval\nfrom transformers import Trainer\nfrom torch.nn import CrossEntropyLoss\nfrom scipy.special import softmax\n\ndef str2bool(v):\n    \"Fix Argparse to process bools\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() == 'true':\n        return True\n    elif v.lower() == 'false':\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef parse_args(config):\n    print(\"Running with the following config\")\n    parser = argparse.ArgumentParser(description='Run training baseline')\n    for k,v in config.__dict__.items():\n        parser.add_argument('--'+k, type=type(v) if type(v) is not bool else str2bool, \n                            default=v, \n                            help=f\"Default: {v}\")\n    args = vars(parser.parse_args())\n    \n    # update config with parsed args\n    for k, v in args.items():\n        try:\n            # attempt to eval it it (e.g. if bool, number, or etc)\n            attempt = literal_eval(v)\n        except (SyntaxError, ValueError):\n            # if that goes wrong, just use the string\n            attempt = v\n        setattr(config, k, attempt)\n        print(f\"--{k}:{v}\")\n\n#Mine\ndef threshold_preds(predictions, ds, threshold = 0.9) -> pd.DataFrame:\n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=-1)\n\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    \n    #Choose label with max probability\n    preds_final = predictions.argmax(-1)\n    # or if using threshold\n    #preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    return preds_final\n\n#My edits\ndef preds_to_df (preds_final, ds, id2label):\n    \"\"\"\n    Create a DataFrame of parsed predictions.\n\n    Parameters:\n    - preds_final (list): List of predictions.\n    - ds (Dataset): Dataset containing the token maps, offset mappings, tokens, and documents.\n    - id2label (dict): Dictionary mapping label IDs to labels.\n\n    Returns:\n    - DataFrame: DataFrame containing the submission information.\n    \"\"\"\n    #TODO based on next \n    # Create lists of submission information\n    triplets = set() #improve runtime as recommended by trip\n    row, document, token, label, token_str = [], [], [], []\n\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n        # Iterate through each prediction and its offset\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]  # Predicted label #CHECK remove str\n\n            if start_idx + end_idx == 0: continue   # For special token or padding token\n\n            if token_map[start_idx] == -1:  # Label is for whitespace so go to next\n                start_idx += 1\n\n            # Ignore leading whitespace token \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n            \n            # Break if index exceeds the length of token mapping\n            if start_idx >= len(token_map): break\n            \n            token_id = token_map[start_idx]  # Token ID at start of index\n            #CHECK original token id= token_map[] vs indices[original token id]\n\n            # Ignore \"O\" predictions and whitespace predictions\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                # Add triplet if not in list of triplets\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    # Create a DataFrame of submission information\n    df = pd.DataFrame({\n        \"eval_row\": row, #CHECK is this needed or only for stride\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    #TODO try out\n    df = df.drop_duplicates().reset_index(drop=True)\n    df[\"row_id\"] = list(range(len(df)))\n\n    return df\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    # Scale last dimension to probabilities for interpretability\n    pred_softmax = softmax(predictions, axis=2)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    #preds_final = predictions.argmax(-1) #Choose label with max probability\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\n\n# Mapping of named colors to their RGB equivalents\nnamed_color_to_rgb = {\n    \"aqua\": (0, 255, 255),\n    \"skyblue\": (135, 206, 235),\n    \"limegreen\": (50, 205, 50),\n    \"lime\": (0, 255, 0),\n    \"hotpink\": (255, 105, 180),\n    \"lightpink\": (255, 182, 193),\n    \"purple\": (128, 0, 128),\n    \"rebeccapurple\": (102, 51, 153),\n    \"red\": (255, 0, 0),\n    \"salmon\": (250, 128, 114),\n    \"silver\": (192, 192, 192),\n    \"lightgray\": (211, 211, 211),\n    \"brown\": (165, 42, 42),\n    \"chocolate\": (210, 105, 30),\n    # Add the rest of your named colors and their RGB values here\n}\n\ndef get_rgba(color_name, opacity):\n    \"\"\"Convert a named color and opacity to an rgba string.\"\"\"\n    rgb = named_color_to_rgb[color_name]  # Get the RGB values for the named color\n    return f'rgba({rgb[0]}, {rgb[1]}, {rgb[2]}, {opacity})'\n\n\ndef visualize_prediction_html(tokenizer, row_id, predictions, id2label, ds, threshold=0.7):\n    # Define colors for each label using the provided color map\n    label_colors = {\n        \"B-NAME_STUDENT\": \"aqua\",\n        \"I-NAME_STUDENT\": \"skyblue\",\n        \"B-EMAIL\": \"limegreen\",\n        \"I-EMAIL\": \"lime\",\n        \"B-USERNAME\": \"hotpink\",\n        \"I-USERNAME\": \"lightpink\",\n        \"B-ID_NUM\": \"purple\",\n        \"I-ID_NUM\": \"rebeccapurple\",\n        \"B-PHONE_NUM\": \"red\",\n        \"I-PHONE_NUM\": \"salmon\",\n        \"B-URL_PERSONAL\": \"silver\",\n        \"I-URL_PERSONAL\": \"lightgray\",\n        \"B-STREET_ADDRESS\": \"brown\",\n        \"I-STREET_ADDRESS\": \"chocolate\",\n    }\n\n    # Process predictions to get softmax probabilities, excluding the last column (\"O\" class)\n    pred_softmax = np.exp(predictions[:, :, :-1]) / np.sum(np.exp(predictions[:, :, :-1]), axis=2, keepdims=True)\n    \n    # Find the document in the dataset using row_id\n    doc_tokens = [tokenizer.decode(x) for x in ds[\"input_ids\"][row_id]]\n    doc_predictions = pred_softmax[row_id]\n    doc_labels = doc_predictions.argmax(-1)\n\n    # Start building HTML content\n    html_content = '<div style=\"font-family: Arial; color: black; font-size: larger;\">'\n    for token, token_pred, token_softmax in zip(doc_tokens, doc_labels, doc_predictions):\n        label_pred = id2label[token_pred]\n        if label_pred != \"O\":\n            color = label_colors.get(label_pred, \"#FFFFFF\")  # Default color is white if label not found\n            opacity = np.max(token_softmax)  # Use the max softmax score as opacity\n            rgba_color = get_rgba(color, opacity)\n            html_content += f'<span style=\"background-color: {rgba_color};\">{token}</span> '\n        else:\n            html_content += f'{token} '\n\n    # Add all tokens with probability above threshold\n    high_prob_indices = np.where(doc_predictions.max(-1) > threshold)\n    for index in high_prob_indices[0]:\n        high_prob_label = id2label[doc_labels[index]]\n        high_prob_value = doc_predictions.max(-1)[index]\n        high_prob_color = label_colors.get(high_prob_label, \"#FFFFFF\")\n        rgba_color = get_rgba(high_prob_color, high_prob_value)\n        html_content += f'<p>High probability token: <span style=\"background-color: {rgba_color};\">{doc_tokens[index]}</span> Label: {high_prob_label} Probability: {high_prob_value}</p>'\n    \n    # Add legend for class-color mapping\n    html_content += '<div><p>Legend:</p><ul>'\n    for label, color in label_colors.items():\n        html_content += f'<li><span style=\"background-color: {get_rgba(color, 1)};\">{label}</span></li>'\n    html_content += '</ul></div></div>'\n    return html_content\n\n#CHECK- modified from https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b/input\ndef get_reference_df(artifact, filename='val_data.parquet'): \n    raw_artifact = wandb.use_artifact(artifact)\n    raw_artifact_dir = raw_artifact.download()\n    raw_df = pd.read_parquet(raw_artifact_dir + f'/{filename}')\n    \n    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n    ref_df['token'] = ref_df.groupby('document').cumcount()\n        \n    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n    reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n    \n    return reference_df\n\n#TODO maybe - optimize visualize functions\n# def process_html(tokenizer, index, predictions, id2label, valid_ds, threshold=0.9):\n#     return index, wandb.Html(visualize_prediction_html(tokenizer, index, predictions, id2label, valid_ds, threshold=0.9))\n\n# def generate_htmls_concurrently(viz_df, tokenizer, predictions, id2label, valid_ds, threshold=0.9):\n#     results_with_index = []\n#     with ProcessPoolExecutor() as executor:\n#         futures = [executor.submit(process_html, tokenizer, i, predictions, id2label, valid_ds, threshold=0.9) for i in viz_df.index.values.tolist()]\n#         for future in tqdm(as_completed(futures), total=len(viz_df)):\n#             results_with_index.append(future.result())\n    \n#     # Sort the results by index to maintain the original order\n#     results_with_index.sort(key=lambda x: x[0])\n#     htmls = [result[1] for result in results_with_index]\n#     return htmls\n\n# def visualize(row, nlp):\n#     options = {\n#         \"colors\": {\n#             \"B-NAME_STUDENT\": \"aqua\",\n#             \"I-NAME_STUDENT\": \"skyblue\",\n#             \"B-EMAIL\": \"limegreen\",\n#             \"I-EMAIL\": \"lime\",\n#             \"B-USERNAME\": \"hotpink\",\n#             \"I-USERNAME\": \"lightpink\",\n#             \"B-ID_NUM\": \"purple\",\n#             \"I-ID_NUM\": \"rebeccapurple\",\n#             \"B-PHONE_NUM\": \"red\",\n#             \"I-PHONE_NUM\": \"salmon\",\n#             \"B-URL_PERSONAL\": \"silver\",\n#             \"I-URL_PERSONAL\": \"lightgray\",\n#             \"B-STREET_ADDRESS\": \"brown\",\n#             \"I-STREET_ADDRESS\": \"chocolate\",\n#         }\n#     }\n#     doc = nlp(row.full_text)\n#     doc.ents = [\n#         Span(doc, idx, idx + 1, label=label)\n#         for idx, label in enumerate(row.labels)\n#         if label != \"O\"\n#     ]\n#     html = displacy.render(doc, style=\"ent\", jupyter=False, options=options)\n#     return html\n\n# def convert_for_upload(viz_df):\n#     mapping = {'index': 'str',\n#      'document_x': 'str',\n#      'tokens': 'str',\n#      'trailing_whitespace': 'str',\n#      'labels': 'str',\n#      'token_indices': 'str',\n#      'full_text': 'str',\n#      'unique_labels': 'str',\n#      'EMAIL': 'str',\n#      'ID_NUM': 'str',\n#      'NAME_STUDENT': 'str',\n#      'PHONE_NUM': 'str',\n#      'STREET_ADDRESS': 'str',\n#      'URL_PERSONAL': 'str',\n#      'USERNAME': 'str',\n#      'OTHER': 'str',\n#      'document_y': 'str',\n#      'token': 'str',\n#      'label': 'str',\n#      'token_str': 'str',\n#     }\n#     for key,type in mapping.items():\n#         viz_df[key] = viz_df[key].astype(type)\n#     return viz_df\n\ndef filter_errors(eval_df, preds_df):\n    target_strings = []\n    for i,row in eval_df.iterrows():\n        target_string = [f'{t}: {l}' for t,l in zip(row.tokens, row.labels) if l != \"O\"]\n        target_strings.append(' '.join(target_string))\n    \n    pred_strings = []\n    for i in range(len(eval_df)):\n        i_preds = preds_df[preds_df.eval_row == i]\n        if len(i_preds) > 0:\n            pred_string = [f'{t}: {l}' for t,l in zip(i_preds.token_str, i_preds.label)]\n        else: \n            pred_string = []\n        pred_strings.append(' '.join(pred_string))\n    \n    eval_df['target_string'] = target_strings\n    eval_df['pred_string'] = pred_strings\n    eval_df['error'] = eval_df['target_string'] != eval_df['pred_string']\n    return eval_df[eval_df.error == True]\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Assuming class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Reshape for loss calculation\n        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n        if self.label_smoother is not None and \"labels\" in inputs:\n            loss = self.label_smoother(outputs, inputs)\n        else:\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\n    \n# https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/465970#2589607\n#CHECK\ndef upload_kaggle_dataset(storage_dir, dataset_name, owner=\"thedrcat\"):\n    \"\"\"\n    :param storage_dir: upload storage dir to kaggle as dataset\n    :param dataset_name: name of the dataset\n    :param owner: name of the dataset owner\n    \"\"\"\n    print(f\"creating metadata...\")\n    os.system(f\"kaggle datasets init -p {storage_dir}\")\n\n    print(f\"updating metadata...\")\n    with open(os.path.join(storage_dir, \"dataset-metadata.json\"), \"r\") as f:\n        metadata = json.load(f)\n\n    metadata['title'] = dataset_name\n    metadata['id'] = f\"{owner}/{dataset_name}\".replace(\"_\", \"-\")\n\n    print(f\"saving updated metadata...\")\n    with open(os.path.join(storage_dir, \"dataset-metadata.json\"), \"w\") as f:\n        json.dump(metadata, f)\n\n    print(\"uploading the dataset ...\")\n    os.system(f\"kaggle datasets create -p {storage_dir}\")\n    print(\"done!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.734058Z","iopub.execute_input":"2024-04-12T21:33:40.734436Z","iopub.status.idle":"2024-04-12T21:33:40.803038Z","shell.execute_reply.started":"2024-04-12T21:33:40.734408Z","shell.execute_reply":"2024-04-12T21:33:40.802217Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n#prep data for NER training by tokenize the text and align labels to tokens\ndef tokenize(example, tokenizer, label2id, max_length):\n    \"\"\"This function ensures that the text is correctly tokenized and the labels \n    are correctly aligned with the tokens for NER training.\n\n    Args:\n        example (dict): The example containing the text and labels.\n        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: The tokenized example with aligned labels.\n\n    Reference: credit to https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n    \"\"\"\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n    token_map = [] \n    \n    idx = 0\n\n    #iterate through tokens, labels, and trailing whitespace using zip to create tuple from three lists\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t)) \n        #extend so we can add multiple elements to end of list if ws\n        labels.extend([l] * len(t))\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1) #CHECK\n            \n        idx += 1\n\n    #Tokenize text and return offsets for start and end character position. Limit length of tokenized text.\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True) #TODO check truncation\n\n    #convert to np array for indexing\n    labels = np.array(labels)\n\n    # join text list into a single string \n    text = \"\".join(text)\n    token_labels = []\n\n    #iterate through each tolken\n    for start_idx, end_idx in tokenized.offset_mapping:\n        #if special tolken (CLS token) then append O\n        #CLS : classification token added to the start of each sequence\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        #append orginal label to token_labels\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length,\"token_map\": token_map, } \n\n#helper function for loading to dataset\ndef load_process_ds_helper(json_path, tokenizer, max_length, label2id):\n    # Load data from JSON file into a dict\n    data = json.load(open(json_path))\n    \n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [str(x[\"document\"]) for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n        \"provided_labels\": [x[\"labels\"] for x in data],\n        \"token_indices\": [x[\"token_indices\"] for x in data],\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length\n        }, \n        num_proc=3\n    )\n\n    return ds\n\n#create dataset if using wandb\ndef create_dataset(data, tokenizer, max_length, label2id):\n    '''\n    data(pandas.DataFrame): for wandb artifact\n    '''\n    # Convert data to Hugging Face Dataset object\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"provided_labels\": data.labels.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n\n    # Map the tokenize function to the Dataset\n    ds = ds.map(\n        tokenize,\n        fn_kwargs={      # pass keyword args\n            \"tokenizer\": tokenizer,\n            \"label2id\": label2id,\n            \"max_length\": max_length\n        }, \n        num_proc=3\n    )\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.804654Z","iopub.execute_input":"2024-04-12T21:33:40.804918Z","iopub.status.idle":"2024-04-12T21:33:40.825967Z","shell.execute_reply.started":"2024-04-12T21:33:40.804895Z","shell.execute_reply":"2024-04-12T21:33:40.825097Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Metric Functions","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n\nfrom collections import defaultdict\nfrom typing import Dict\n# from utils import parse_predictions #SCRIPT version\n\nclass PRFScore:\n    \"\"\"A precision / recall / F score.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        tp: int = 0,\n        fp: int = 0,\n        fn: int = 0,\n    ) -> None:\n        self.tp = tp\n        self.fp = fp\n        self.fn = fn\n\n    def __len__(self) -> int:\n        return self.tp + self.fp + self.fn\n\n    def __iadd__(self, other):  # in-place add\n        self.tp += other.tp\n        self.fp += other.fp\n        self.fn += other.fn\n        return self\n\n    def __add__(self, other):\n        return PRFScore(\n            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n        )\n\n    def score_set(self, cand: set, gold: set) -> None:\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)\n\n    @property\n    def precision(self) -> float:\n        return self.tp / (self.tp + self.fp + 1e-100)\n\n    @property\n    def recall(self) -> float:\n        return self.tp / (self.tp + self.fn + 1e-100)\n\n    @property\n    def f1(self) -> float:\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))\n\n    @property\n    def f5(self) -> float:\n        beta = 5\n        p = self.precision\n        r = self.recall\n\n        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n        return fbeta\n\n    def to_dict(self) -> Dict[str, float]:\n        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n\n\ndef compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n    \"\"\"\n    Compute the LB metric (lb) and other auxiliary metrics\n    \"\"\"\n    predictions, labels = p\n    \n    pred_df = parse_predictions(predictions, id2label, valid_ds, threshold=threshold)\n    \n    references = zip(valid_df.document, valid_df.token, valid_df.label)\n    predictions = zip(pred_df.document, pred_df.token, pred_df.label)\n    \n#     references = {(row.document, row.token, row.label) for row in valid_df.itertuples()}\n#     predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n    \n    score_per_type = defaultdict(PRFScore)\n    references = set(references)\n\n    for ex in predictions:\n        pred_type = ex[-1] # (document, token, label)\n        if pred_type != 'O':\n            pred_type = pred_type[2:] # avoid B- and I- prefix\n            \n        if pred_type not in score_per_type:\n            score_per_type[pred_type] = PRFScore()\n\n        if ex in references:\n            score_per_type[pred_type].tp += 1\n            references.remove(ex)\n        else:\n            score_per_type[pred_type].fp += 1\n\n    for doc, tok, ref_type in references:\n        if ref_type != 'O':\n            ref_type = ref_type[2:] # avoid B- and I- prefix\n        \n        if ref_type not in score_per_type:\n            score_per_type[ref_type] = PRFScore()\n        score_per_type[ref_type].fn += 1\n\n    totals = PRFScore()\n    \n    for prf in score_per_type.values():\n        totals += prf\n\n    results = {\n        \"ents_p\": totals.precision,\n        \"ents_r\": totals.recall,\n        \"ents_f5\": totals.f5,\n        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n    }\n    \n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                if isinstance(v, dict):\n                    for n2, v2 in v.items():\n                        final_results[f\"{key}_{n}_{n2}\"] = v2\n                else:\n                    final_results[f\"{key}_{n}\"] = v              \n        else:\n            final_results[key] = value\n            \n    return final_results","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.827511Z","iopub.execute_input":"2024-04-12T21:33:40.828153Z","iopub.status.idle":"2024-04-12T21:33:40.852688Z","shell.execute_reply.started":"2024-04-12T21:33:40.828127Z","shell.execute_reply":"2024-04-12T21:33:40.851548Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# #CHECK Alt compute metrics that i have editted\n\n# #def compute_metrics(p, id2label, valid_ds, valid_df, threshold=0.9):\n# from seqeval.metrics import recall_score, precision_score\n# from seqeval.metrics import classification_report\n# from seqeval.metrics import f1_score\n\n# def compute_metrics(p, all_labels):\n#     \"\"\"Compute the F1, recall, precision metrics for a NER task.\n\n#     Args:\n#         p (Tuple[np.ndarray, np.ndarray]): The predictions and labels.\n#         all_labels (List[str]): The list of all possible labels.\n\n#     Returns:\n#         Dict[str, float]: The computed metrics (recall, precision, f1_score).\n#     Ref: https://www.kaggle.com/code/valentinwerner/915-deberta3base-training/notebook\n#     \"\"\"\n#     #Note: seqeval framework for sequence labeling like NER\n    \n#     # Unpack the predictions and labels\n#     predictions, labels = p\n#     predictions = np.argmax(predictions, axis=2)\n\n#     # Remove ignored index (special tokens)\n#     true_predictions = [\n#         [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n#         for prediction, label in zip(predictions, labels)\n#     ]\n#     true_labels = [\n#         [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n#         for prediction, label in zip(predictions, labels)\n#     ]\n    \n#     recall = recall_score(true_labels, true_predictions)\n#     precision = precision_score(true_labels, true_predictions)\n#     f5_score = (1 + 5*5) * recall * precision / (5*5*precision + recall + 1e-10)\n    \n#     results = {\n#         'recall': recall,\n#         'precision': precision,\n#         'f5': f5_score\n#     }\n#     return results","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.855668Z","iopub.execute_input":"2024-04-12T21:33:40.856031Z","iopub.status.idle":"2024-04-12T21:33:40.875513Z","shell.execute_reply.started":"2024-04-12T21:33:40.855999Z","shell.execute_reply":"2024-04-12T21:33:40.874291Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Training Script\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport pandas as pd\nfrom types import SimpleNamespace\nimport torch\nimport wandb\nimport spacy\n\n# Import necessary functions and classes from other files\n# from metric import compute_metrics\n# from data import create_dataset\n# from utils import get_reference_df, parse_predictions\n# from utils import filter_errors, generate_htmls_concurrently, visualize, convert_for_upload\n# from utils import CustomTrainer\n# from utils import upload_kaggle_dataset, parse_args\n\n# Define the project name for Weights & Biases\nWANDB_PROJECT = 'pii'\n\n\n# # Define the configuration for the experiment #TODO modify later\n# config = SimpleNamespace(\n#     experiment='pii000',\n#     threshold=0.9,\n#     o_weight=0.05,\n#     raw_artifact='csci566sp24/pii/base_data:v0',\n#     train_artifact = 'csci566sp24/pii/mini_no_overlap_data:v0',\n#     val_artifact = 'csci566sp24/pii/val_data:v0',\n#     train_artifact_name = 'mini_no_overlap_data',\n#     val_artifact_name = 'val_data',\n#     external_data_1='none',\n#     external_data_2='none',\n#     external_data_3='none',\n#     external_data_4='none',\n#     external_data_5='none',\n#     output_dir=\"output\",\n#     inference_max_length=1024, #input sequence for inference\n#     training_max_length=1024,  #inference sequence for training\n#     training_model_path=\"microsoft/deberta-v3-xsmall\", #base model\n#     fp16=True,\n#     learning_rate=1e-5,\n#     num_train_epochs=0.1,\n#     per_device_train_batch_size=16,\n#     per_device_eval_batch_size=16,\n#     gradient_accumulation_steps=2,\n#     report_to=\"wandb\",\n#     evaluation_strategy=\"epoch\",\n#     do_eval=True,\n#     save_total_limit=1,\n#     logging_steps=10,\n#     lr_scheduler_type='cosine',\n#     warmup_ratio=0.1,\n#     weight_decay=0.01,\n    \n# #     load_best_model_at_end = True,\n# #     metric_for_best_model = \"f1\",\n# #     greater_is_better = True,\n# )\n\ndef main(config):\n    # Initialize Weights & Biases\n    wandb.init(project=WANDB_PROJECT, job_type='train', config=config)\n    config = wandb.config\n    \n   # Load the training data\n    train_artifact = wandb.use_artifact(config.train_artifact)\n    train_artifact_dir = train_artifact.download()\n    print(f'train art dir: {train_artifact_dir}')\n    print(f'train_artifact_name: {config.train_artifact_name}')\n    train_df = pd.read_parquet(train_artifact_dir + '/'+ config.train_artifact_name +'.parquet')\n\n    # Load the validation data\n    val_artifact = wandb.use_artifact(config.val_artifact)\n    val_artifact_dir = val_artifact.download()\n    val_df = pd.read_parquet(val_artifact_dir + '/' + config.val_artifact_name + '.parquet')\n    eval_df = val_df.copy()\n\n    # Load external data\n    for art in [config.external_data_1, config.external_data_2, config.external_data_3, config.external_data_4, config.external_data_5]:\n        if art != 'none':\n            print(f'Loading external data {art}...')\n            artifact = wandb.use_artifact(art)\n            artifact_dir = artifact.download()\n            ext_df = pd.read_parquet(artifact_dir + '/ext_data.parquet')\n            train_df = pd.concat([train_df, ext_df], ignore_index=True)\n\n    # Prepare references and labels from val set\n    reference_df = get_reference_df(config.val_artifact)\n#     all_labels = sorted(list(set(chain(*[x.tolist() for x in df.labels.values]))))\n    all_labels = sorted(list(set(chain(*[x.tolist() for x in val_df.labels.values])))) #get from val df\n    label2id = {l: i for i,l in enumerate(all_labels)}\n    id2label = {v:k for k,v in label2id.items()}\n\n    \n    # Create the training and validation datasets\n    tokenizer = AutoTokenizer.from_pretrained(config.training_model_path)\n    train_ds = create_dataset(train_df, tokenizer, config.training_max_length, label2id)\n    valid_ds = create_dataset(val_df, tokenizer, config.inference_max_length, label2id)\n    \n    #TEMP\n    print(valid_ds.column_names)\n\n    # Initialize the model and data collator\n    model = AutoModelForTokenClassification.from_pretrained(\n        config.training_model_path,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id,\n        ignore_mismatched_sizes=True\n    )\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n    # Define the training arguments\n    args = TrainingArguments(\n        output_dir=config.output_dir, \n        fp16=config.fp16,\n        learning_rate=config.learning_rate,\n        num_train_epochs=config.num_train_epochs,\n        per_device_train_batch_size=config.per_device_train_batch_size,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        report_to=config.report_to,\n        evaluation_strategy=config.evaluation_strategy,\n        do_eval=config.do_eval,\n        save_total_limit=config.save_total_limit,\n        logging_steps=config.logging_steps,\n        lr_scheduler_type=config.lr_scheduler_type,\n        warmup_ratio=config.warmup_ratio,\n        weight_decay=config.weight_decay,\n        \n        #other args? metric_for_best_model, greater_is_better, \n    )\n\n    #class weights based on dataset to go to CustomTrainer Class #TODO try without\n    # Calculate class weights based on your dataset (TODO: move to config)\n    class_weights = torch.tensor([1.]*12 + [config.o_weight]).to('cuda')\n\n    #TODO try without custom trainer\n    # Initialize Trainer with custom class weights\n    trainer = CustomTrainer(\n        model=model, \n        args=args, \n        train_dataset=train_ds,\n        eval_dataset=valid_ds,\n        data_collator=collator, \n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold),\n        class_weights=class_weights,\n    )\n    \n# #     #TODO try this trainer\n#     #inialize trainer for training and evaluation interface\n#     trainer = Trainer(\n#         model=model, \n#         args=args, \n#         train_dataset=train_ds,\n#         eval_dataset= valid_ds,\n#         data_collator=collator, \n#         tokenizer=tokenizer,\n#         compute_metrics= partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.threshold), #partial to fix all_label argument\n#     )\n\n    # Train the model\n    trainer.train()\n\n    print(\"trained\")#DEBUG\n    \n    \n    # Make predictions on the validation dataset\n    preds = trainer.predict(valid_ds)\n    \n    print(\"predict done\")#DEBUG\n\n    # Compute the final metrics and log them to Weights & Biases\n    print('Computing final metrics...')\n    final_metrics = {\n        f'final_p_at_threshold_.9': compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold= .9)['ents_r'],\n        f'final_f5_at_.9': compute_metrics((preds.predictions, None), id2label, valid_ds, reference_df, threshold=.9)['ents_f5'],\n    }\n    wandb.log(final_metrics)\n    print(final_metrics)\n\n    # pick the best threshold from the final metrics and use it to generate preds_df\n    best_threshold = float(max(final_metrics, key=final_metrics.get).split('_')[-1])\n    wandb.config.best_threshold = best_threshold\n    preds_df = parse_predictions(preds.predictions, id2label, valid_ds, threshold=best_threshold)\n\n#     print('Visualizing errors...')\n#     grouped_preds = preds_df.groupby('eval_row')[['document', 'token', 'label', 'token_str']].agg(list)\n#     eval_df.set_index('index', inplace=True) # Set 'index' as the index of eval_df\n#     # Merge on indices\n#     viz_df = pd.merge(eval_df, grouped_preds, how='left', left_index=True, right_index=True)\n#     viz_df = filter_errors(viz_df, preds_df)\n#     viz_df['pred_viz'] = generate_htmls_concurrently(viz_df, tokenizer, preds.predictions, id2label, valid_ds, threshold=best_threshold)\n#     nlp = spacy.blank(\"en\")\n#     htmls = viz_df.apply(lambda row: visualize(row, nlp), axis=1) # Use apply instead of iterrows\n#     wandb_htmls = [wandb.Html(html) for html in htmls]\n#     viz_df['gt_viz'] = wandb_htmls\n#     viz_df.fillna(\"\", inplace=True) # Fill NaN values in-place\n#     viz_df = convert_for_upload(viz_df)\n#     errors_table = wandb.Table(dataframe=viz_df)\n#     wandb.log({'errors_table': errors_table})\n\n    # Save the model and upload it to Kaggle\n    os.makedirs(config.experiment, exist_ok=True)\n    trainer.save_model(config.experiment)\n    tokenizer.save_pretrained(config.experiment)\n    # if training on a local machine, uncomment and fill in your username to upload the model to Kaggle\n    # upload_kaggle_dataset(config.experiment, config.experiment, owner=\"thedrcat\")\n    print('Experiment finished, test it out on the inference notebook!')\n    \n    # do stuff with predictions object and val dataset object later\n    with open('val_set_preds.pkl', 'wb') as f:\n        pickle.dump(preds_df, f)\n\n    with open('valid_ds.pkl', 'wb') as f:\n        pickle.dump(valid_ds, f)\n        \n    print(\"saved val_set_preds and valid_ds with pickle for loss testing\")\n    \n    return preds #predicitons for validation set\n\n# if __name__ == \"__main__\":\n#     parse_args(config)\n#     main(config)\n\n#if in notebook then call to main is below in W and B section","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.877197Z","iopub.execute_input":"2024-04-12T21:33:40.877813Z","iopub.status.idle":"2024-04-12T21:33:40.914098Z","shell.execute_reply.started":"2024-04-12T21:33:40.877785Z","shell.execute_reply":"2024-04-12T21:33:40.913002Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# if helper function code is attached\n# !cp /kaggle/input/pii-data-detection-training-code/* .","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.915276Z","iopub.execute_input":"2024-04-12T21:33:40.915612Z","iopub.status.idle":"2024-04-12T21:33:40.930330Z","shell.execute_reply.started":"2024-04-12T21:33:40.915579Z","shell.execute_reply":"2024-04-12T21:33:40.929395Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# W and B ","metadata":{}},{"cell_type":"code","source":"# make sure to attach key from secrets in add-ons\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nimport wandb\nwandb.login(key=wandb_api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:40.931505Z","iopub.execute_input":"2024-04-12T21:33:40.931842Z","iopub.status.idle":"2024-04-12T21:33:41.043278Z","shell.execute_reply.started":"2024-04-12T21:33:40.931812Z","shell.execute_reply":"2024-04-12T21:33:41.042162Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# runs training script\nval_set_preds = main(config)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T21:33:41.044565Z","iopub.execute_input":"2024-04-12T21:33:41.045395Z","iopub.status.idle":"2024-04-12T21:43:52.420949Z","shell.execute_reply.started":"2024-04-12T21:33:41.045363Z","shell.execute_reply":"2024-04-12T21:43:52.419900Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:mz20npg4) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df9e2f11f81a4d07b7ee85f1592d05ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sandy-totem-59</strong> at: <a href='https://wandb.ai/csci566sp24/pii/runs/mz20npg4' target=\"_blank\">https://wandb.ai/csci566sp24/pii/runs/mz20npg4</a><br/> View project at: <a href='https://wandb.ai/csci566sp24/pii' target=\"_blank\">https://wandb.ai/csci566sp24/pii</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240412_212859-mz20npg4/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:mz20npg4). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240412_213341-g0r4yx45</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/csci566sp24/pii/runs/g0r4yx45' target=\"_blank\">eager-sponge-60</a></strong> to <a href='https://wandb.ai/csci566sp24/pii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/csci566sp24/pii' target=\"_blank\">https://wandb.ai/csci566sp24/pii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/csci566sp24/pii/runs/g0r4yx45' target=\"_blank\">https://wandb.ai/csci566sp24/pii/runs/g0r4yx45</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"train art dir: /kaggle/working/artifacts/mini_no_overlap_data:v4\ntrain_artifact_name: mini_no_overlap\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/3061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"441718f631c646039dc18c924ae9cf17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ab1eda10d74e8883b2f6cdc5e4014d"}},"metadata":{}},{"name":"stdout","text":"['full_text', 'document', 'tokens', 'trailing_whitespace', 'provided_labels', 'token_indices', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length', 'token_map']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='382' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [382/382 08:28, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.253100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.005200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.508500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.729700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.165800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.182500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.148900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.143100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.061200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.149700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.067400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.049800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.036100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.048000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.035300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.046200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.033300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.021900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.048500</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.047600</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.037300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.041800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.025200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"trained\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"predict done\nComputing final metrics...\n{'final_p_at_threshold_.9': 0.7940298507462686, 'final_f5_at_.9': 0.7569224034146876}\nExperiment finished, test it out on the inference notebook!\nsaved val_set_preds and valid_ds with pickle for loss testing\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TODO\n- move helper functions to seperate script","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}