{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":171266604,"sourceType":"kernelVersion"},{"sourceId":171237695,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PII W and B Inference\nREference https://www.kaggle.com/code/thedrcat/pii-data-detection-infer-with-w-b","metadata":{}},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"INFERENCE_MODEL_PATH = '/kaggle/input/pii-wandb-training/pii002'\nDATA_PATH = '../input/pii-detection-removal-from-educational-data'\nVAL_PATH = '/kaggle/input/pii-wandb-prep/val.json'\nINFERENCE_MAX_LENGTH = 1024\nOUTPUT_DIR = \"/kaggle/working/\"\nTHRESHOLD=0.9","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:14.969597Z","iopub.execute_input":"2024-04-10T17:36:14.969908Z","iopub.status.idle":"2024-04-10T17:36:14.982168Z","shell.execute_reply.started":"2024-04-10T17:36:14.969881Z","shell.execute_reply":"2024-04-10T17:36:14.981269Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport json\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset, features\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:14.983936Z","iopub.execute_input":"2024-04-10T17:36:14.984276Z","iopub.status.idle":"2024-04-10T17:36:45.327138Z","shell.execute_reply.started":"2024-04-10T17:36:14.984246Z","shell.execute_reply":"2024-04-10T17:36:45.326149Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-10 17:36:31.455329: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 17:36:31.455458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 17:36:31.711401: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Util Functions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# data Functions","metadata":{}},{"cell_type":"code","source":"def add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:45.332367Z","iopub.execute_input":"2024-04-10T17:36:45.332647Z","iopub.status.idle":"2024-04-10T17:36:45.337293Z","shell.execute_reply.started":"2024-04-10T17:36:45.332623Z","shell.execute_reply":"2024-04-10T17:36:45.336429Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# helpers","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From Training Helpers\n","metadata":{}},{"cell_type":"code","source":"def infer_tokenize(example, tokenizer):\n    \"\"\"\n    Tokenize an example for NER using the given tokenizer.\n\n    Args:\n        example (dict): A dictionary containing \"tokens\" and \"trailing_whitespace\" lists.\n            - \"tokens\": A list of token strings.\n            - \"trailing_whitespace\": A list of boolean values indicating whether each token has trailing whitespace.\n        tokenizer: The tokenizer to use for tokenization.\n        label2id (dict): A dictionary mapping labels to their corresponding ids.\n        max_length (int): The maximum length of the tokenized text.\n\n    Returns:\n        dict: A dictionary containing tokenized output, including offsets mapping and token map.\n            - \"input_ids\": List of token IDs.\n            - \"attention_mask\": List of attention mask values.\n            - \"offset_mapping\": List of character offsets for each token.\n            - \"token_map\": List mapping each input token to its original position in the example.\n            \n    Reference: https://www.kaggle.com/code/valentinwerner/893-deberta3base-Inference\n    \"\"\"\n    #empty list to store text and tokens in respective map\n    text = []\n    token_map = []\n    \n    #keep track of tokens\n    idx = 0\n    \n    #for the example go through tokens and whitespace\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        #add token to text\n        text.append(t)\n        #extend token length number of idx\n        token_map.extend([idx]*len(t))\n        #for whitespace add a space to text and label -1 in token map\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    #Tokenize the text and return offset mapping with the token map    \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    length = len(tokenized.input_ids)\n        \n    return {\n        **tokenized,\n        \"length\": length,\n        \"token_map\": token_map,\n    }\n\ndef create_dataset(data, tokenizer, max_length):\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n    ds = ds.map(\n        infer_tokenize,\n        fn_kwargs={\"tokenizer\": tokenizer,\n                   # \"max_length\": max_length #CHECK\n                  }, \n        num_proc=3\n    )\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:45.339652Z","iopub.execute_input":"2024-04-10T17:36:45.339938Z","iopub.status.idle":"2024-04-10T17:36:45.353154Z","shell.execute_reply.started":"2024-04-10T17:36:45.339913Z","shell.execute_reply":"2024-04-10T17:36:45.352389Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Load and predict","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(INFERENCE_MODEL_PATH)\nmodel = AutoModelForTokenClassification.from_pretrained(INFERENCE_MODEL_PATH)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n# Load id2label configuration from model\nconfig = json.load(open(INFERENCE_MODEL_PATH + \"/config.json\"))\nid2label = config[\"id2label\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:45.354268Z","iopub.execute_input":"2024-04-10T17:36:45.354657Z","iopub.status.idle":"2024-04-10T17:36:49.773042Z","shell.execute_reply.started":"2024-04-10T17:36:45.354630Z","shell.execute_reply":"2024-04-10T17:36:49.771947Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\n\ntest_data = json.load(open(DATA_PATH + \"/test.json\"))\nsub_df = pd.DataFrame(test_data)\n\nsub_df['token_indices'] = sub_df['tokens'].apply(add_token_indices)\nsub_ds = create_dataset(sub_df, tokenizer, INFERENCE_MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:49.774355Z","iopub.execute_input":"2024-04-10T17:36:49.774672Z","iopub.status.idle":"2024-04-10T17:36:50.776034Z","shell.execute_reply.started":"2024-04-10T17:36:49.774646Z","shell.execute_reply":"2024-04-10T17:36:50.774857Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2fe196a4e3d4048a9c9d306b26c4484"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\n#CHECK no training_args\ntrainer = Trainer(\n    model=model, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\n\npreds = trainer.predict(sub_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:50.777778Z","iopub.execute_input":"2024-04-10T17:36:50.778094Z","iopub.status.idle":"2024-04-10T17:36:56.246456Z","shell.execute_reply.started":"2024-04-10T17:36:50.778064Z","shell.execute_reply":"2024-04-10T17:36:56.245534Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"markdown","source":"# Post Process","metadata":{}},{"cell_type":"code","source":"#helper\n#note need to update softmax\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\n    triplets = set()\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            #CHECK\n            token_id = token_map[start_idx] #token ID at the start of the index\n#             original_token_id = token_map[start_idx]\n#             token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.add(triplet)\n\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.248074Z","iopub.execute_input":"2024-04-10T17:36:56.248451Z","iopub.status.idle":"2024-04-10T17:36:56.262848Z","shell.execute_reply.started":"2024-04-10T17:36:56.248417Z","shell.execute_reply":"2024-04-10T17:36:56.261711Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"id2label.keys()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.264225Z","iopub.execute_input":"2024-04-10T17:36:56.264582Z","iopub.status.idle":"2024-04-10T17:36:56.289167Z","shell.execute_reply.started":"2024-04-10T17:36:56.264551Z","shell.execute_reply":"2024-04-10T17:36:56.288039Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'])"},"metadata":{}}]},{"cell_type":"code","source":"#CHECK why preds.predictions\npreds_df = parse_predictions(preds.predictions, id2label, sub_ds, threshold=THRESHOLD)\n\n#look at to see\ndisplay(preds_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.292274Z","iopub.execute_input":"2024-04-10T17:36:56.292559Z","iopub.status.idle":"2024-04-10T17:36:56.420624Z","shell.execute_reply.started":"2024-04-10T17:36:56.292536Z","shell.execute_reply":"2024-04-10T17:36:56.419682Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"   eval_row  document  token           label token_str  row_id\n0         0         7     60        B-ID_NUM   Dessine       0\n1         1        10    356  B-NAME_STUDENT         …       1\n2         2        16      5        B-ID_NUM    Gamboa       2\n3         2        16    527        B-ID_NUM        ’s       3\n4         3        20      5        B-ID_NUM     Sindy       4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eval_row</th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>7</td>\n      <td>60</td>\n      <td>B-ID_NUM</td>\n      <td>Dessine</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10</td>\n      <td>356</td>\n      <td>B-NAME_STUDENT</td>\n      <td>…</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>16</td>\n      <td>5</td>\n      <td>B-ID_NUM</td>\n      <td>Gamboa</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>16</td>\n      <td>527</td>\n      <td>B-ID_NUM</td>\n      <td>’s</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>20</td>\n      <td>5</td>\n      <td>B-ID_NUM</td>\n      <td>Sindy</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"preds_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.422227Z","iopub.execute_input":"2024-04-10T17:36:56.422601Z","iopub.status.idle":"2024-04-10T17:36:56.441550Z","shell.execute_reply.started":"2024-04-10T17:36:56.422567Z","shell.execute_reply":"2024-04-10T17:36:56.440606Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Test other F5 validation checks\n- move helper functions to seperate script","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef determine_metrics_preds_gt(preds_final, gt_ds, id2label, beta=5):\n    \"\"\"\n    Create a DataFrame of submission information.\n\n    Parameters:\n    - preds_final (list): List of predictions.\n    - id2label (dict): Dictionary mapping label IDs to labels.\n    - gt_ds (Dataset): Dataset containing the token maps, offset mappings, tokens, gt_labels, and documents.\n\n    Returns:\n    - DataFrame: DataFrame containing the submission information.\n    \"\"\"\n    # Create lists of submission information\n    triplets = []\n    document, token, p_label, gt_label, token_str, compare = [], [], [], [], [], []\n\n    for p, gt_labels, token_map, offsets, tokens, doc in zip(preds_final, gt_ds[\"labels\"], gt_ds[\"token_map\"], gt_ds[\"offset_mapping\"], gt_ds[\"tokens\"], gt_ds[\"document\"]):\n        # Iterate through each label and its offset\n        for label_pred, label_gt, (start_idx, end_idx) in zip(p, gt_labels, offsets):\n            label_pred = id2label[str(label_pred)]  # Predicted label\n\n            if start_idx + end_idx == 0: continue   # For special token or padding token\n\n            if token_map[start_idx] == -1:  # Label is for whitespace so go to next\n                start_idx += 1\n\n            # Ignore leading whitespace token \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n            \n            # Break if index exceeds the length of token mapping\n            if start_idx >= len(token_map): break\n            \n            token_id = token_map[start_idx]  # Token ID at start of index\n\n            # Ignore \"O\" labels and whitespace labels\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n                \n                # If the ground truth label is missing, it's a false positive\n                if label_gt == \"O\" and label_pred !=\"O\": match = \"FP\"\n                    \n                # If the predicted label is missing or doesn't match the ground truth, it's a false negative\n                elif (label_pred is None) or (label_gt != label_pred) : match = \"FN\"\n    \n                # If the ground truth label is missing, it's a false positive\n                elif label_gt == \"O\" and label_pred !=\"O\": match = \"FP\"\n\n                # If the predicted label matches the ground truth, it's a true positive\n                elif label_gt == label_pred : match = \"TP\"\n                    \n                else: match = \"?\"\n\n                # Add triplet if not in list of triplets\n                if triplet not in triplets:\n                    document.append(doc)\n                    token.append(token_id)\n                    p_label.append(label_pred)\n                    gt_label.append(label_gt)\n                    token_str.append(tokens[token_id])\n                    compare.append(match)\n                    triplets.append(triplet)\n\n    # Create a DataFrame of submission information\n    df = pd.DataFrame({\n        \"document\": document,\n        \"token\": token,\n        \"pred_label\": p_label,\n        \"gt_label\": gt_label,\n        \"token_str\": token_str,\n        \"compare\": compare\n    })\n    \n    # Count the number of false positives, false negatives, and true positives\n    FP = (df['compare'] == \"FP\").sum()\n    FN = (df['compare'] == \"FN\").sum()\n    TP = (df['compare'] == \"TP\").sum()\n    \n    # Calculate the precision, recall, and F-beta score\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    fbeta_mircro_score = (1 + (beta**2)) * precision * recall / (((beta**2) * precision) + recall) if precision + recall > 0 else 0\n    \n    # Print the precision, recall, and F-beta score\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F-beta score: {fbeta_mircro_score}\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.442865Z","iopub.execute_input":"2024-04-10T17:36:56.443212Z","iopub.status.idle":"2024-04-10T17:36:56.460876Z","shell.execute_reply.started":"2024-04-10T17:36:56.443183Z","shell.execute_reply":"2024-04-10T17:36:56.460010Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#test Metrics\ndef pii_metrics_score(pred_df, gt_df, beta=5):\n    \"\"\"\n    Calculate print the Precision, Recall and Micro F-beta score for predicted PII labels. Determines which were false negatives\n\n    Parameters:\n    - pred_df (DataFrame): DataFrame containing predicted PII labels [\"row_id\", \"document\", \"token\", \"label\"].\n    - gt_df (DataFrame): DataFrame containing ground truth PII labels [\"row_id\", \"document\", \"token\", \"label\"].\n    - beta (float): The beta parameter for the F-beta score, controlling the trade-off between precision and recall.\n\n    Returns:\n    - results (dict): Dictionary containing the precision, recall, and F-beta score.\n    \"\"\"   \n    # Merge the predicted and ground truth DataFrames on 'document' and 'token' columns\n    df = pred_df.merge(gt_df, how='outer', on=['document', \"token\"], suffixes=('_pred', '_gt'))\n\n    # Initialize a new column 'compare' with empty strings\n    df['compare'] = \"\"\n\n    # If the predicted label is missing or doesn't match the ground truth, it's a false negative\n    df.loc[df.label_pred.isna() | (df.label_gt != df.label_pred), 'compare'] = \"FN\"\n    \n    # If the ground truth label is missing, it's a false positive\n    df.loc[df.label_gt.isna(), 'compare'] = \"FP\"\n\n    # If the predicted label matches the ground truth, it's a true positive\n    df.loc[(df.label_pred.notna()) & (df.label_gt == df.label_pred), 'compare'] = \"TP\"\n    \n    # Count the number of false positives, false negatives, and true positives\n    FP = (df['compare'] == \"FP\").sum()\n    FN = (df['compare'] == \"FN\").sum()\n    TP = (df['compare'] == \"TP\").sum()\n\n\n    # Calculate the precision, recall, and F-beta score\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    fbeta_mircro_score = (1 + (beta**2)) * precision * recall / (((beta**2) * precision) + recall) if precision + recall > 0 else 0\n\n    # Get a DataFrame of false negatives\n    fn_df = df.loc[df.label_pred.isna() | (df.label_gt != df.label_pred)]\n    \n    # Print the precision, recall, and F-beta score\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F-beta score: {fbeta_mircro_score}\")\n    \n    return fn_df","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.462195Z","iopub.execute_input":"2024-04-10T17:36:56.462510Z","iopub.status.idle":"2024-04-10T17:36:56.481149Z","shell.execute_reply.started":"2024-04-10T17:36:56.462464Z","shell.execute_reply":"2024-04-10T17:36:56.480300Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(\"submission sample metrics:\")\npred_df = pd.read_csv('/kaggle/working/submission.csv')\ngt_df= pd.read_csv('/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv')\nfn_df = pii_metrics_score(pred_df, gt_df, beta=5)\n\nfn_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:56.482206Z","iopub.execute_input":"2024-04-10T17:36:56.482471Z","iopub.status.idle":"2024-04-10T17:36:56.541711Z","shell.execute_reply.started":"2024-04-10T17:36:56.482449Z","shell.execute_reply":"2024-04-10T17:36:56.540746Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"submission sample metrics:\nPrecision: 0.08333333333333333\nRecall: 0.07142857142857142\nF-beta score: 0.0718232044198895\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   row_id_pred  document  token label_pred  row_id_gt        label_gt compare\n0          NaN         7      9        NaN        0.0  B-NAME_STUDENT      FN\n1          NaN         7     10        NaN        1.0  I-NAME_STUDENT      FN\n2          0.0         7     60   B-ID_NUM        NaN             NaN      FP\n3          NaN         7    482        NaN        2.0  B-NAME_STUDENT      FN\n4          NaN         7    483        NaN        3.0  I-NAME_STUDENT      FN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id_pred</th>\n      <th>document</th>\n      <th>token</th>\n      <th>label_pred</th>\n      <th>row_id_gt</th>\n      <th>label_gt</th>\n      <th>compare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>7</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>FN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>7</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>I-NAME_STUDENT</td>\n      <td>FN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>7</td>\n      <td>60</td>\n      <td>B-ID_NUM</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>7</td>\n      <td>482</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>FN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>7</td>\n      <td>483</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>I-NAME_STUDENT</td>\n      <td>FN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_data = json.load(open(VAL_PATH))\nval_df = pd.DataFrame(val_data)\n\nprint(val_df.columns)\n\nval_df['token_indices'] = val_df['tokens'].apply(add_token_indices)\n\nds = Dataset.from_pandas(val_df)\nval_gt_ds = ds.map(\n        infer_tokenize,\n        fn_kwargs={\"tokenizer\": tokenizer,}, \n        num_proc=3\n    )\n\nval_ds = val_gt_ds.remove_columns('labels')\n\npreds = trainer.predict(sub_ds)\n\npredictions = preds.predictions\nthreshold =THRESHOLD\npred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\npreds = predictions.argmax(-1)\npreds_without_O = pred_softmax[:,:,:12].argmax(-1)\nO_preds = pred_softmax[:,:,12]\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)\n\nprint(\"validation sample metrics:\")\n\nval_compare_df= determine_metrics_preds_gt(preds_final, val_gt_ds, id2label)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:43:01.430611Z","iopub.execute_input":"2024-04-10T17:43:01.430990Z","iopub.status.idle":"2024-04-10T17:43:12.993317Z","shell.execute_reply.started":"2024-04-10T17:43:01.430956Z","shell.execute_reply":"2024-04-10T17:43:12.992201Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Index(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels',\n       'token_indices'],\n      dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/688 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b9fd6487174a2facfbf47e71e63f66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"validation sample metrics:\nPrecision: 0.0005970149253731343\nRecall: 0.3333333333333333\nF-beta score: 0.014857142857142857\n","output_type":"stream"}]},{"cell_type":"code","source":"# Count the number of false positives, false negatives, and true positives\nprint(\"FP: \", (val_compare_df['compare'] == \"FP\").sum()) \nprint(\"FN: \", (val_compare_df['compare'] == \"FN\").sum()) \nprint(\"TP: \", (val_compare_df['compare'] == \"TP\").sum())\nprint(\"?: \", (val_compare_df['compare'] == \"?\").sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:44:07.098920Z","iopub.execute_input":"2024-04-10T17:44:07.099286Z","iopub.status.idle":"2024-04-10T17:44:07.108313Z","shell.execute_reply.started":"2024-04-10T17:44:07.099254Z","shell.execute_reply":"2024-04-10T17:44:07.107208Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"FP:  1674\nFN:  2\nTP:  1\n?:  0\n","output_type":"stream"}]},{"cell_type":"code","source":"val_compare_df[val_compare_df['compare'] == \"FP\"].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:36:59.499094Z","iopub.status.idle":"2024-04-10T17:36:59.499433Z","shell.execute_reply.started":"2024-04-10T17:36:59.499272Z","shell.execute_reply":"2024-04-10T17:36:59.499287Z"},"trusted":true},"execution_count":null,"outputs":[]}]}